<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">

<script>
    (function(){
        if(''){
                         If (prompt('Please enter the article password') !== ''){
                                 Alert('Password error!');
                history.back();
            }
        }
    })();
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"snakecoding.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":"flat","style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":false},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Refuse to Fall">
<meta property="og:type" content="website">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://snakecoding.com/page/10/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="Refuse to Fall">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Karan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://snakecoding.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Machine Learning</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Machine Learning</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">13</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">61</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/yourname" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/03/03_shallow-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/03/03_shallow-neural-networks/" class="post-title-link" itemprop="url">03_shallow-neural-networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-03 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-03T00:00:00+05:30">2018-02-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:22" itemprop="dateModified" datetime="2020-04-06T20:25:22+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>19k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>17 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the third week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p><h2 id="01-shallow-neural-network"><a href="#01-shallow-neural-network" class="headerlink" title="01_shallow-neural-network"></a>01_shallow-neural-network</h2><h3 id="01-neural-networks-overview"><a href="#01-neural-networks-overview" class="headerlink" title="01_neural-networks-overview"></a>01_neural-networks-overview</h3><p>welcome back in this week’s you learn to implement a neural network before diving into the technical details I wanted in this video to give you a quick overview of what you’ll be seeing in this week’s videos so if you don’t follow all the details in this video don’t worry about it we’ll delve in the technical details in the next few videos but for now let’s give a quick overview of how you implement in your network.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/1.png" alt="what&#39;s a Neural Network"></p><h3 id="02-neural-network-representation"><a href="#02-neural-network-representation" class="headerlink" title="02_neural-network-representation"></a>02_neural-network-representation</h3><p>You see me draw a few pictures of neural networks. In this video, we’ll talk about exactly what those pictures means. In other words, exactly what those neural networks that we’ve been drawing represent. And we’ll start with focusing on the case of neural networks with what was called a single hidden layer. Here’s a picture of a neural network. Let’s give different parts of<br>these pictures some names.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/2.png" alt="2 layer NN"></p><p><strong>note</strong>:</p><ol><li><p>The term hidden layer refers to the fact that in the training set, the true values for these nodes in the middle are not observed. That is you don’t see what they should be in the training set. You see what the inputs are. You see what the output should be. But the things in the hidden layer are not seen in the training set. So that kind of explains the name hidden there just because you don’t see it in the training set.</p></li><li><p>One funny thing about notational conventions in neural networks is that this network that you’ve seen here is called a two layer neural network. And the reason is that when we count layers in neural networks, we don’t count the input layer. So the hidden layer is layer one and the output layer is layer two. In our notational convention, we’re calling the input layer layer zero, so technically maybe there are three layers in this neural network, because there’s the input layer, the hidden layer, and the output layer. But in conventional usage, if you read research papers and elsewhere in the course, you see people refer to this particular neural network as a two layer neural network, because we don’t count the input layer as an official layer.</p></li><li><p>the shape of the parameter $w$ between 2 layers is <strong>(the_number_of_nodes_in_output_layer, the_number_of_nodes_in_intput_layer)</strong>, that’s merely a conventional way. And the parameter $b$ is only a constant, that’s a (1, 1) matrix.</p></li></ol><h3 id="03-computing-a-neural-networks-output"><a href="#03-computing-a-neural-networks-output" class="headerlink" title="03_computing-a-neural-networks-output"></a>03_computing-a-neural-networks-output</h3><p>In the last video you saw what a single hidden layer neural network looks like in this video let’s go through the details of exactly how this neural network computers outputs what you see is that is like logistic regression the repeat of all the times.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/3.png" alt="2 layer NN"></p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/4.png" alt="vectorization 2 layer NN"></p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/5.png" alt="vectorization 2 layer NN"></p><h3 id="04-vectorizing-across-multiple-examples"><a href="#04-vectorizing-across-multiple-examples" class="headerlink" title="04_vectorizing-across-multiple-examples"></a>04_vectorizing-across-multiple-examples</h3><p>In the last video, you saw how to compute the prediction on a neural network, given a single training example. In this video, you see how to vectorize across multiple training examples. And the outcome will be quite similar to what you saw for logistic regression. Whereby stacking up different training examples in different columns of the matrix, you’d be able to take the equations you had from the previous video. And with very little modification, change them to make the neural network compute the outputs on all the examples on pretty much all at the same time. So let’s see the details on how to do that.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/6.png" alt="vectorization 2 layer NN"></p><p><strong>note</strong> : the row and column indices of the matrix $A, Z$ respectively correspond to the sequence numbers of the train examples and the nodes(units) in layers. And, the row and column indices of the matrix $X$ separately correspond to the sequence numbers of the train examples and the features of a example. Finally, the row and column indices of the matrix $W$ separately correspond to the sequence numbers of the output units and the input units in the layer.</p><h3 id="05-explanation-for-vectorized-implementation"><a href="#05-explanation-for-vectorized-implementation" class="headerlink" title="05_explanation-for-vectorized-implementation"></a>05_explanation-for-vectorized-implementation</h3><p>In the previous video, we saw how with your training examples stacked up horizontally in the matrix $X$, you can derive a vectorized implementation for propagation through your neural network. Let’s give a bit more justification for why the equations we wrote down is a correct implementation of vectorizing across multiple examples.</p><p>So let’s go through part of the propagation calculation for the few examples.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/7.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/8.png" alt=""></p><h3 id="06-activation-functions"><a href="#06-activation-functions" class="headerlink" title="06_activation-functions"></a>06_activation-functions</h3><p>When you boost a neural network, one of the choices you get to make is what activation functions use independent layers as well as at the output unit of your neural network so far we’ve just been using the sigmoid activation function but sometimes other choices can work much better let’s take a look at some of the options.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/9.png" alt=""></p><p>In the forward propagation steps for the neural network we have these two steps where we use the sigmoid function here so that sigmoid is called an <strong>activation function</strong>, so in the more general case we can have a different function G of z, $g(z)$, where G could be a nonlinear function that may not be the sigmoid function.</p><p>So for example the sigmoid function goes between 0 &amp; 1, an activation function that almost always works better than the sigmoid function is the tanh function or the hyperbolic tangent function. The <strong>tanh</strong> function or the <strong>hyperbolic tangent</strong> function is actually mathematically a shifted version of the sigmoid function so as a you know sigmoid function just like that but shift it so that it now crosses a zero zero point and rescale. So it goes to minus one and plus one and it turns out that for hidden units if you let the function G of Z, $g(z)$, be equal to $tanh(z)$. This almost always works better than the sigmoid function because with values between plus one and minus one. The mean of the activations that come out of your head in there are closer to having a zero mean and so just as sometimes when you train a learning algorithm you might Center the data and have your data have zero mean. Using a $tanh$ function instead of a $sigmoid$ function kind of has the effect of centering your data, so that the mean of the data is close to the zero rather than maybe a 0.5 and this actually makes learning for the next layer a little bit easier we’ll say more about this in the second course when we talk about optimization algorithms as well but <strong>one takeaway is that I pretty much never use the sigmoid activation function anymore, the tanh function is almost always strictly superior</strong>. <strong>The only one exception</strong> is for the output layer because if Y is either 0 or 1 then it makes sense for $\hat{y}$ to be a number that you want to output. There’s between 0 and 1 rather than between minus 1 and 1 so the one exception where I would use the sigmoid activation function is when you’re using binary classification in which case you might use the sigmoid activation function for the output layer.</p><p>Now <strong>one of the downsides of both the sigmoid function and the tanh function</strong> is that if Z is either very large or very small then the gradient of the derivative or the slope of this function becomes very small so Z is very large or Z is very small the slope of the function you know ends up being close to zero and so this can slow down gradient descent, so one of the toys that is very popular in machine learning is what’s called the <strong>rectified linear unit, ReLU</strong>. so the value function looks like the down-left function of the above slide and the formula is a equals max of 0 comma Z, $max={0, z}$ , So the derivative is 1 so long as Z is positive and derivative or the slope is 0 when Z is negative. If you’re implementing this technically the derivative when Z is exactly 0 is not well-defined but when you implement is in the computer, the odds that you get exactly the equals 0 0 0 0 0 0 0 0 0 0 it is very small, so you don’t need to worry about it. In practice you could pretend a derivative when Z is equal to 0 you can pretend is either 1 or 0 and you can work just fine athlough the fact that is not differentiable.</p><p>So here are <strong>some rules of thumb for choosing activation functions</strong>:</p><p>If your output is 0 1 value if you’re I’m using binary classification then the sigmoid activation function is very natural for the output layer and then for all other units on ReLU or the rectified linear unit is increasingly the default choice of activation function. so if you’re not sure what to use for your head in there I would just use the relu activation function that’s what you see most people using these days although sometimes people also use the tannish activation function. <strong>One advantage of the ReLU</strong> is that the derivative is equal to zero when z is negative in practice this works just fine, but there is another version of the ReLU called the <strong>leaky ReLU</strong> will give you the formula on the next slide, but instead of it being zero** when z is negative it just takes a slight slope** like the down-right of the above slide. So this is called the leaky ReLU. This usually works better than the RELU activation function although it’s just not used as much in practice. Either one should be fine although if you had to pick one I usually just use the revenue and <strong>the advantage of both the ReLU and the leaky ReLU</strong> is that for a lot of the space of Z the derivative of the activation function the slope of the activation function is very different from zero and so in practice using the ReLU activation function your new network will often learn much faster than using the tanh or the sigmoid activation function and the main reason is that on this less of this effect of the slope of the function going to zero which slows down learning and I know that for half of the range of Z the slope of relu is zero but in practice enough of your hidden units will have Z greater than zero so learning can still be quite mask for most training examples.</p><p>On the above slide, the leaky ReLU is $max={0.01z, z}$. You might say you know why is that constant 0.01 well you can also make that another parameter of the learning algorithm and some people say that works even better but I hardly see people do that so but if you feel like trying it in your application you know please feel free to do so and and you can just see how it works and how long works and stick with it if it gives you good result.</p><p>One of the themes we’ll see in deep learning is that you often have a lot of different choices in how you code your neural network ranging from number of credit units to the chosen activation function to how you initialize the parameters which we’ll see later a lot of choices like that and it turns out that is sometimes difficult to get good guidelines for exactly what will work best for your problem so so these three courses. I’ll keep on giving you a sense of what I see in the industry in terms of what’s more or less popular but for your application with your applications idiosyncrasy. <strong>It’s actually very difficult to know in advance exactly what will work best so a piece of advice would be if you’re not sure which one of these activation functions work best you know try them all and then evaluate on like a holdout validation set or like a development set which we’ll talk about later and see which one works better and then go of that</strong> and I think that by testing these different choices for your application you’d be better at future proofing your neural network architecture against the idiosyncrasy in our problem as well evolutions of the algorithms rather than you know if I were to tell you always use a ReLU activation and don’t use anything else that just may or may not apply for whatever problem you end up working on you know either either in the near future.</p><h3 id="07-why-do-you-need-non-linear-activation-functions"><a href="#07-why-do-you-need-non-linear-activation-functions" class="headerlink" title="07_why-do-you-need-non-linear-activation-functions"></a>07_why-do-you-need-non-linear-activation-functions</h3><p>Why does your nerual network need a nonlinear activation function turns out that for your new network to compute interesting functions you do need to take a nonlinear activation function unless you want.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/10.png" alt="why-do-you-need-non-linear-activation-functions"></p><p>No matter how many layers your neural network has always doing is just computing a linear activation function so you might as well not have any hidden layers some of the cases that briefly mentioned it turns out that if you have a linear activation function here and a sigmoid function here(on output layer) then this model is no more expressive than standard logistic regression without any hidden layer so I won’t bother to prove that but you could try to do so if you want but the take-home is that a linear hidden layer is more or less useless because <strong>the composition of two linear functions is itself a linear function</strong>.</p><p>there is <strong>just one place where you might use a linear activation function G of Z equals Z and that’s if you are doing machine learning on a regression problem so if Y is a real number</strong> so for example if you’re trying to predict housing prices so Y is a it’s not 0 1 but it’s a real number you know anywhere from zero dollars is a price of holes up to however expensive right house of kin I guess maybe however can be you know potentially millions of dollars so however however much houses cost in your data set but if Y takes on these real values then it might be OK to have a linear activation function here so that your output Y hat is also a real number going anywhere from minus infinity to plus infinity but then the hidden units should not use the new activation functions they could use relu or 10h or these you relu or maybe something else so the one place you might use a linear activation function others usually in the output layer but other than that using a linear activation function in a fitting layer except for some very special circumstances relating to compression that won’t want to talk about using a linear activation function is extremely rare oh and of course today actually predicting housing prices as you saw on the week 1 video because housing prices are all non-negative perhaps even then you can use a value activation function so that your outputs Y hat are all greater than or equal to 0.</p><p>so I hope that gives you a sense of why having a nonlinear activation function is a critical part of neural networks.</p><h3 id="08-derivatives-of-activation-functions"><a href="#08-derivatives-of-activation-functions" class="headerlink" title="08_derivatives-of-activation-functions"></a>08_derivatives-of-activation-functions</h3><p>When you implement back-propagation for your neural network you need to really compute the slope or the derivative of the activation functions so let’s take a look at our choices of activation functions and how you can compute the slope of these functions.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/11.png" alt="why-do-you-need-non-linear-activation-functions"></p><p>Sometimes instead of writing this thing $\frac{dg(z)}{dz}$, the shorthand for the derivative is G prime of Z, $g’(z)$ . so G prime of Z in calculus the the little dash on top is called <strong>prime</strong> but so G prime of Z is a shorthand for the in calculus for the derivative of the function of G with respect to the input variable Z and then in a new network we have $a = g(z)$, right equals this then this formula also simplifies to $a(1-a)$.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/12.png" alt="why-do-you-need-non-linear-activation-functions"></p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/13.png" alt="why-do-you-need-non-linear-activation-functions"></p><h3 id="09-gradient-descent-for-neural-networks"><a href="#09-gradient-descent-for-neural-networks" class="headerlink" title="09_gradient-descent-for-neural-networks"></a>09_gradient-descent-for-neural-networks</h3><p>All right I think that’s be an exciting video in this video you see how to implement gradient descent for your neural network with one hidden layer in this video I’m going to just give you the equations you need to implement in order to get back propagation of the gradient descent working and then in the video after this one I’ll give some more intuition about why these particular equations are the accurate equations or the correct equations for computing the gradients you need for your neural network.</p><p>In logistic regression, what we want to do is to modify the parameters, W and B, in order to reduce this loss.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/15.png" alt=""></p><p>$$da = \frac{\partial{L}}{\partial{a}} =\frac{\partial \left{ {-(ylog(a)+(1-y)log(1-a))} \right} }{\partial{a}} = -\frac{y}{a} + \frac{1-y}{1-a} $$</p><p>$$dz=\frac{\partial{L}}{\partial{z}}=\frac{\partial{L}}{\partial{a}}\cdot \frac{\partial{a}}{\partial{z}} = \left(-\frac{y}{a} + \frac{1-y}{1-a}\right) \cdot a(1-a) = a - y$$</p><p>$$dw_1=\frac{\partial{L}}{\partial{w_1}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_1}} = x_1\cdot dz = x_1(a-y)$$</p><p>$$dw_2=\frac{\partial{L}}{\partial{w_2}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_2}} = x_2\cdot dz = x_2(a-y)$$</p><p>$$db=\frac{\partial{L}}{\partial{b}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{b}} = 1 \cdot dz = a - y$$</p><p>$$w_1 := w_1 - \alpha dw_1$$</p><p>$$w_2 := w_2 - \alpha dw_2$$</p><p>$$b := b - \alpha db$$</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/14.png" alt="why-do-you-need-non-linear-activation-functions"></p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/15.png" alt="why-do-you-need-non-linear-activation-functions"></p><h3 id="11-random-initialization"><a href="#11-random-initialization" class="headerlink" title="11_random-initialization"></a>11_random-initialization</h3><p>When you change your neural network, it’s important to initialize the weights randomly. For logistic regression, it was okay to initialize the weights to zero. But for a neural network of initialize the weights to parameters to all zero and then applied gradient descent, it won’t work. Let’s see why.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/16.png" alt="why-do-you-need-non-linear-activation-functions"></p><p>So you have here two input features, so n0=2, and two hidden units, so n1=2. And so the matrix associated with the hidden layer, w 1, is going to be two-by-two. Let’s say that you initialize it to all 0s, so 0 0 0 0, two-by-two matrix. And let’s say B1 is also equal to 0 0. It turns out initializing the bias terms b to 0 is actually okay, but initializing w to all 0s is a problem. So the problem with this formalization is that for any example you give it, you’ll have that a1,1 and a1,2, will be equal, right? So this activation and this activation will be the same, because both of these hidden units are computing exactly the same function. And then, when you compute backpropagation, it turns out that dz11 and dz12 will also be the same colored by symmetry, right? Both of these hidden unit will initialize the same way. Technically, for what I’m saying, I’m assuming that the outgoing weights or also identical. So that’s w2 is equal to 0 0. But if you initialize the neural network this way, then this hidden unit and this hidden unit are completely identical. Sometimes you say they’re completely symmetric, which just means that they’re completing exactly the same function. And by kind of a proof by induction, it turns out that after every single iteration of training your two hidden units are still computing exactly the same function. Since [INAUDIBLE] show that dw will be a matrix that looks like this. Where every row takes on the same value. So we perform a weight update. So when you perform a weight update, w1 gets updated as w1- alpha times dw. You find that w1, after every iteration, will have the first row equal to the second row. So it’s possible to construct a proof by induction that if you initialize all the ways, all the values of w to 0, then because both hidden units start off computing the same function. And both hidden the units have the same influence on the output unit, then after one iteration, that same statement is still true, the two hidden units are still symmetric. And therefore, by induction, after two iterations, three iterations and so on, no matter how long you train your neural network, both hidden units are still computing exactly the same function. And so in this case, there’s really no point to having more than one hidden unit. Because they are all computing the same thing. And of course, for larger neural networks, let’s say of three features and maybe a very large number of hidden units, a similar argument works to show that with a neural network like this. [INAUDIBLE] drawing all the edges, if you initialize the weights to zero, then all of your hidden units are symmetric. And no matter how long you’re upgrading the center, all continue to compute exactly the same function. So that’s not helpful, because you want the different hidden units to compute different functions.</p><p>The solution to this is to initialize your parameters randomly. So here’s what you do.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/17.png" alt="why-do-you-need-non-linear-activation-functions"></p><p>You can set w1 = np.random.randn. This generates a gaussian random variable (2,2). And then usually, you multiply this by very small number, such as 0.01. So you initialize it to very small random values. And then b, <strong>it turns out that b does not have the symmetry problem, what’s called the symmetry breaking problem</strong>. So it’s okay to initialize b to just zeros. Because so long as w is initialized randomly, you start off with the different hidden units computing different things. And so you no longer have this symmetry breaking problem. And then similarly, for w2, you’re going to initialize that randomly. And b2, you can initialize that to 0. <strong>So you might be wondering, where did the constant come from and why is it 0.01? Why not put the number 100 or 1000? Turns out that we usually prefer to initialize the weights to very small random values. Because if you are using a tanh or sigmoid activation function, or the other sigmoid, even just at the output layer. If the weights are too large, then when you compute the activation values, remember that z[1]=w1 x + b. And then a1 is the activation function applied to z1. So if w is very big, z will be very, or at least some values of z will be either very large or very small. And so in that case, you’re more likely to end up at these fat parts of the tanh function or the sigmoid function, where the slope or the gradient is very small. Meaning that gradient descent will be very slow. So learning was very slow. So just a recap, if w is too large, you’re more likely to end up even at the very start of training, with very large values of z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don’t have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue. But if you’re doing binary classification, and your output unit is a sigmoid function, then you just don’t want the initial parameters to be too large. So that’s why multiplying by 0.01 would be something reasonable to try, or any other small number</strong>. And same for w2, right? This can be random.random. I guess this would be 1 by 2 in this example, times 0.01. Missing an s there. So finally, it turns out that sometimes they can be better constants than 0.01. When you’re training a neural network with just one hidden layer, it is a relatively shallow neural network, without too many hidden layers. Set it to 0.01 will probably work okay. But when you’re training a very very deep neural network, then you might want to pick a different constant than 0.01. And in next week’s material, we’ll talk a little bit about how and when you might want to choose a different constant than 0.01. But either way, it will usually end up being a relatively small number. So that’s it for this week’s videos. You now know how to set up a neural network of a hidden layer, initialize the parameters, make predictions using. As well as compute derivatives andimplement gradient descent, using backprop.</p><p>So that,you should be able to do the quizzes, as well as this week’s programming exercises. Best of luck with that. I hope you have fun with the problem exercise, and look forward to seeing you in the week four materials.</p>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/02/greek-letters/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/02/greek-letters/" class="post-title-link" itemprop="url">Greek Letters</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-02 18:06:02" itemprop="dateCreated datePublished" datetime="2018-02-02T18:06:02+05:30">2018-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:33" itemprop="dateModified" datetime="2020-04-06T20:25:33+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>0</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <img src="https://upload.wikimedia.org/wikipedia/en/3/37/Greek_ext.png">
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/02/02_neural-networks-basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/02/02_neural-networks-basics/" class="post-title-link" itemprop="url">02_logistic-regression-as-a-neural-network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-02 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-02T00:00:00+05:30">2018-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:18" itemprop="dateModified" datetime="2020-04-06T20:25:18+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>8.7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>8 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the 2nd week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p><h2 id="01-logistic-regression-as-a-neural-network"><a href="#01-logistic-regression-as-a-neural-network" class="headerlink" title="01_logistic-regression-as-a-neural-network"></a>01_logistic-regression-as-a-neural-network</h2><h3 id="01-binary-classification"><a href="#01-binary-classification" class="headerlink" title="01_binary-classification"></a>01_binary-classification</h3><h4 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h4><p>In a binary classification problem, the result is a discrete value output. For example</p><ul><li>account hacked (1) or compromised (0)</li><li>a tumor malign (1) or benign (0)</li></ul><p><strong>Example: Cat vs Non-Cat</strong><br>The goal is to train a classifier that the input is an image represented by a feature vector, $x$, and predicts whether the corresponding label $y$ is 1 or 0. In this case, whether this is a cat image (1) or a non-cat image (0).</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/1.png" alt=""></p><p>An image is store in the computer in three separate matrices corresponding to the Red, Green, and Blue color channels of the image. The three matrices have the same size as the image, for example, the resolution of the cat image is 64 pixels X 64 pixels, the three matrices (RGB) are 64 X 64 each.<br>The value in a cell represents the pixel intensity which will be used to create a feature vector of ndimension. In pattern recognition and machine learning, a feature vector represents an object, in this case, a cat or no cat.<br>To create a feature vector, $x$, the pixel intensity values will be “unroll” or “reshape” for each color. The dimension of the input feature vector $x$ is $ n_x = 64 \times 64 \times 3 = 12 288.$<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/2.png" alt=""></p><h4 id="notation"><a href="#notation" class="headerlink" title="notation"></a>notation</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/3.png" alt=""></p><h3 id="02-Logistic-Regression"><a href="#02-Logistic-Regression" class="headerlink" title="02_Logistic Regression"></a>02_Logistic Regression</h3><p>Logistic regression is a learning algorithm used in a supervised learning problem when the output $y$ are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data.</p><h4 id="Example-Cat-vs-No-cat"><a href="#Example-Cat-vs-No-cat" class="headerlink" title="Example: Cat vs No - cat"></a>Example: Cat vs No - cat</h4><p>Given an image represented by a feature vector $x$, the algorithm will evaluate the probability of a cat being in that image.</p><p>$$\text{Civen }x, \hat{y}=P(y=1|x), \text{where } 0 \le \hat{y} \le 1$$</p><p>The parameters used in Logistic regression are:<br>• The input features vector: $x ∈ ℝ^{n_x}$, where $n_x$ is the number of features<br>• The training label: $y ∈ 0,1$<br>• The weights: $w ∈ ℝ^{n_x}$ , where $n_x$ is the number of features<br>• The threshold: $b ∈ ℝ$<br>• The output: $\hat{y} = \sigma(w^Tx+b)$<br>• Sigmoid function: $s = \sigma(w^Tx+b) = \sigma(z)= \frac{1}{1+e^{-z}}$</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/5.png" alt=""></p><p>$(w^Tx +b )$ is a linear function $(ax + b)$, but since we are looking for a probability constraint between [0,1], the sigmoid function is used. The function is bounded between [0,1] as shown in the graph above.<br>Some observations from the graph:<br>• If $z$ is a large positive number, then $\sigma(z) = 1$<br>• If $z$ is small or large negative number, then $\sigma(z) = 0$<br>• If $z$ = 0, then $\sigma(z) = 0.5$</p><h4 id="notation-1"><a href="#notation-1" class="headerlink" title="notation"></a>notation</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/4.png" alt=""></p><h3 id="03-logistic-regression-cost-function"><a href="#03-logistic-regression-cost-function" class="headerlink" title="03_logistic-regression-cost-function"></a>03_logistic-regression-cost-function</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/6.png" alt=""></p><h3 id="04-gradient-descent"><a href="#04-gradient-descent" class="headerlink" title="04_gradient-descent"></a>04_gradient-descent</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/7.png" alt=""></p><h3 id="05-06-derivatives"><a href="#05-06-derivatives" class="headerlink" title="05_06_derivatives"></a>05_06_derivatives</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/8.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/9.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/10.png" alt=""></p><h3 id="07-computation-graph"><a href="#07-computation-graph" class="headerlink" title="07_computation-graph"></a>07_computation-graph</h3><p>You’ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives. The computation graph explains why it is organized this way.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/11.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/12.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/13.png" alt=""></p><h3 id="09-logistic-regression-gradient-descent"><a href="#09-logistic-regression-gradient-descent" class="headerlink" title="09_logistic-regression-gradient-descent"></a>09_logistic-regression-gradient-descent</h3><p>Welcome back. In this video, we’ll talk about how to compute derivatives for you to implement gradient descent for logistic regression. <strong>The key takeaways will be what you need to implement. That is, the key equations you need in order to implement gradient descent for logistic regression</strong>. In this video, I want to do this computation using the computation graph. I have to admit, using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about fully-fledged neural networks. To that, let’s dive into gradient descent for logistic regression.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/14.png" alt=""><br>In logistic regression, what we want to do is to modify the parameters, W and B, in order to reduce this loss.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/15.png" alt=""></p><p>$$da = \frac{\partial{L}}{\partial{a}} =\frac{\partial \left{ {-(ylog(a)+(1-y)log(1-a))} \right} }{\partial{a}} = -\frac{y}{a} + \frac{1-y}{1-a} $$</p><p>$$dz=\frac{\partial{L}}{\partial{z}}=\frac{\partial{L}}{\partial{a}}\cdot \frac{\partial{a}}{\partial{z}} = \left(-\frac{y}{a} + \frac{1-y}{1-a}\right) \cdot a(1-a) = a - y$$</p><p>$$dw_1=\frac{\partial{L}}{\partial{w_1}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_1}} = x_1\cdot dz = x_1(a-y)$$</p><p>$$dw_2=\frac{\partial{L}}{\partial{w_2}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_2}} = x_2\cdot dz = x_2(a-y)$$</p><p>$$db=\frac{\partial{L}}{\partial{b}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{b}} = 1 \cdot dz = a - y$$</p><p>$$w_1 := w_1 - \alpha dw_1$$</p><p>$$w_2 := w_2 - \alpha dw_2$$</p><p>$$b := b - \alpha db$$</p><h3 id="10-gradient-descent-on-m-examples"><a href="#10-gradient-descent-on-m-examples" class="headerlink" title="10_gradient-descent-on-m-examples"></a>10_gradient-descent-on-m-examples</h3><p>in a previous video you saw how to compute derivatives and implement gradient descent with respect to just one training example for religious regression now we want to do it for m training examples.</p><h4 id="one-single-step-gradient-descent"><a href="#one-single-step-gradient-descent" class="headerlink" title="one single step gradient descent"></a>one single step gradient descent</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/16.png" alt="one single step gradient descent"></p><h2 id="02-python-and-vectorization"><a href="#02-python-and-vectorization" class="headerlink" title="02_python-and-vectorization"></a>02_python-and-vectorization</h2><h3 id="01-vectorization"><a href="#01-vectorization" class="headerlink" title="01_vectorization"></a>01_vectorization</h3><p>Welcome back. Vectorization is basically the art of getting rid of explicit folders in your code. In the deep learning era safety in deep learning in practice, you often find yourself training on relatively large data sets, because that’s when deep learning algorithms tend to shine. And so, it’s important that your code very quickly because otherwise, if it’s running on a big data set, your code might take a long time to run then you just find yourself waiting a very long time to get the result. So in the deep learning era, I think the ability to perform vectorization has become a key skill.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/18.png" alt="what is vectorization"></p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/17.png" alt="a example of the difference of run time between vectorization implementation and non-vectorization implementation"><br>Yeah. Vectorize version 1.5 milliseconds seconds and the four loop. So 481 milliseconds, again, about <strong>300 times slower</strong> to do the explicit four loop. If the engine x slows down, it’s the difference between your code taking maybe one minute to run versus taking say five hours to run. And when you are implementing deep learning algorithms, you can really get a result back faster. It will be much faster if you vectorize your code. Some of you might have heard that a lot of <strong>scaleable deep learning implementations</strong> are done on a GPU or a graphics processing unit. But all the demos I did just now in the Jupiter notebook where actually on the CPU. And it turns out that both GPU and CPU have parallelization instructions. They’re sometimes called <strong>SIMD instructions</strong>. This stands for a <strong>single instruction multiple data</strong>. But what this basically means is that, if you use built-in functions such as this np.function or other functions that don’t require you explicitly implementing a for loop. It enables Phyton Pi to take much better advantage of parallelism to do your computations much faster. And this is true both computations on CPUs and computations on GPUs. It’s just that GPUs are remarkably good at these SIMD calculations but CPU is actually also not too bad at that. Maybe just not as good as GPUs. You’re seeing how vectorization can significantly speed up your code. <strong>The rule of thumb to remember is whenever possible, avoid using explicit four loops.</strong></p><h3 id="02-more-vectorization-examples"><a href="#02-more-vectorization-examples" class="headerlink" title="02_more-vectorization-examples"></a>02_more-vectorization-examples</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/20.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/21.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/22.png" alt=""></p><h3 id="03-vectorizing-logistic-regression"><a href="#03-vectorizing-logistic-regression" class="headerlink" title="03_vectorizing-logistic-regression"></a>03_vectorizing-logistic-regression</h3><p>We have talked about how vectorization lets you speed up your code significantly. In this video, we’ll talk about how you can vectorize the implementation of logistic regression, so they can process an entire training set, that is implement a single elevation of grading descent with respect to an entire training set without using even a single explicit for loop. I’m super excited about this technique, and when we talk about neural networks later without using even a single explicit for loop.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/23.png" alt="vectorization implementation of logistic regression of forward of propagation"></p><p>Here are details of <a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">python broadcasting</a></p><h3 id="04-vectorizing-logistic-regressions-gradient-output"><a href="#04-vectorizing-logistic-regressions-gradient-output" class="headerlink" title="04_vectorizing-logistic-regressions-gradient-output"></a>04_vectorizing-logistic-regressions-gradient-output</h3><p>In the previous video, you saw how you can use vectorization to compute their predictions. The lowercase a’s for an entire training set O at the same time. In this video, you see <strong>how you can use vectorization to also perform the gradient computations for all M training samples</strong>. Again, all sort of at the same time. And then at the end of this video, we’ll put it all together and show how you can derive a very efficient implementation of logistic regression.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/24.png" alt="vectorizing-logistic-regressions-gradient-output"></p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/25.png" alt="vectorizing-logistic-regressions-gradient-output"></p><h3 id="05-broadcasting-in-python"><a href="#05-broadcasting-in-python" class="headerlink" title="05_broadcasting-in-python"></a>05_broadcasting-in-python</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/26.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/29.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/27.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/28.png" alt=""></p><p><strong>Summary: Python or Numpy automatically expands two arrays or numbers to the same dimensions and operate element-wise.</strong></p><h3 id="06-a-note-on-python-numpy-vectors"><a href="#06-a-note-on-python-numpy-vectors" class="headerlink" title="06_a-note-on-python-numpy-vectors"></a>06_a-note-on-python-numpy-vectors</h3><p>The ability of python to allow you to use broadcasting operations and more generally, <strong>the great flexibility of the python numpy program language is, I think, both a strength as well as a weakness of the programming language</strong>. I think it’s a strength because they create expressivity of the language. A great flexibility of the language lets you get a lot done even with just a single line of code. But there’s also weakness because with broadcasting and <strong>this great amount of flexibility, sometimes it’s possible you can introduce very subtle bugs or very strange looking bugs</strong>, if you’re not familiar with all of the intricacies of how broadcasting and how features like broadcasting work. <strong>For example, if you take a column vector and add it to a row vector, you would expect it to throw up a dimension mismatch or type error or something. But you might actually get back a matrix as a sum of a row vector and a column vector. So there is an internal logic to these strange effects of Python.</strong> But if you’re not familiar with Python, I’ve seen some students have very strange, very hard to find bugs. So what I want to do in this video is share with you some couple tips and tricks that have been very useful for me to eliminate or simplify and eliminate all the strange looking bugs in my own code. <strong>And I hope that with these tips and tricks, you’ll also be able to much more easily write bug-free, python and numpy code</strong>.</p><p>To illustrate one of the less intuitive effects of Python-Numpy, especially how you construct vectors in Python-Numpy, let me do a <strong>quick demo</strong>.</p><h4 id="one-rank-array"><a href="#one-rank-array" class="headerlink" title="one rank array"></a>one rank array</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/30.png" alt=""></p><h4 id="practical-tips"><a href="#practical-tips" class="headerlink" title="practical tips"></a>practical tips</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/31.png" alt=""></p><h3 id="07-quick-tour-of-jupyter-ipython-notebooks"><a href="#07-quick-tour-of-jupyter-ipython-notebooks" class="headerlink" title="07_quick-tour-of-jupyter-ipython-notebooks"></a>07_quick-tour-of-jupyter-ipython-notebooks</h3><p>With everything you’ve learned, you’re just about ready to tackle your first programming assignment. Before you do that, let me just give you a quick tour of iPython notebooks in Coursera.</p><p>Please see <a href="">the video</a> to get details.</p><h3 id="08-explanation-of-logistic-regression-cost-function-optional"><a href="#08-explanation-of-logistic-regression-cost-function-optional" class="headerlink" title="08_explanation-of-logistic-regression-cost-function-optional"></a>08_explanation-of-logistic-regression-cost-function-optional</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/32.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/33.png" alt=""></p>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/01/01_introduction-to-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/01/01_introduction-to-deep-learning/" class="post-title-link" itemprop="url">01_introduction-to-deep-learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-01 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-01T00:00:00+05:30">2018-02-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:13" itemprop="dateModified" datetime="2020-04-06T20:25:13+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>2.7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the first week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p><h2 id="01-introduction-to-deep-learning"><a href="#01-introduction-to-deep-learning" class="headerlink" title="01_introduction-to-deep-learning"></a>01_introduction-to-deep-learning</h2><h3 id="01-What-is-neural-network"><a href="#01-What-is-neural-network" class="headerlink" title="01_What is neural network?"></a>01_What is neural network?</h3><p>It is a powerful learning algorithm inspired by how the brain works.</p><h4 id="Example-1-–-single-neural-network"><a href="#Example-1-–-single-neural-network" class="headerlink" title="Example 1 – single neural network"></a>Example 1 – single neural network</h4><p>Given data about the size of houses on the real estate market and you want to fit a function that will predict their price. It is a linear regression problem because the price as a function of size is a continuous output.<br>We know the prices can never be negative so we are creating a function called <strong>Rectified Linear Unit</strong> (ReLU) which starts at zero.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/1.png" alt="example of a neuron"><br>The input is the size of the house (x)<br>The output is the price (y)<br>The “neuron” implements the function ReLU (blue line)</p><h4 id="Example-2-–-Multiple-neural-network"><a href="#Example-2-–-Multiple-neural-network" class="headerlink" title="Example 2 – Multiple neural network"></a>Example 2 – Multiple neural network</h4><p>The price of a house can be affected by other features such as size, number of bedrooms, zip code andwealth. The role of the neural network is to predicted the price and it will automatically generate the hidden units. We only need to give the inputs x and the output y.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/2.png" alt="example of simple neural network"></p><h3 id="02-supervised-learning-with-neural-networks"><a href="#02-supervised-learning-with-neural-networks" class="headerlink" title="02_supervised-learning-with-neural-networks"></a>02_supervised-learning-with-neural-networks</h3><h4 id="Supervised-learning-for-Neural-Network"><a href="#Supervised-learning-for-Neural-Network" class="headerlink" title="Supervised learning for Neural Network"></a>Supervised learning for Neural Network</h4><p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.<br>Supervised learning problems are categorized into “<strong>regression</strong>“ and “<strong>classification</strong>“ problems. In a regression problem, we are trying to predict results within a <strong>continuous outpu</strong>t, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a <strong>discrete output</strong>. In other words, we are trying to map input variables into discrete categories.<br>Here are some examples of supervised learning.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/3.png" alt="some examples of supervised learning"></p><p>There are different types of neural network, for example <strong>Convolution Neural Network</strong> (CNN) used often for image application and <strong>Recurrent Neural Network</strong> (RNN) used for one-dimensional sequence data such as translating English to Chinses or a temporal component such as text transcript. As for the autonomous driving, it is a hybrid neural network architecture.</p><h4 id="Structured-vs-unstructured-data"><a href="#Structured-vs-unstructured-data" class="headerlink" title="Structured vs unstructured data"></a>Structured vs unstructured data</h4><p>Structured data refers to things that has a defined meaning such as price, age whereas unstructured data refers to thing like pixel, raw audio, text.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/4.png" alt="Structured data vs Unstructured data"></p><h3 id="03-why-is-deep-learning-taking-off"><a href="#03-why-is-deep-learning-taking-off" class="headerlink" title="03_why-is-deep-learning-taking-off"></a>03_why-is-deep-learning-taking-off</h3><h4 id="Why-is-deep-learning-taking-off"><a href="#Why-is-deep-learning-taking-off" class="headerlink" title="Why is deep learning taking off?"></a>Why is deep learning taking off?</h4><p>Deep learning is taking off due to a large amount of data available through the digitization of the society, faster computation and innovation in the development of neural network algorithm.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/5.png" alt=""></p><p>Two things have to be considered to get to the high level of performance:</p><ol><li>Being able to train a big enough neural network</li><li>Huge amount of labeled data</li></ol><p>The process of training a neural network is iterative.</p><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/6.png" alt=""></p><p>It could take a good amount of time to train a neural network, which affects your productivity. Faster computation helps to iterate and improve new algorithm.</p><h3 id="04-about-this-course"><a href="#04-about-this-course" class="headerlink" title="04_about-this-course"></a>04_about-this-course</h3><h4 id="Outline-of-this-Course"><a href="#Outline-of-this-Course" class="headerlink" title="Outline of this Course"></a>Outline of this Course</h4><ul><li>Week 1: Introduction</li><li>Week 2: Basics of Neural Network programming</li><li>Week 3: One hidden layer Neural Networks</li><li>Week 4: Deep Neural Networks</li></ul>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Karan"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Karan</p>
  <div class="site-description" itemprop="description">Refuse to Fall</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/yourname" title="YouTube → https:&#x2F;&#x2F;youtube.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/yourname" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="/www.massivefile.com" title="www.massivefile.com">DataBases</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Karan</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">2.1m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">32:08</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


    </div>
</body>
</html>
