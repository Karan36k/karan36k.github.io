<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">

<script>
    (function(){
        if(''){
                         If (prompt('Please enter the article password') !== ''){
                                 Alert('Password error!');
                history.back();
            }
        }
    })();
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"snakecoding.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":"flat","style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":false},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Refuse to Fall">
<meta property="og:type" content="website">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://snakecoding.com/page/7/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="Refuse to Fall">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Karan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://snakecoding.com/page/7/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Machine Learning</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Machine Learning</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">17</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">91</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/yourname" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/04/01/01_ml-strategy-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/04/01/01_ml-strategy-1/" class="post-title-link" itemprop="url">01_ml-strategy-1</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-04-01 00:00:00" itemprop="dateCreated datePublished" datetime="2018-04-01T00:00:00+05:30">2018-04-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>71k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:05</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note after studying the course of the first week  <a href="https://www.coursera.org/learn/machine-learning-projects" target="_blank" rel="noopener">Structuring Machine Learning Projects</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="01-introduction-to-ml-strategy"><a href="#01-introduction-to-ml-strategy" class="headerlink" title="01_introduction-to-ml-strategy"></a>01_introduction-to-ml-strategy</h2><h3 id="01-why-ml-strategy"><a href="#01-why-ml-strategy" class="headerlink" title="01_why-ml-strategy"></a>01_why-ml-strategy</h3><p>Hi, welcome to this course on how to structure your machine learning project, that is on machine learning strategy. I hope that through this course you will learn how to much more quickly and efficiently get your machine learning systems working. So, what is machine learning strategy.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/1.png" alt=""><br>Let’s start with a motivating example. Let’s say you are working on your cat cost file. And after working it for some time, you’ve gotten your system to have 90% accuracy, but this isn’t good enough for your application. You might then have a lot of ideas as to how to improve your system. For example, you might think well let’s collect more data, more training data. Or you might say, maybe your training set isn’t diverse enough yet, you should collect images of cats in more diverse poses, or maybe a more diverse set of negative examples. Well maybe you want to train the algorithm longer with gradient descent. Or maybe you want to try a different optimization algorithm, like the Adam optimization algorithm. Or maybe trying a bigger network or a smaller network or maybe you want to try to dropout or maybe L2 regularization. Or maybe you want to change the network architecture such as changing activation functions, changing the number of hidden units and so on and so on. </p>
<p><strong>When trying to improve a deep learning system, you often have a lot of ideas or things you could try. And the problem is that if you choose poorly, it is entirely possible that you end up spending six months charging in some direction only to realize after six months that that didn’t do any good</strong>. For example, I’ve seen some teams spend literally six months collecting more data only to realize after six months that it barely improved the performance of their system. So, assuming you don’t have six months to waste on your problem, won’t it be nice if you had quick and effective ways to figure out which of all of these ideas and maybe even other ideas, are worth pursuing and which ones you can safely discard. </p>
<p><strong>So what I hope to do in this course is teach you a number of strategies, that is, ways of analyzing a machine learning problem that will point you in the direction of the most promising things to try. What I will do in this course also is share with you a number of lessons I’ve learned through building and shipping large number of deep learning products</strong>. And I think these materials are actually quite unique to this course. I don’t see a lot of these ideas being taught in universities’ deep learning courses for example. It turns out also that machine learning strategy is changing in the era of deep learning because the things you could do are now different with deep learning algorithms than with previous generation of machine learning algorithms. I hope that these ideas will help you become much more effective at getting your deep learning systems to work.</p>
<h3 id="02-orthogonalization"><a href="#02-orthogonalization" class="headerlink" title="02_orthogonalization"></a>02_orthogonalization</h3><p>One of the challenges with building machine learning systems is that there’s so many things you could try, so many things you could change. Including, for example, so many hyperparameters you could tune. One of the things I’ve noticed is about the most effective machine learning people is they’re very clear-eyed about what to tune in order to try to achieve one effect. This is a process we call orthogonalization. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/2.png" alt=""><br>Let me tell you what I mean. Here’s a picture of an old school television, with a lot of knobs that you could tune to adjust the picture in various ways. So for these old TV sets, maybe there was one knob to adjust how <strong>tall</strong> vertically your image is and another knob to adjust how <strong>wide</strong> it is. Maybe another knob to adjust how <strong>trapezoidal</strong> it is, another knob to adjust how much to move the picture <strong>left and right</strong>, another one to adjust how much the picture’s <strong>rotated</strong>, and so on. And what TV designers had spent a lot of time doing was to build the circuitry, really often analog circuitry back then, to make sure each of the knobs had a relatively interpretable function. Such as one knob to tune this  (height), one knob to tune this (width), one knob to tune this (trapezoidal), and so on. <strong>In contrast, imagine if you had a knob that</strong> tunes 0.1 x how tall the image is, + 0.3 x how wide the image is,- 1.7 x how trapezoidal the image is, + 0.8 times the position of the image on the horizontal axis, and so on(that is conbining all various functions). <strong>If you tune this knob, then the height of the image, the width of the image, how trapezoidal it is, how much it shifts, it all changes all at the same time. If you have a knob like that, it’d be almost impossible to tune the TV so that the picture gets centered in the display area. So in this context, orthogonalization refers to that the TV designers had designed the knobs so that each knob kind of does only one thing. And this makes it much easier to tune the TV, so that the picture gets centered where you want it to be</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/3.png" alt=""><br>Here’s another example of orthogonalization. If you think about learning to drive a car, a car has three main controls, which are steering, the steering wheel decides how much you go left or right, acceleration, and braking. <strong>So these three controls, or really one control for steering and another two controls for your speed. it makes it relatively interpretable, what your different actions through different controls will do to your car</strong>. But now imagine if someone were to build a car so that there was a joystick, where one axis of the joystick controls 0.3 x your steering angle,- 0.8 x your speed. And you had a different control that controls 2 x the steering angle, + 0.9 x the speed of your car. In theory, by tuning these two knobs, you could get your car to steer at the angle and at the speed you want. But it’s much harder than if you had just one single control for controlling the steering angle, and a separate, distinct set of controls for controlling the speed. <strong>So the concept of orthogonalization refers to that, if you think of one dimension of what you want to do as controlling a steering angle, and another dimension as controlling your speed. Then you want one knob to just affect the steering angle as much as possible, and another knob, in the case of the car, is really acceleration and braking, that controls your speed. But if you had a control that mixes the two together, like a control like this one that affects both your steering angle and your speed, something that changes both at the same time, then it becomes much harder to set the car to the speed and angle you want.</strong> And by having orthogonal, orthogonal means at <strong>90 degrees</strong> to each other. By having orthogonal controls that are ideally aligned with the things you actually want to control, it makes it much easier to tune the knobs you have to tune. To tune the steering wheel angle, and your accelerator, your braking, to get the car to do what you want. <strong>So how does this relate to machine learning?</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/4.png" alt=""><br><strong>For a supervised learning system to do well, you usually need to tune the knobs of your system to make sure that four things hold true. First, is that you usually have to make sure that you’re at least doing well on the training set. So performance on the training set needs to pass some acceptability assessment.</strong> For some applications, this might mean doing comparably to human level performance. But this will depend on your application, and we’ll talk more about comparing to human level performance next week. But after doing well on the training sets, you then hope that this leads to also doing well on the dev set. And you then hope that this also does well on the test set. And finally, you hope that doing well on the test set on the cost function results in your system performing in the real world. So you hope that this resolves in happy cat picture app users, for example. </p>
<p><strong>So to relate back to the TV tuning example, if the picture of your TV was either too wide or too narrow, you wanted one knob to tune in order to adjust that. You don’t want to have to carefully adjust five different knobs, which also affect different things. You want one knob to just affect the width of your TV image. So in a similar way, if your algorithm is not fitting the training set well on the cost function, you want one knob, yes, that’s my attempt to draw a knob. Or maybe one specific set of knobs that you can use, to make sure you can tune your algorithm to make it fit well on the training set.</strong> So the knobs you use to tune this are, you might train a bigger network. Or you might switch to a better optimization algorithm, like the Adam optimization algorithm, and so on, into some other options we’ll discuss later this week and next week. In contrast, if you find that the algorithm is not fitting the dev set well, then there’s a separate set of knobs. Yes, that’s my not very artistic rendering of another knob, you want to have a distinct set of knobs to try. So for example, if your algorithm is not doing well on the dev set, it’s doing well on the training set but not on the dev set, then you have a set of knobs around regularization that you can use to try to make it satisfy the second criteria. So the analogy is, now that you’ve tuned the width of your TV set, if the height of the image isn’t quite right, then you want a different knob in order to tune the height of the TV image. And you want to do this hopefully without affecting the width of your TV image too much. And getting a bigger training set would be another knob you could use, that helps your learning algorithm generalize better to the dev set. Now, having adjusted the width and height of your TV image, <strong>well, what if it doesn’t meet the third criteria? What if you do well on the dev set but not on the test set? If that happens, then the knob you tune is, you probably want to get a bigger dev set. Because if it does well on the dev set but not the test set, it probably means you’ve overtuned to your dev set, and you need to go back and find a bigger dev set. And finally, if it does well on the test set, but it isn’t delivering to you a happy cat picture app user, then what that means is that you want to go back and change either the dev set or the cost function. Because if doing well on the test set according to some cost function doesn’t correspond to your algorithm doing what you need it to do in the real world, then <em>it means that either your dev test set distribution isn’t set correctly, or your cost function isn’t measuring the right thing</em>.</strong> I know I’m going over these examples quite quickly, but we’ll go much more into detail on these specific knobs later this week and next week. So if you aren’t following all the details right now, don’t worry about it. But I want to give you a sense of this orthogonalization process, that you want to be very clear about which of these maybe four issues, the different things you could tune, are trying to address. </p>
<p>And when I train a neural network, I tend not to use early stopping. It’s not a bad technique, quite a lot of people do it. But I personally find early stopping difficult to think about. Because this is an op that simultaneously affects how well you fit the training set, because if you stop early, you fit the training set less well. It also simultaneously is often done to improve your dev set performance. So this is one knob that is less orthogonalized, because it simultaneously affects two things. It’s like a knob that simultaneously affects both the width and the height of your TV image. And it doesn’t mean that it’s bad, not to use, you can use it if you want. But when you have more orthogonalized controls, such as these other ones that I’m writing down here, then it just makes the process of tuning your network much easier. </p>
<p>So I hope that gives you a sense of what orthogonalization means. Just like when you look at the TV image, it’s nice if you can say, my TV image is too wide, so I’m going to tune this knob, or it’s too tall, so I’m going to tune that knob, or it’s too trapezoidal, so I’m going to have to tune that knob. In machine learning, it’s nice if you can look at your system and say, this piece of it is wrong. It does not do well on the training set, it does not do well on the dev set, it does not do well on the test set, or it’s doing well on the test set but just not in the real world. <strong>But figure out exactly what’s wrong, and then have exactly one knob, or a specific set of knobs that helps to just solve that problem that is limiting the performance of machine learning system. So what we’re going to do this week and next week is go through how to diagnose what exactly is the bottleneck to your system’s performance. As well as identify the specific set of knobs you could use to tune your system to improve that aspect of its performance.</strong> So let’s start going more into the details of this process. </p>
<h4 id="Orthogonalization"><a href="#Orthogonalization" class="headerlink" title="Orthogonalization"></a>Orthogonalization</h4><p>Orthogonalization or orthogonality is a system design property that assures that modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system. It becomes easier to verify the algorithms independently from one another, it reduces testing and development time. When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.</p>
<ol>
<li>Fit training set well in cost function<ul>
<li>If it doesn’t fit well, the use of a bigger neural network or switching to a better optimization algorithm might help.</li>
</ul>
</li>
<li>Fit development set well on cost function<ul>
<li>If it doesn’t fit well, regularization or using bigger training set might help.</li>
</ul>
</li>
<li>Fit test set well on cost function<ul>
<li>If it doesn’t fit well, the use of a bigger development set might help</li>
</ul>
</li>
<li>Performs well in real world<ul>
<li>If it doesn’t perform well, the development test set is not set correctly or the cost function is not evaluating the right thing.</li>
</ul>
</li>
</ol>
<h2 id="02-setting-up-your-goal"><a href="#02-setting-up-your-goal" class="headerlink" title="02_setting-up-your-goal"></a>02_setting-up-your-goal</h2><h3 id="01-single-number-evaluation-metric"><a href="#01-single-number-evaluation-metric" class="headerlink" title="01_single-number-evaluation-metric"></a>01_single-number-evaluation-metric</h3><p>Whether you’re tuning hyperparameters, or trying out different ideas for learning algorithms, or just trying out different options for building your machine learning system. <strong>You’ll find that your progress will be much faster if you have a single real number evaluation metric that lets you quickly tell if the new thing you just tried is working better or worse than your last idea</strong>. So when teams are starting on a machine learning project, I often recommend that you set up a single real number evaluation metric for your problem. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/5.png" alt=""><br>Let’s look at an example. You’ve heard me say before that applied machine learning is a very empirical process. We often have an idea, code it up, run the experiment to see how it did, and then use the outcome of the experiment to refine your ideas. And then keep going around this loop as you keep on improving your algorithm. So let’s say for your classifier, you had previously built some classifier A. And by changing the hyperparameters and the training sets or some other thing, you’ve now trained a new classifier, B. So one reasonable way to evaluate the performance of your classifiers is to look at its precision and recall. The exact details of what’s precision and recall don’t matter too much for this example. But briefly, the definition of precision is, of the examples that your classifier recognizes as cats, What percentage actually are cats? So if classifier A has 95% precision, this means that when classifier A says something is a cat, there’s a 95% chance it really is a cat. And recall is, of all the images that really are cats, what percentage were correctly recognized by your classifier? So what percentage of actual cats, Are correctly recognized? So if classifier A is 90% recall, this means that of all of the images in, say, your dev sets that really are cats, classifier A accurately pulled out 90% of them. So don’t worry too much about the definitions of precision and recall. It turns out that there’s often a tradeoff between precision and recall, and you care about both. You want that, when the classifier says something is a cat, there’s a high chance it really is a cat. But of all the images that are cats, you also want it to pull a large fraction of them as cats. So it might be reasonable to try to evaluate the classifiers in terms of its precision and its recall. The problem with using precision recall as your evaluation metric is that if classifier A does better on recall, which it does here, the classifier B does better on precision, then you’re not sure which classifier is better. And if you’re trying out a lot of different ideas, a lot of different hyperparameters, you want to rather quickly try out not just two classifiers, but maybe a dozen classifiers and quickly pick out the, quote, best ones, so you can keep on iterating from there. And with two evaluation metrics, it is difficult to know how to quickly pick one of the two or quickly pick one of the ten. So what I recommend is rather than using two numbers, precision and recall, to pick a classifier, you just have to find a new evaluation metric that combines precision and recall. In the machine learning literature, the standard way to combine precision and recall is something called an F1 score. And the details of F1 score aren’t too important, but informally, you can think of this as the average of precision, P, and recall, R. Formally, the F1 score is defined by this formula, it’s 2/ 1/P + 1/R. And in mathematics, this function is called the harmonic mean of precision P and recall R. But less formally, you can think of this as some way that averages precision and recall. Only instead of taking the arithmetic mean, you take the harmonic mean, which is defined by this formula. And it has some advantages in terms of trading off precision and recall. But in this example, you can then see right away that classifier A has a better F1 score. And assuming F1 score is a reasonable way to combine precision and recall, you can then quickly select classifier A over classifier B. So what I found for a lot of machine learning teams is that having a well-defined dev set, which is how you’re measuring precision and recall, plus a single number evaluation metric, sometimes I’ll call it single real number. Evaluation metric allows you to quickly tell if classifier A or classifier B is better, and <strong>therefore having a dev set plus single number evaluation metric distance to speed up iterating. It speeds up this iterative process of improving your machine learning algorithm</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/6.png" alt=""><br>Let’s look at another example. Let’s say you’re building a cat app for cat lovers in four major geographies, the US, China, India, and other, the rest of the world. And let’s say that your two classifiers achieve different errors in data from these four different geographies. So algorithm A achieves 3% error on pictures submitted by US users and so on. So it might be reasonable to keep track of how well your classifiers do in these different markets or these different geographies. But by tracking four numbers, it’s very difficult to look at these numbers and quickly decide if algorithm A or algorithm B is superior. And if you’re testing a lot of different classifiers, then it’s just difficult to look at all these numbers and quickly pick one. So what I recommend in this example is, <strong>in addition to tracking your performance in the four different geographies, to also compute the average. And assuming that average performance is a reasonable single real number evaluation metric, by computing the average, you can <em>quickly</em> tell that it looks like algorithm C has a lowest average error. And you might then go ahead with that one. You have to pick an algorithm to keep on iterating from.</strong> </p>
<p>So your work load machine learning is often, you have an idea, you implement it try it out, and you want to know whether your idea helped. So what was seen in this video is that having a single number evaluation metric can really improve your efficiency or the efficiency of your team in making those decisions. Now we’re not yet done with the discussion on how to effectively set up evaluation metrics. In the next video, I’m going to share with you how to set up optimizing, as well as satisfying matrix. So let’s take a look at the next video. </p>
<h3 id="02-satisficing-and-optimizing-metric"><a href="#02-satisficing-and-optimizing-metric" class="headerlink" title="02_satisficing-and-optimizing-metric"></a>02_satisficing-and-optimizing-metric</h3><p>It’s not always easy to combine all the things you care about into a single real number evaluation metric. In those cases I’ve found it sometimes useful to set up satisficing as well as optimizing matrix. Let me show you what I mean. </p>
<p>Let’s say that you’ve decided you care about the classification accuracy of your cat’s classifier, this could have been F1 score or some other measure of accuracy, but let’s say that in addition to accuracy you also care about <strong>the running time</strong>.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/7.png" alt=""><br>So how long it takes to classify an image and classifier A takes 80 milliseconds, B takes 95 milliseconds, and C takes 1,500 milliseconds, that’s 1.5 seconds to classify an image. So one thing you could do is combine accuracy and running time into an overall evaluation metric. And so the costs such as maybe the overall cost is accuracy minus 0.5 times running time. But maybe it seems a bit artificial to combine accuracy and running time using a formula like this, like a linear weighted sum of these two things. <strong>So here’s something else you could do instead which is that you might want to choose a classifier that maximizes accuracy but subject to that the running time, that is the time it takes to classify an image</strong>, that that has to be less than or equal to 100 milliseconds. So in this case we would say that accuracy is <strong>an optimizing metric</strong> because you want to maximize accuracy. You want to do as well as possible on accuracy but that running time is what we call <strong>a satisficing metric</strong>. <strong>Meaning that it just has to be good enough, it just needs to be less than 100 milliseconds and beyond that you don’t really care, or at least you don’t care that much. So this will be a pretty reasonable way to trade off or to put together accuracy as well as running time. And it may be the case that so long as the running time is less that 100 milliseconds, your users won’t care that much whether it’s 100 milliseconds or 50 milliseconds or even faster.</strong> And by defining optimizing as well as satisficing matrix, this gives you a clear way to pick the, quote, best classifier, which in this case would be classifier B because of all the ones with a running time better than 100 milliseconds it has the best accuracy. <strong>So more generally, if you have N matrix that you care about it’s sometimes reasonable to pick one of them to be optimizing. So you want to do as well as is possible on that one. And then N minus 1 to be satisficing, meaning that so long as they reach some threshold such as running times faster than 100 milliseconds, but so long as they reach some threshold, you don’t care how much better it is in that threshold, but they have to reach that threshold.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/8.png" alt=""><br>Here’s another example. Let’s say you’re building a system to detect wake words, also called trigger words. So this refers to the voice control devices like the Amazon Echo where you wake up by saying Alexa or some Google devices which you wake up by saying okay Google or some Apple devices which you wake up by saying Hey Siri or some Baidu devices we should wake up by saying you ni hao Baidu. Oh I guess, you want to read the Chinese, that’s ni hao Baidu. Right, so these are the wake words you use to tell one of these voice control devices to wake up and listen to something you want to say. And for these other Chinese characters for ni hao Baidu. So you might care about the accuracy of your trigger word detection system. So when someone says one of these trigger words, how likely are you to actually wake up your device, and you might also care about the number of false positives. So when no one actually said this trigger word, how often does it randomly wake up? So in this case maybe one reasonable way of combining these two evaluation matrix might be to maximize accuracy, so when someone says one of the trigger words, maximize the chance that your device wakes up. And subject to that, you have at most one false positive every 24 hours of operation, right? So that your device randomly wakes up only once per day on average when no one is actually talking to it. So in this case accuracy is the optimizing metric and a number of false positives every 24 hours is the satisficing metric where you’d be satisfied so long as there is at most one false positive every 24 hours. </p>
<p><strong>To summarize, if there are multiple things you care about by say there’s one as the optimizing metric that you want to do as well as possible on and one or more as satisficing metrics were you’ll be satisfice. Almost it does better than some threshold you can now have an almost automatic way of quickly looking at multiple core size and picking the, quote, best one.</strong> Now these evaluation matrix must be evaluated or calculated on a training set or a development set or maybe on the test set. So one of the things you also need to do is set up training, dev or development, as well as test sets. In the next video, I want to share with you some guidelines for how to set up training, dev, and test sets. So let’s go on to the next.</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p><strong>Satisficing and optimizing metric</strong></p>
<p>There are different metrics to evaluate the performance of a classifier, they are called evaluation matrices.They can be categorized as satisficing and optimizing matrices. It is important to note that these evaluation matrices must be evaluated on a training set, a development set or on the test set.</p>
<p>Example: Cat vs Non-cat<br>|Classifier|Accuracy|Running time|<br>|:-:|:-:|:-:|<br>|A|90%|80ms|<br>|B|92%|95ms|<br>|C|95%|1500ms|</p>
<p>In this case, accuracy and running time are the evaluation matrices. Accuracy is the optimizing metric, because you want the classifier to correctly detect a cat image as accurately as possible. The running time which is set to be under 100 ms in this example, is the satisficing metric which mean that the metric has to meet expectation set.</p>
<p>The general rule is:<br>$$N_{metric}:\cases{1 &amp; \text{Optimizing metric} \ N_{metric}-1 &amp; \text{Satisficing metric}}$$</p>
<h3 id="03-train-dev-test-distributions"><a href="#03-train-dev-test-distributions" class="headerlink" title="03_train-dev-test-distributions"></a>03_train-dev-test-distributions</h3><p><strong>The way you set up your training dev, or development sets and test sets, can have a huge impact on how rapidly you or your team can make progress on building machine learning application. The same teams, even teams in very large companies, set up hese data sets in ways that really slows down, rather than speeds up, the progress of the team.</strong> Let’s take a look at how you can set up these data sets to maximize your team’s efficiency. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/9.png" alt=""><br>In this video, I want to focus on how you set up your <strong>dev and test sets</strong>. So, that dev set is also called <strong>the development set</strong>, or sometimes called the <strong>hold out cross validation set</strong>. And, workflow in machine learning is that you try a lot of ideas, train up different models on the training set, and then use the dev set to evaluate the different ideas and pick one. And, keep innovating to improve dev set performance until, finally, you have one clause that you’re happy with that you then evaluate on your test set. Now, let’s say, by way of example, that you’re building a cat crossfire, and you are operating in these regions: in the U.S, U.K, other European countries, South America, India, China, other Asian countries, and Australia. So, how do you set up your dev set and your test set? Well, one way you could do so is to pick four of these regions. I’m going to use these four but it could be four randomly chosen regions. And say, that data from these four regions will go into the dev set. And, the other four regions, I’m going to use these four, could be randomly chosen four as well, that those will go into the test set. It turns out, this is a very bad idea because in this example, your dev and test sets come from different distributions. <strong>I would, instead, recommend that you find a way to make your dev and test sets come from the same distribution. So, here’s what I mean. One picture to keep in mind is that, I think, setting up your dev set, plus, your single role number evaluation metric</strong>, that’s like placing a target and telling your team where you think is the bull’s eye you want to aim at. <strong>Because, what happen once you’ve established that dev set and the metric is that, the team can innovate very quickly, try different ideas, run experiments and very quickly use the dev set and the metric to evaluate classifier and try to pick the best one</strong>. So, machine learning teams are often very good at shooting different arrows into targets and innovating to get closer and closer to hitting the bullseye. So, doing well on your metric on your dev sets. And, the problem with how we’ve set up the dev and test sets in the example on the left is that, your team might spend months innovating to do well on the dev set only to realize that, when you finally go to test them on the test set, that data from these four countries or these four regions at the bottom, might be very different than the regions in your dev set. So, you might have a nasty surprise and realize that, all the months of work you spent optimizing to the dev set, is not giving you good performance on the test set. So, having dev and test sets from different distributions is like setting a target, having your team spend months trying to aim closer and closer to bull’s eye, only to realize after months of work that, you’ll say, “Oh wait, to test it, I’m going to move target over here.” And, the team might say, “Well, why did you make us spend months optimizing for a different bull’s eye when suddenly, you can move the bull’s eye to a different location somewhere else?” So, to avoid this, what I recommend instead is that, you take all this randomly shuffled data into the dev and test set. So that, both the dev and test sets have data from all eight regions and that <strong>the dev and test sets really come from the same distribution, which is the distribution of all of your data mixed together</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/10.png" alt=""><br>Here’s another example. This is a, actually, true story but with some details changed. So, I know a machine learning team that actually spent several months optimizing on a dev set which was comprised of loan approvals for medium income zip codes. So, the specific machine learning problem was, “Given an input X about a loan application, can you predict why and which is, whether or not, they’ll repay the loan?” So, this helps you decide whether or not to approve a loan. And so, the dev set came from loan applications. They came from medium income zip codes. Zip codes is what we call postal codes in the United States. But, after working on this for a few months, the team then, suddenly decided to test this on data from low income zip codes or low income postal codes. And, of course, the distributional data for medium income and low income zip codes is very different. And, the crossfire, that they spend so much time optimizing in the former case, just didn’t work well at all on the latter case. And so, this particular team actually wasted about three months of time and had to go back and really re-do a lot of work. And, what happened here was, the team spent three months aiming for one target, and then, after three months, the manager asked, “Oh, how are you doing on hitting this other target?” This is a totally different location. And, it just was a very frustrating experience for the team. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/11.png" alt=""><br>So, what I recommand for setting up a dev set and test set is, choose a dev set and test set to reflect data you expect to get in future and consider important to do well on. And, in particular, the dev set and the test set here, should come from the same distribution. So, whatever type of data you expect to get in the future, and once you do well on, try to get data that looks like that. And, whatever that data is, put it into both your dev set and your test set. Because that way, you’re putting the target where you actually want to hit and you’re having the team innovate very efficiently to hitting that same target, hopefully, the same targets well. Since we haven’t talked yet about how to set up a training set, we’ll talk about the training set in a later video. But, the important take away from this video is that, setting up the dev set, as well as the validation metric, is really defining what target you want to aim at. And hopefully, by setting the dev set and the test set to the same distribution, you’re really aiming at whatever target you hope your machine learning team will hit. <strong>The way you choose your training set will affect how well you can actually hit that target</strong>. But, we can talk about that separately in a later video. </p>
<p>So, I know some machine learning teams that could literally have saved themselves months of work could they follow the guidelines in this video. So, I hope these guidelines will help you, too. Next, it turns out, that the size of your dev and test sets, how to choose the size of them, is also changing the area of deep learning. Let’s talk about that in the next video.</p>
<h4 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h4><p><strong>Training, development and test distributions</strong></p>
<p>Setting up the training, development and test sets have a huge impact on productivity. It is important to choose the development and test sets from the same distribution and it must be taken randomly from all the data.</p>
<p><strong>Guideline</strong></p>
<p>Choose a development set and test set to reflect data you expect to get in the future and consider important to do well.</p>
<h3 id="04-size-of-the-dev-and-test-sets"><a href="#04-size-of-the-dev-and-test-sets" class="headerlink" title="04_size-of-the-dev-and-test-sets"></a>04_size-of-the-dev-and-test-sets</h3><p>In the last video, you saw how your dev and test sets should come from the same distribution, but how long should they be? The guidelines to help set up your dev and test sets are changing in the Deep Learning era. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/12.png" alt=""><br>Let’s take a look at some best practices. You might have heard of the rule of thumb in machine learning of taking all the data you have and using a 70/30 split into a train and test set, or have you had to set up train dev and test sets maybe, you would use a 60% training and 20% dev and 20% tests. <strong>In earlier eras of machine learning, this was pretty reasonable, especially back when data set sizes were just smaller.</strong> So if you had a hundred examples in total, these 70/30 or 60/20/20 rule of thumb would be pretty reasonable. If you had thousand examples, maybe if you had ten thousand examples, these things are not unreasonable. But in the modern machine learning era, we are now used to working with much larger data set sizes. So let’s say you have a million training examples, it might be quite reasonable to set up your data so that you have 98% in the training set, 1% dev, and 1% test. And when you use DNT to abbreviate dev and test sets. Because if you have a million examples, then 1% of that, is 10,000 examples, and that might be plenty enough for a dev set or for a test set. So, in the modern Deep Learning era where sometimes we have much larger data sets, It’s quite reasonable to use a much smaller than 20 or 30% of your data for a dev set or a test set. And because Deep Learning algorithms have such a huge hunger for data, I’m seeing that, the problems we have large data sets that have much larger fraction of it goes into the training set. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/13.png" alt=""><br>So, how about the test set? Remember the purpose of your test set is that, after you finish developing a system, the test set helps evaluate how good your final system is. The guideline is, to set your test set to big enough to give high confidence in the overall performance of your system. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/14.png" alt=""><br>So, unless you need to have a very accurate measure of how well your final system is performing, maybe you don’t need millions and millions of examples in a test set, and maybe for your application if you think that having 10,000 examples gives you enough confidence to find the performance on maybe 100,000 or whatever it is, that might be enough. And this could be much less than, say 30% of the overall data set, depend on how much data you have. For some applications, maybe you don’t need a high confidence in the overall performance of your final system. Maybe all you need is a train and dev set, And I think, not having a test set might be okay. In fact, what sometimes happened was, people were talking about using train test splits but what they were actually doing was iterating on the test set. So rather than test set, what they had was a train dev split and no test set. If you’re actually tuning to this set, to this dev set and this test set, It’s better to call the dev set. Although I think in the history of machine learning, not everyone has been completely clean and completely records of about calling the dev set when it really should be treated as test set. But, if all you care about is having some data that you train on, and having some data to tune to, and you’re just going to shake the final system and not worry too much about how it was actually doing, I think it will be healthy and just call the train dev set and acknowledge that you have no test set. This a bit unusual? I’m definitely not recommending not having a test set when building a system. I do find it reassuring to have a separate test set you can use to get an unbiased estimate of how I was doing before you shift it, but if you have a very large dev set so that you think you won’t overfit the dev set too badly. Maybe it’s not totally unreasonable to just have a train dev set, although it’s not what I usually recommend. </p>
<p>So to summarize, in the era of big data, I think the old rule of thumb of a 70/30 is that, that no longer applies. And the trend has been to use more data for training and less for dev and test, especially when you have a very large data sets. And the rule of thumb is really to try to set the dev set to big enough for its purpose, which helps you evaluate different ideas and pick this up from AOP better. And the purpose of test set is to help you evaluate your final cost buys. You just have to set your test set big enough for that purpose, and that could be much less than 30% of the data. So, I hope that gives some guidance or some suggestions on how to set up your dev and test sets in the Deep Learning era. Next, it turns out that sometimes, part way through a machine learning problem, you might want to change your evaluation metric, or change your dev and test sets. Let’s talk about it when you might want to do.</p>
<h4 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/15.png" alt=""></p>
<h3 id="05-when-to-change-dev-test-sets-and-metrics"><a href="#05-when-to-change-dev-test-sets-and-metrics" class="headerlink" title="05_when-to-change-dev-test-sets-and-metrics"></a>05_when-to-change-dev-test-sets-and-metrics</h3><p>You’ve seen how sets of a dev set and evaluation metric is like placing a target somewhere for your team to aim at. But sometimes partway through a project you might realize you put your target in the wrong place. In that case you should move your target. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/16.png" alt=""><br>Let’s take a look at an example. Let’s say you build a cat classifier to try to find lots of pictures of cats to show to your cat loving users and the metric that you decided to use is classification error. So algorithms A and B have, respectively, 3 percent error and 5 percent error, so <strong>it seems like Algorithm A is doing better</strong>. But let’s say you try out these algorithms, you look at these algorithms and <strong>Algorithm A, for some reason, is letting through a lot of the pornographic images</strong>. So if you shift Algorithm A the users would see more cat images because you’ll see 3 percent error and identify cats, but it also shows the users some pornographic images which is totally unacceptable both for your company, as well as for your users. <strong>In contrast, Algorithm B has 5 percent error so this classifies fewer images but it doesn’t have pornographic images</strong>. So from your company’s point of view, as well as from a user acceptance point of view, <strong>Algorithm B is actually a much better algorithm because it’s not letting through any pornographic images</strong>. So, what has happened in this example is that Algorithm A is doing better on evaluation metric. It’s getting 3 percent error but it is actually a worse algorithm. <strong>In this case, the evaluation metric plus the dev set prefers Algorithm A because they’re saying, look, Algorithm A has lower error which is the metric you’re using but you and your users prefer Algorithm B because it’s not letting through pornographic images. So when this happens, when your evaluation metric is no longer correctly rank ordering preferences between algorithms, in this case is mispredicting that Algorithm A is a better algorithm, then that’s a sign that you should change your evaluation metric or perhaps your development set or test set.</strong> In this case the misclassification error metric that you’re using can be written as follows: this one over m, a number of examples in your development set, of sum from i equals 1 to mdev, number of examples in this development set of indicator of whether or not the prediction of example i in your development set is not equal to the actual label i, where they use this notation to denote their predictive value. Right. So these are zero. And this, $I\left{\right}$ , indicates a function notation, counts up the number of examples on which this thing inside it’s true. So this formula just counts up the number of misclassified examples. The problem with this evaluation metric is that they treat pornographic and non-pornographic images equally but you really want your classifier to not mislabel pornographic images, like maybe you recognize a pornographic image in cat image and therefore show it to unsuspecting user, therefore very unhappy with unexpectedly seeing porn. One way to change this evaluation metric would be if you add the weight term here, we call this w(i) where w(i) is going to be equal to 1 if x(i) is non-porn and maybe 10 or maybe even large number like a 100 if x(i) is porn. So this way you’re giving a much larger weight to examples that are pornographic so that the error term goes up much more if the algorithm makes a mistake on classifying a pornographic image as a cat image. In this example you giving 10 times bigger weights to classify pornographic images correctly. If you want this normalization constant, technically this becomes sum over i of w(i), so then this error would still be between zero and one. The details of this weighting aren’t important and actually to implement this weighting, you need to actually go through your dev and test sets, so label the pornographic images in your dev and test sets so you can implement this weighting function. <strong>But the high level of take away is, if you find that evaluation metric is not giving the correct rank order preference for what is actually better algorithm, then there’s a time to think about defining a new evaluation metric. And this is just one possible way that you could define an evaluation metric. The goal of the evaluation metric is accurately tell you, given two classifiers, which one is better for your application. For the purpose of this video, don’t worry too much about the details of how we define a new error metric, the point is that if you’re not satisfied with your old error metric then don’t keep coasting with an error metric you’re unsatisfied with, instead try to define a new one that you think better captures your preferences in terms of what’s actually a better algorithm.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/17.png" alt=""><br>One thing you might notice is that so far we’ve only talked about how to define a metric to evaluate classifiers. That is, we’ve defined an evaluation metric that helps us better rank order classifiers when they are performing at varying levels in terms of streaming of porn. <strong>And this is actually an example of an orthogonalization where I think you should take a machine learning problem and break it into distinct steps</strong>. One step is to figure out how to define a metric that captures what you want to do, and I would worry separately about how to actually do well on this metric. So think of the machine learning task as two distinct steps. To use the target analogy, the first step is to place the target. So define where you want to aim and then as a completely separate step, this is one you can tune which is how do you place the target as a completely separate problem. Think of it as a separate step to tune in terms of how to do well at this algorithm, how to aim accurately or how to shoot at the target. Defining the metric is step one and you do something else for step two. In terms of shooting at the target, maybe your learning algorithm is optimizing some cost function that looks like this, where you are minimizing some of losses on your training set. One thing you could do is to also modify this in order to incorporate these weights and maybe end up changing this normalization constant as well. So it just 1 over a sum of w(i). Again, the details of how you define J aren’t important, but <strong>the point was with the philosophy of orthogonalization think of placing the target as one step and aiming and shooting at a target as a distinct step which you do separately. In other words I encourage you to think of, defining the metric as one step and only after you define a metric, figure out how to do well on that metric which might be changing the cost function J that your neural network is optimizing.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/18.png" alt=""><br>Before going on, let’s look at just one more example. Let’s say that your two cat classifiers A and B have, respectively, 3 percent error and 5 percent error as evaluated on your dev set. Or maybe even on your test set which are images downloaded off the internet, so high quality well framed images. But maybe when you deploy your algorithm product, you find that algorithm B actually looks like it’s performing better, even though it’s doing better on your dev set. And you find that you’ve been training off very nice high quality images downloaded off the Internet but when you deploy those on the mobile app, users are uploading all sorts of pictures, they’re much less framed, you haven’t only covered the cat, the cats have funny facial expressions, maybe images are much blurrier, and when you test out your algorithms you find that Algorithm B is actually doing better. So this would be another example of your metric and dev test sets falling down. The problem is that you’re evaluating on the dev and test sets a very nice, high resolution, well-framed images but what your users really care about is you have them doing well on images they are uploading, which are maybe less professional shots and blurrier and less well framed. So the guideline is, if doing well on your metric and your current dev sets or dev and test sets’ distribution, if that does not correspond to doing well on the application you actually care about, then change your metric and your dev test set. In other words, if we discover that your dev test set has these very high quality images but evaluating on this dev test set is not predictive of how well your app actually performs, because your app needs to deal with lower quality images, then that’s a good time to change your dev test set so that your data better reflects the type of data you actually need to do well on. But the overall guideline is if your current metric and data you are evaluating on doesn’t correspond to doing well on what you actually care about, then change your metrics and/or your dev/test set to better capture what you need your algorithm to actually do well on. </p>
<p>Having an evaluation metric and the dev set allows you to much more quickly make decisions about is Algorithm A or Algorithm B better. It really speeds up how quickly you and your team can iterate. <strong>So my recommendation is, even if you can’t define the perfect evaluation metric and dev set, just set something up quickly and use that to drive the speed of your team iterating. And if later down the line you find out that it wasn’t a good one, you have better idea, change it at that time, it’s perfectly okay. But what I recommend against for the most teams is to run for too long without any evaluation metric and dev set up because that can slow down the efficiency of what your team can iterate and improve your algorithm</strong>. So that says on when to change your evaluation metric and/or dev and test sets. I hope that these guidelines help you set up your whole team to have a well-defined target that you can iterate efficiently towards improving performance.</p>
<h3 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h3><p><strong>When to change development/test sets and metrics</strong></p>
<p>Example: Cat vs Non-cat</p>
<p>A cat classifier tries to find a great amount of cat images to show to cat loving users. The evaluation metric used is a classification error.</p>
<table>
<thead>
<tr>
<th align="center">Algorithm</th>
<th align="center">Classification error [%]</th>
</tr>
</thead>
<tbody><tr>
<td align="center">A</td>
<td align="center">3%</td>
</tr>
<tr>
<td align="center">B</td>
<td align="center">5%</td>
</tr>
</tbody></table>
<p>It seems that Algorithm A is better than Algorithm B since there is only a 3% error, however for some reason, Algorithm A is letting through a lot of the pornographic images. </p>
<p>Algorithm B has 5% error thus it classifies fewer images but it doesn’t have pornographic images. From a company’s point of view, as well as from a user acceptance point of view, Algorithm B is actually a better algorithm. The evaluation metric fails to correctly rank order preferences between algorithms. The evaluation metric or the development set or test set should be changed. </p>
<p>The misclassification error metric can be written as a function as follow:</p>
<p>$$Error:\frac{1}{m_{dev}}\sum\limits^{m_{dev}}_{i=1}\mathcal{L}{\hat{y}^{(i)}\ne y^{(i)}}$$</p>
<p>This function counts up the number of misclassified examples.</p>
<p>The problem with this evaluation metric is that it treats pornographic vs non-pornographic images equally. On way to change this evaluation metric is to add the weight term $w^{(i)}$</p>
<p>$$W^{(i)}=\cases{1 &amp; \text{if x^{(i)} is non-porngraphic} \ 10 &amp; \text{if x^{(i)} is porngraphic} }$$</p>
<p>The function becomes:</p>
<p>$$Error:\frac{1}{\sum w^{(i)}}\sum\limits^{m_{dev}}_{i=1}w^{(i)}\mathcal{L}{\hat{y}^{(i)}\ne y^{(i)}}$$</p>
<p>Guideline</p>
<ol>
<li>Define correctly an evaluation metric that helps better rank order classifiers</li>
<li>Optimize the evaluation metric.</li>
</ol>
<h2 id="03-comparing-to-human-level-performance"><a href="#03-comparing-to-human-level-performance" class="headerlink" title="03_comparing-to-human-level-performance"></a>03_comparing-to-human-level-performance</h2><h3 id="01-why-human-level-performance"><a href="#01-why-human-level-performance" class="headerlink" title="01_why-human-level-performance"></a>01_why-human-level-performance</h3><p>In the last few years, <strong>a lot more machine learning teams have been talking about comparing the machine learning systems to human level performance. Why is this? I think there are two main reasons</strong>. <strong>First</strong> is that because of advances in deep learning, machine learning algorithms are suddenly working much better and so it has become much more feasible in a lot of application areas for machine learning algorithms to actually become competitive with human-level performance. <strong>Second</strong>, it turns out that the workflow of designing and building a machine learning system, the workflow is much more efficient when you’re trying to do something that humans can also do. So in those settings, it becomes natural to talk about comparing, or trying to mimic human-level performance. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/19.png" alt=""><br>Let’s see a couple examples of what this means. I’ve seen on a lot of machine learning tasks that as you work on a problem over time, so the x-axis, time, this could be many months or even many years over which some team or some research community is working on a problem. Progress tends to be relatively rapid as you approach human level performance. But then after a while, the algorithm surpasses human-level performance and then progress and accuracy actually slows down. And maybe it keeps getting better but after surpassing human level performance it can still get better, but performance, the slope of how rapid the accuracy’s going up, often that slows down. And the hope is it achieves some theoretical optimum level of performance. And over time, as you keep training the algorithm, maybe bigger and bigger models on more and more data, the performance approaches but never surpasses some theoretical limit, which is called the Bayes optimal error. So Bayes optimal error, think of this as the best possible error. And that’s just the way for any function mapping from x to y to surpass a certain level of accuracy. So for example, for speech recognition, if x is audio clips, some audio is just so noisy it is impossible to tell what is in the correct transcription. So the perfect error may not be 100%. Or for cat recognition. Maybe some images are so blurry, that it is just impossible for anyone or anything to tell whether or not there’s a cat in that picture. So, the perfect level of accuracy may not be 100%. And Bayes optimal error, or Bayesian optimal error, or sometimes Bayes error for short, is the very best theoretical function for mapping from x to y. That can never be surpassed. So it should be no surprise that this purple line, no matter how many years you work on a problem you can never surpass Bayes error, Bayes optimal error. And it turns out that progress is often quite fast until you surpass human level performance. And it sometimes slows down after you surpass human level performance. And I think there are two reasons for that, for why progress often slows down when you surpass human level performance. One reason is that human level performance is for many tasks not that far from Bayes’ optimal error. People are very good at looking at images and telling if there’s a cat or listening to audio and transcribing it. So, by the time you surpass human level performance maybe there’s not that much head room to still improve. But the second reason is that so long as your performance is worse than human level performance, then there are actually certain tools you could use to improve performance that are harder to use once you’ve surpassed human level performance. So here’s what I mean. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/21.png" alt=""><br>For tasks that humans are quite good at, and this includes looking at pictures and recognizing things, or listening to audio, or reading language, really natural data tasks humans tend to be very good at. For tasks that humans are good at, so long as your machine learning algorithm is still worse than the human, you can get labeled data from humans. That is you can ask people, ask higher humans, to label examples for you so that you can have more data to feed your learning algorithm. Something we’ll talk about next week is manual error analysis. But so long as humans are still performing better than any other algorithm, you can ask people to look at examples that your algorithm’s getting wrong, and try to gain insight in terms of why a person got it right but the algorithm got it wrong. And we’ll see next week that this helps improve your algorithm’s performance. And you can also get a better analysis of bias and variance which we’ll talk about in a little bit. But so long as your algorithm is still doing worse then humans you have these important tactics for improving your algorithm. Whereas once your algorithm is doing better than humans, then these three tactics are harder to apply. So, this is maybe another reason why comparing to human level performance is helpful, especially on tasks that humans do well. And why machine learning algorithms tend to be really good at trying to replicate tasks that people can do and kind of catch up and maybe slightly surpass human level performance. </p>
<p>In particular, even though you know what is bias and what is variance it turns out that knowing how well humans can do on a task can help you understand better how much you should try to reduce bias and how much you should try to reduce variance. I want to show you an example of this in the next video. </p>
<h4 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h4><p><strong>Why human-level performance?</strong></p>
<p>Today, machine learning algorithms can compete with human-level performance since they are more productive and more feasible in a lot of application. Also, the workflow of designing and building a machine learning system, is much more efficient than before. </p>
<p>Moreover, some of the tasks that humans do are close to “perfection”, which is why machine learning tries to mimic human-level performance. </p>
<p>The graph below shows the performance of humans and machine learning over time.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/22.png" alt=""></p>
<p>Machine learning progresses slowly when it surpasses human-level performance. One of the reason is that human-level performance can be close to Bayes optimal error, especially for natural perception problem.</p>
<p>Bayes optimal error is defined as the best possible error. In other words, it means that any functions mapping from x to y can’t surpass a certain level of accuracy.</p>
<p>Also, when the performance of machine learning is worse than the performance of humans, you can improve it with different tools. They are harder to use once its surpasses human-level performance.</p>
<p>These tools are:</p>
<ul>
<li>Get labeled data from humans</li>
<li>Gain insight from manual error analysis: Why did a person get this right?</li>
<li>Better analysis of bias/variance.</li>
</ul>
<h3 id="02-avoidable-bias"><a href="#02-avoidable-bias" class="headerlink" title="02_avoidable-bias"></a>02_avoidable-bias</h3><p>We talked about how you want your learning algorithm to do well on the training set but sometimes you don’t actually want to do too well and knowing what human level performance is, can tell you exactly how well but not too well you want your algorithm to do on the training set. Let me show you what I mean. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/23.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/24.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/25.png" alt=""><br>We have used Cat classification a lot and given a picture, let’s say humans have near-perfect accuracy so the human level error is one percent. In that case, if your learning algorithm achieves 8 percent training error and 10 percent dev error, then maybe you wanted to do better on the training set. So the fact that there’s a huge gap between how well your algorithm does on your training set versus how humans do shows that your algorithm isn’t even fitting the training set well. So in terms of tools to reduce <strong>bias or variance</strong>, in this case I would say focus on reducing bias. So you want to do things like train a bigger neural network or run training set longer, just try to do better on the training set. But now let’s look at the same training error and dev error and imagine that human level performance was not 1%. So this copy is over but you know in a different application or maybe on a different data set, let’s say that human level error is actually 7.5%. Maybe the images in your data set are so blurry that even humans can’t tell whether there’s a cat in this picture. This example is maybe slightly contrived because humans are actually very good at looking at pictures and telling if there’s a cat in it or not. But for the sake of this example, let’s say your data sets images are so blurry or so low resolution that even humans get 7.5% error. In this case, even though your training error and dev error are the same as the other example, you see that maybe you’re actually doing just fine on the training set. It’s doing only a little bit worse than human level performance. And in this second example, you would maybe want to focus on reducing this component, reducing the variance in your learning algorithm. So you might try regularization to try to bring your dev error closer to your training error for example. So in the earlier courses discussion on bias and variance, we were mainly assuming that there were tasks where Bayes error is nearly zero. So to explain what just happened here, for our Cat classification example, think of human level error as a proxy or as a estimate for Bayes error or for Bayes optimal error. And for computer vision tasks, this is a pretty reasonable proxy because humans are actually very good at computer vision and so whatever a human can do is maybe not too far from Bayes error. By definition, human level error is worse than Bayes error because nothing could be better than Bayes error but human level error might not be too far from Bayes error. So the surprising thing we saw here is that depending on what human level error is or really this is really approximately Bayes error or so we assume it to be, but depending on what we think is achievable, with the same training error and dev error in these two cases, we decided to focus on bias reduction tactics or on variance reduction tactics. And what happened is in the example on the left, 8% training error is really high when you think you could get it down to 1% and so bias reduction tactics could help you do that. Whereas in the example on the right, if you think that Bayes error is 7.5% and here we’re using human level error as an estimate or as a proxy for Bayes error, but you think that Bayes error is close to seven point five percent then you know there’s not that much headroom for reducing your training error further down. You don’t really want it to be that much better than 7.5% because you could achieve that only by maybe starting to offer further training so, and instead, there’s much more room for improvement in terms of taking this 2% gap and trying to reduce that by using variance reduction techniques such as regularization or maybe getting more training data. So to give these things a couple of names, this is not widely used terminology but I found this useful terminology and a useful way of thinking about it, which is I’m going to call the difference between Bayes error or approximation of Bayes error and the training error to be <strong>the avoidable bias</strong>. So what you want is maybe keep improving your training performance until you get down to Bayes error but you don’t actually want to do better than Bayes error. You can’t actually do better than Bayes error unless you’re overfitting. And this, the difference between your training area and the dev error, there’s a measure still of the variance problem of your algorithm. And the term avoidable bias acknowledges that there’s some bias or some minimum level of error that you just cannot get below which is that if Bayes error is 7.5%, you don’t actually want to get below that level of error. So rather than saying that if you’re training error is 8%, then the 8% is a measure of bias in this example, you’re saying that the avoidable bias is maybe 0.5% or 0.5% is a measure of the avoidable bias whereas 2% is a measure of the variance and so there’s much more room in reducing this 2% than in reducing this 0.5%. Whereas in contrast in the example on the left, this 7% is a measure of the avoidable bias, whereas 2% is a measure of how much variance you have. And so in this example on the left, there’s much more potential in focusing on reducing that avoidable bias. </p>
<p>So in this example, understanding human level error, understanding your estimate of Bayes error really causes you in different scenarios to focus on different tactics, whether bias avoidance tactics or variance avoidance tactics. There’s quite a lot more nuance in how you factor in human level performance into how you make decisions in choosing what to focus on. Thus in the next video, go deeper into understanding of what human level performance really mean.</p>
<h4 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h4><p><strong>Avoidable bias</strong><br>By knowing what the human-level performance is, it is possible to tell when a training set is performing well or not.</p>
<p><strong>Example: Cat vs Non-Cat</strong><br>In this case, the human level error as a proxy for Bayes error since humans are good to identify images. If you want to improve the performance of the training set but you can’t do better than the Bayes error otherwise the training set is overfitting. By knowing the Bayes error, it is easier to focus on whether bias or variance avoidance tactics will improve the performance of the model.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/26.png" alt=""></p>
<p><strong>Scenario A</strong><br>There is a 7% gap between the performance of the training set and the human level error. It means that the algorithm isn’t fitting well with the training set since the target is around 1%. To resolve the issue, we use bias reduction technique such as training a bigger neural network or running the training set longer.</p>
<p><strong>Scenario B</strong><br>The training set is doing good since there is only a 0.5% difference with the human level error. The difference between the training set and the human level error is called avoidable bias. The focus here is to reduce the variance since the difference between the training error and the development error is 2%. To resolve the issue, we use variance reduction technique such as regularization or have a bigger training set.</p>
<h3 id="03-understanding-human-level-performance"><a href="#03-understanding-human-level-performance" class="headerlink" title="03_understanding-human-level-performance"></a>03_understanding-human-level-performance</h3><p>The term human-level performance is sometimes used casually in research articles. But let me show you how we can define it a bit more precisely. And in particular, use the definition of the phrase, human-level performance, that is most useful for helping you drive progress in your machine learning project. </p>
<p>So remember from our last video that one of the uses of this phrase, human-level error, is that it gives us a way of estimating Bayes error. What is the best possible error any function could, either now or in the future, ever, ever achieve? So bearing that in mind, let’s look at a medical image classification example.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/27.png" alt=""><br>Let’s say that you want to look at a radiology image like this, and make a diagnosis classification decision. And suppose that a typical human, untrained human, achieves 3% error on this task. A typical doctor, maybe a typical radiologist doctor, achieves 1% error. An experienced doctor does even better, 0.7% error. And a team of experienced doctors, that is if you get a team of experienced doctors and have them all look at the image and discuss and debate the image, together their consensus opinion achieves 0.5% error. So the question I want to pose to you is, how should you define human-level error? Is human-level error 3%, 1%, 0.7% or 0.5%? Feel free to pause this video to think about it if you wish. And to answer that question, I would urge you to bear in mind that one of the most useful ways to think of human error is as a proxy or an estimate for Bayes error. So please feel free to pause this video to think about it for a while if you wish. <strong>But here’s how I would define human-level error. Which is if you want a proxy or an estimate for Bayes error, then given that a team of experienced doctors discussing and debating can achieve 0.5% error, we know that Bayes error is less than equal to 0.5%. So because some system, team of these doctors can achieve 0.5% error, so by definition, this directly, optimal error has got to be 0.5% or lower. We don’t know how much better it is, maybe there’s a even larger team of even more experienced doctors who could do even better, so maybe it’s even a little bit better than 0.5%. But we know the optimal error cannot be higher than 0.5%. So what I would do in this setting is use 0.5% as our estimate for Bayes error. So I would define human-level performance as 0.5%. At least if you’re hoping to use human-level error in the analysis of bias and variance as we saw in the last video.</strong> </p>
<p><strong>Now, for the purpose of publishing a research paper or for the purpose of deploying a system, maybe there’s a different definition of human-level error</strong> that you can use which is so long as you surpass the performance of a typical doctor. That seems like maybe a very useful result if accomplished, and maybe surpassing a single radiologist, a single doctor’s performance might mean the system is good enough to deploy in some context. So maybe the takeaway from this is to be clear about what your purpose is in defining the term human-level error. And if it is to show that you can surpass a single human and therefore argue for deploying your system in some context, maybe this is the appropriate definition. <strong>But if your goal is the proxy for Bayes error, then this is the appropriate definition.</strong> To see why this matters, let’s look at an error analysis example. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/28.png" alt=""><br>Let’s say, for a medical imaging diagnosis example, that your training error is 5% and your dev error is 6%. And in the example from the previous slide, our human-level performance, and I’m going to think of this as proxy for Bayes error. Depending on whether you defined it as a typical doctor’s performance or experienced doctor or team of doctors, you would have either 1% or 0.7% or 0.5% for this. And remember also our definitions from the previous video, that this gap between Bayes error or estimate of Bayes error and training error is calling that a measure of the avoidable bias. And this as a measure or an estimate of how much of a variance problem you have in your learning algorithm. So in this first example, whichever of these choices you make, the measure of avoidable bias will be something like 4%. It will be somewhere between I guess, 4%, if you take that to 4.5%, if you use 0.5%, whereas this is 1%. So in this example, I would say, it doesn’t really matter which of the definitions of human-level error you use, whether you use the typical doctor’s error or the single experienced doctor’s error or the team of experienced doctor’s error. Whether this is 4% or 4.5%, this is clearly bigger than the variance problem. And so in this case, you should focus on bias reduction techniques such as train a bigger network. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/29.png" alt=""><br>Now let’s look at a second example. Let’s see your training error is 1% and your dev error is 5%. Then again it doesn’t really matter, seems but academic whether the human-level performance is 1% or 0.7% or 0.5%. Because whichever of these definitions you use, your measure of avoidable bias will be, I guess somewhere between 0% if you use that, to 0.5%, right? That’s the gap between the human-level performance and your training error, whereas this gap is 4%. So this 4% is going to be much bigger than the avoidable bias either way. And so they’ll just suggest you should focus on variance reduction techniques such as regularization or getting a bigger training set. </p>
<p>But where it really matters will be if your training error is 0.7%. So you’re doing really well now, and your dev error is 0.8%. In this case, it really matters that you use your estimate for Bayes error as 0.5%. Because in this case, your measure of how much avoidable bias you have is 0.2% which is twice as big as your measure for your variance, which is just 0.1%. And so this suggests that maybe both the bias and variance are both problems but maybe the avoidable bias is a bit bigger of a problem. And in this example, 0.5% as we discussed on the previous slide was the best measure of Bayes error, because a team of human doctors could achieve that performance. If you use 0.7 as your proxy for Bayes error, you would have estimated avoidable bias as pretty much 0%, and you might have missed that. You actually should try to do better on your training set. So I hope this gives a sense also of why making progress in a machine learning problem gets harder as you achieve or as you approach human-level performance. In this example, once you’ve approached 0.7% error, unless you’re very careful about estimating Bayes error, you might not know how far away you are from Bayes error. And therefore how much you should be trying to reduce aviodable bias. In fact, if all you knew was that a single typical doctor achieves 1% error, and it might be very difficult to know if you should be trying to fit your training set even better. And this problem arose only when you’re doing very well on your problem already, only when you’re doing 0.7%, 0.8%, really close to human-level performance. Whereas in the two examples on the left, when you are further away human-level performance, it was easier to target your focus on bias or variance. So this is maybe an illustration of why as your pro human-level performance is actually harder to tease out the bias and variance effects. And therefore why progress on your machine learning project just gets harder as you’re doing really well. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/30.png" alt=""><br>So just to summarize what we’ve talked about. If you’re trying to understand bias and variance where you have an estimate of human-level error for a task that humans can do quite well, you can use human-level error as a proxy or as a approximation for Bayes error. And so the difference between your estimate of Bayes error tells you how much avoidable bias is a problem, how much avoidable bias there is. And the difference between training error and dev error, that tells you how much variance is a problem, whether your algorithm’s able to generalize from the training set to the dev set. And the big difference between our discussion here and what we saw in an earlier course was that instead of comparing training error to 0%, And just calling that the estimate of the bias. In contrast, in this video we have a more nuanced analysis in which there is no particular expectation that you should get 0% error. Because sometimes Bayes error is non zero and sometimes it’s just not possible for anything to do better than a certain threshold of error. And so in the earlier course, we were measuring training error, and seeing how much bigger training error was than zero. And just using that to try to understand how big our bias is. And that turns out to work just fine for problems where Bayes error is nearly 0%, such as recognizing cats. Humans are near perfect for that, so Bayes error is also near perfect for that. So that actually works okay when Bayes error is nearly zero. But for problems where the data is noisy, like speech recognition on very noisy audio where it’s just impossible sometimes to hear what was said and to get the correct transcription. For problems like that, having a better estimate for Bayes error can help you better estimate avoidable bias and variance. And therefore make better decisions on whether to focus on bias reduction tactics, or on variance reduction tactics. </p>
<p><strong>So to recap, having an estimate of human-level performance gives you an estimate of Bayes error. And this allows you to more quickly make decisions as to whether you should focus on trying to reduce a bias or trying to reduce the variance of your algorithm. And these techniques will tend to work well until you surpass human-level performance, whereupon you might no longer have a good estimate of Bayes error that still helps you make this decision really clearly. Now, one of the exciting developments in deep learning has been that for more and more tasks we’re actually able to surpass human-level performance.</strong> In the next video, let’s talk more about the process of surpassing human-level performance. </p>
<h4 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h4><p><strong>Understanding human-level performance</strong></p>
<p>Human-level error gives an estimate of Bayes error.</p>
<p><strong>Example 1: Medical image classification</strong></p>
<p>This is an example of a medical image classification in which the input is a radiology image and the output is a diagnosis classification decision.</p>
<p>The definition of human-level error depends on the purpose of the analysis, in this case, by definition the Bayes error is lower or equal to 0.5%.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/35.png" alt=""></p>
<p><strong>Example 2: Error analysis</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/36.png" alt=""><br>Scenario A</p>
<p>In this case, the choice of human-level performance doesn’t have an impact. The avoidable bias is between 4%-4.5% and the variance is 1%. Therefore, the focus should be on bias reduction technique.</p>
<p>Scenario B</p>
<p>In this case, the choice of human-level performance doesn’t have an impact. The avoidable bias is between 0%-0.5% and the variance is 4%. Therefore, the focus should be on variance reduction technique.</p>
<p>Scenario C</p>
<p>In this case, the estimate for Bayes error has to be 0.5% since you can’t go lower than the human-level performance otherwise the training set is overfitting. Also, the avoidable bias is 0.2% and the variance is 0.1%. Therefore, the focus should be on bias reduction technique. Summary of bias/variance with human-level performance</p>
<ul>
<li>Human - level error – proxy for Bayes error</li>
<li>If the difference between human-level error and the training error is bigger than the difference between the training error and the development error. The focus should be on bias reduction technique</li>
<li>If the difference between training error and the development error is bigger than the difference between the human-level error and the training error. The focus should be on variance reduction technique</li>
</ul>
<p><strong>Example 2: Error analysis</strong></p>
<h3 id="04-surpassing-human-level-performance"><a href="#04-surpassing-human-level-performance" class="headerlink" title="04_surpassing-human-level-performance"></a>04_surpassing-human-level-performance</h3><p>A lot of teams often find it exciting to surpass human-level performance on the specific recreational classification task. Let’s talk over some of the things you see if you try to accomplish this yourself. We’ve discussed before how machine learning progress gets harder as you approach or even surpass human-level performance. Let’s talk over one more example of why that’s the case. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/31.png" alt=""><br>Let’s say you have a problem where a team of humans discussing and debating achieves 0.5% error, a single human 1% error, and you have an algorithm of 0.6% training error and 0.8% dev error. So in this case, what is the avoidable bias? So this one is relatively easier to answer, 0.5% is your estimate of base error, so your avoidable bias is, you’re not going to use this 1% number as reference, you can use this difference, so maybe you estimate your avoidable bias is at least 0.1% and your variance as 0.2%. So there’s maybe more to do to reduce your variance than your avoidable bias perhaps. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/32.png" alt=""><br>But now let’s take <strong>a harder example</strong>, let’s say, a team of humans and single human performance, the same as before, but your algorithm gets 0.3% training error, and 0.4% dev error. Now, what is the avoidable bias? It’s now actually much harder to answer that. Is the fact that your training error, 0.3%, does this mean you’ve over-fitted by 0.2%, or is base error, actually 0.1%, or maybe is base error 0.2%, or maybe base error is 0.3%? You don’t really know, <strong>but based on the information given in this example, you actually don’t have enough information to tell if you should focus on reducing bias or reducing variance in your algorithm. So that slows down the efficiency where you should make progress. Moreover, if your error is already better than even a team of humans looking at and discussing and debating the right label, for an example, then it’s just also harder to rely on human intuition to tell your algorithm what are ways that your algorithm could still improve the performance</strong>? So in this example, once you’ve surpassed this 0.5% threshold, your options, your ways of making progress on the machine learning problem are just less clear. <strong>It doesn’t mean you can’t make progress, you might still be able to make significant progress, but some of the tools you have for pointing you in a clear direction just don’t work as well</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/33.png" alt=""><br>Now, there are many problems where machine learning significantly surpasses human-level performance. For example, I think, <strong>online advertising</strong>, estimating how likely someone is to click on that. Probably, learning algorithms do that much better today than any human could, or <strong>making product recommendations</strong>, recommending movies or books to you. I think that web sites today can do that much better than maybe even your closest friends can. All <strong>logistics predicting</strong> how long will take you to drive from A to B or predicting how long to take a delivery vehicle to drive from A to B, or trying to predict whether someone will repay a loan, and therefore, whether or not you should <strong>approve a loan offer</strong>. All of these are problems where I think today machine learning far surpasses a single human’s performance. Notice something about these four examples. <strong>All four of these examples are actually learning from structured data, where you might have a database of what has users clicked on, database of proper support for, databases of how long it takes to get from A to B, database of previous loan applications and their outcomes. And these are not natural perception problems, so these are not computer vision, or speech recognition, or natural language processing task. Humans tend to be very good in natural perception task. So it is possible, but it’s just a bit harder for computers to surpass human-level performance on natural perception task. And finally, all of these are problems where there are teams that have access to huge amounts of data</strong>. So for example, the best systems for all four of these applications have probably looked at far more data of that application than any human could possibly look at. And so, that’s also made it relatively easy for a computer to surpass human-level performance. <strong>Now, the fact that there’s so much data that computer could examine, so it can petrifies that’s called patterns than even the human mind.</strong> Other than these problems, today there are speech recognition systems that can surpass human-level performance. <strong>And there are also some computer vision, some image recognition tasks, where computers have surpassed human-level performance</strong>. But because humans are very good at this natural perception task, I think it was harder for computers to get there. And then there are some <strong>medical tasks</strong>, for example, <strong>reading ECGs or diagnosing skin cancer</strong>, or <strong>certain narrow radiology task</strong>, where computers are getting really good and maybe surpassing a single human-level performance. And I guess one of the exciting things about recent advances in deep learning is that even for these tasks we can now surpass human-level performance in some cases, <strong>but it has been a bit harder because humans tend to be very good at this natural perception task.</strong> </p>
<p>So surpassing human-level performance is often not easy, but given enough data there’ve been lots of deep learning systems have surpassed human-level performance on a single supervisory problem. So that makes sense for an application you’re working on. I hope that maybe someday you manage to get your deep learning system to also surpass human-level performance.</p>
<h4 id="summary-5"><a href="#summary-5" class="headerlink" title="summary"></a>summary</h4><p><strong>Surpassing human-level performance</strong></p>
<p>Example1: Classification task</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/34.png" alt=""></p>
<p><strong>Scenario A</strong></p>
<p>In this case, the Bayes error is 0.5%, therefore the available bias is 0.1% et the variance is 0.2%.</p>
<p><strong>Scenario B</strong></p>
<p>In this case, there is not enough information to know if bias reduction or variance reduction has to be done on the algorithm. It doesn’t mean that the model cannot be improve, it means that the conventional ways to know if bias reduction or variance reduction are not working in this case.</p>
<p>There are many problems where machine learning significantly surpasses human-level performance, especially with structured data:</p>
<ul>
<li>Online advertising</li>
<li>Product recommendations</li>
<li>Logistics (predicting transit time)</li>
<li>Loan approvals</li>
</ul>
<h3 id="05-improving-your-model-performance"><a href="#05-improving-your-model-performance" class="headerlink" title="05_improving-your-model-performance"></a>05_improving-your-model-performance</h3><p>You have heard about orthogonalization. How to set up your dev and test sets, human level performance as a proxy for Bayes’s error and how to estimate your avoidable bias and variance. Let’s pull it all together into a set of guidelines for how to improve the performance of your learning algorithm. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/38.png" alt=""></p>
<p><strong>So, I think getting a supervised learning algorithm to work well means fundamentally hoping or assuming that you can do two things. First</strong> is that you can fit the training set pretty well and you can think of this as roughly saying that you can achieve low avoidable bias. <strong>And the second</strong> thing you’re assuming can do well is that doing well in the training set generalizes pretty well to the dev set or the test set and this is sort of saying that variance is not too bad. <strong>And in the spirit of thought organization, what you see is that there’s a second set of knobs to fix the avoidable bias issues such as training a bigger network or training longer. And there’s a separate set of things you can use to address variance problems, such as regularization or getting more training data</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/39.png" alt=""><br>So to summarize of the process seen in the last several videos, if you want to improve the performance of your machine on your system, I would recommend looking at the difference between your training error and your proxy for base error and this gives you a sense of the avoidable bias. In other words, just how much better do you think you should be trying to do on your training set and then look at the difference between your dev error and your training error as an estimate. So, it’s how much of a variance problem you have. In other words, how much harder you should be working to make your performance generalize from the training set to the desk set, that it wasn’t trained on explicitly? So to whatever extent you want to try to reduce avoidable bias, I would try to apply tactics like train a bigger model. So, you can just do better on your training sets or train longer. Use a better optimization algorithm such as. Adds momentum or RMS prop, or use a better algorithm like ADOM. Or one of the thing you could try is to just find a better new nether architecture or better said, hyperparameters and this could include everything from changing the activation functions or changing the number of layers or hidden do this. Although you do that, it would be in the direction of increasing the model size to China other models or other models architectures, such as the current neural network and competitive neural networks which we’ll see in later courses. Whether or not a new neural network architecture will fit your training set better is sometimes hard to tell in events, but sometimes you can get much better results with a better architecture. Next to the extent that you find out variance is a problem. Some of the many of the techniques you could try, then includes the following. You can try to get more data, because getting more data to train on could help you generalize better to dev set data that you didn’t see. You could try regularization. So this includes things like or dropout, or data augmentation which she talks about the in the previous course. Or once again, you can also try various neural network architecture, hyperparameters search to see if that can help you find a new architecture that is better suited for problem. </p>
<p>I think that this notion of bias or avoidable bias and there is one of those things that easily learned, but tough to master and we’re able to systematically find the concept from this week’s videos. You actually be much more efficient and much more systematic and much more strategic than a lot of machine learning teams in terms of how to systematically go by improving the performance of their machine learning system. So, that this week’s whole work will allow you to practice and exercise more your understanding of these concepts. Best of luck with this homework and I look forward to also seeing you in next week’s videos. Variances are further.</p>
<h4 id=""><a href="#" class="headerlink" title=""></a></h4><p><strong>Improving your model performance</strong></p>
<p>The two fundamental assumptions of supervised learning </p>
<p>There are <strong>2 fundamental assumptions</strong> of supervised learning. The <strong>first</strong> one is to have a low avoidable bias which means that the training set fits well. The <strong>second</strong> one is to have a low or acceptable variance which means that the training set performance generalizes well to the development set and test set.</p>
<p>If the difference between human-level error and the training error is bigger than the difference between the training error and the development error, the focus should be on bias reduction technique which are training a bigger model, training longer or change the neural networks architecture or try various hyperparameters search.</p>
<p>If the difference between training error and the development error is bigger than the difference between the human-level error and the training error, the focus should be on variance reduction technique which are bigger data set, regularization or change the neural networks architecture or try various hyperparameters search.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/StructuringMachineLearningProjects/01_ml-strategy-1/37.png" alt=""></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/03/02/summary_of_Improving-Deep-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/02/summary_of_Improving-Deep-Neural-Networks/" class="post-title-link" itemprop="url">summary of Improving-Deep-Neural-Networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-02 00:00:00" itemprop="dateCreated datePublished" datetime="2018-03-02T00:00:00+05:30">2018-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>1.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>2 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal summary after studying the course, <a href="https://www.coursera.org/learn/deep-neural-network/" target="_blank" rel="noopener">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</a>, which belongs to Deep Learning Specialization. and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="My-personal-notes"><a href="#My-personal-notes" class="headerlink" title="My personal notes"></a>My personal notes</h2><p>${1_{st}}$ week: <a href="/2018/03/01/01_practical-aspects-of-deep-learning/">practical-aspects-of-deep-learning</a></p>
<ul>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#01_setting-up-your-machine-learning-application">01_setting-up-your-machine-learning-application</a><ul>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#01_train-dev-test-sets">01_train-dev-test-sets</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#02_bias-variance">02_bias-variance</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#03_basic-recipe-for-machine-learning">03_basic-recipe-for-machine-learning</a></li>
</ul>
</li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#02_regularizing-your-neural-network">02_regularizing-your-neural-network</a><ul>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#01_regularization">01_regularization</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#02_why-regularization-reduces-overfitting">02_why-regularization-reduces-overfitting</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#03_dropout-regularization">03_dropout-regularization</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#04_understanding-dropout">04_understanding-dropout</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#05_other-regularization-methods">05_other-regularization-methods</a></li>
</ul>
</li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#03_setting-up-your-optimization-problem">03_setting-up-your-optimization-problem</a><ul>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#01_normalizing-inputs">01_normalizing-inputs</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#02_vanishing-exploding-gradients">02_vanishing-exploding-gradients</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#03_weight-initialization-for-deep-networks">03_weight-initialization-for-deep-networks</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#04_numerical-approximation-of-gradients">04_numerical-approximation-of-gradients</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#05_gradient-checking">05_gradient-checking</a></li>
<li><a href="/2018/03/01/01_practical-aspects-of-deep-learning/#06_gradient-checking-implementation-notes">06_gradient-checking-implementation-notes</a></li>
</ul>
</li>
</ul>
<p>$2_{nd}$ week: <a href="/2018/03/02/02_optimization-algorithms/">optimization-algorithms</a></p>
<ul>
<li><a href="/2018/03/02/02_optimization-algorithms/#01_mini-batch-gradient-descent">01_mini-batch-gradient-descent</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#02_understanding-mini-batch-gradient-descent">02_understanding-mini-batch-gradient-descent</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#03_exponentially-weighted-averages">03_exponentially-weighted-averages</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#04_understanding-exponentially-weighted-averages">04_understanding-exponentially-weighted-averages</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#05_bias-correction-in-exponentially-weighted-averages">05_bias-correction-in-exponentially-weighted-averages</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#06_gradient-descent-with-momentum">06_gradient-descent-with-momentum</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#07_rmsprop">07_rmsprop</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#08_adam-optimization-algorithm">08_adam-optimization-algorithm</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#09_learning-rate-decay">09_learning-rate-decay</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#10_the-problem-of-local-optima">10_the-problem-of-local-optima</a></li>
</ul>
<p>$3_{rd}$ week: <a href="/2018/03/02/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/">hyperparameter-tuning-batch-normalization-and-programming-frameworks</a></p>
<ul>
<li><a href="/2018/03/02/02_optimization-algorithms/#01_hyperparameter-tuning">01_hyperparameter-tuning</a><ul>
<li><a href="/2018/03/02/02_optimization-algorithms/#01_tuning-process">01_tuning-process</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#02_using-an-appropriate-scale-to-pick-hyperparameters">02_using-an-appropriate-scale-to-pick-hyperparameters</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#03_hyperparameters-tuning-in-practice-pandas-vs-caviar">03_hyperparameters-tuning-in-practice-pandas-vs-caviar</a></li>
</ul>
</li>
<li><a href="/2018/03/02/02_optimization-algorithms/#02_batch-normalization">02_batch-normalization</a><ul>
<li><a href="/2018/03/02/02_optimization-algorithms/#01_normalizing-activations-in-a-network">01_normalizing-activations-in-a-network</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#02_fitting-batch-norm-into-a-neural-network">02_fitting-batch-norm-into-a-neural-network</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#03_why-does-batch-norm-work">03_why-does-batch-norm-work</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#04_batch-norm-at-test-time">04_batch-norm-at-test-time</a></li>
</ul>
</li>
<li><a href="/2018/03/02/02_optimization-algorithms/#03_multi-class-classification">03_multi-class-classification</a><ul>
<li><a href="/2018/03/02/02_optimization-algorithms/#01_softmax-regression">01_softmax-regression</a></li>
<li><a href="/2018/03/02/02_optimization-algorithms/#02_training-a-softmax-classifier">02_training-a-softmax-classifier</a></li>
</ul>
</li>
<li><a href="/2018/03/02/02_optimization-algorithms/#04_introduction-to-programming-frameworks">04_introduction-to-programming-frameworks</a><ul>
<li><a href="/2018/03/02/02_optimization-algorithms/#02_tensorflow">tensorflow</a></li>
</ul>
</li>
</ul>
<h2 id="My-personal-programming-assignments"><a href="#My-personal-programming-assignments" class="headerlink" title="My personal programming assignments"></a>My personal programming assignments</h2><p>week1: <a href="/2018/03/01/practical-aspects-of-deep-learning/">practical-aspects-of-deep-learning</a><br>week2: <a href="/2018/03/02/OptimizationMethods/">optimization-algorithms</a><br>week3: <a href="/Tensorflow%20Tutorial/">hyperparameter-tuning-batch-normalization-and-programming-frameworks</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/03/02/OptimizationMethods/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/02/OptimizationMethods/" class="post-title-link" itemprop="url">Optimization Methods</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-02 00:00:00" itemprop="dateCreated datePublished" datetime="2018-03-02T00:00:00+05:30">2018-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>33k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>30 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>These are my personal programming assignments at the 2nd week after studying the course <a href="https://www.coursera.org/learn/deep-neural-network/" target="_blank" rel="noopener">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h1 id="Optimization-Methods"><a href="#Optimization-Methods" class="headerlink" title="Optimization Methods"></a>Optimization Methods</h1><p>Until now, you’ve always used Gradient Descent to update the parameters and minimize the cost. In this notebook, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result.</p>
<p>Gradient descent goes “downhill” on a cost function $J$. Think of it as trying to do this:</p>
<img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/images/1.png" style="width:600px;height:400px;">
<caption><center> <u> **Figure 1** </u>: ** : Minimizing the cost is like finding the lowest point in a hilly landscape()**<br> At each step of the training, you update your parameters following a certain direction to try to get to the lowest possible point.</center></caption>




<p><strong>Notations</strong>: As usual, $\frac{∂J}{∂a}= da$ for any variable $a$.</p>
<p>To get started, run the following code to import the libraries you will need.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure>

<h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1. Gradient Descent"></a>1. Gradient Descent</h2><p>A simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all $m$ examples on each step, it is also called Batch Gradient Descent.</p>
<p>Warm-up exercise: Implement the gradient descent update rule. The gradient descent rule is, for $l=1,…,L$:<br>$$W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{1}$$<br>$$b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{2}$$</p>
<p>where $L$ is the number of layers and $α$ is the learning rate. All parameters should be stored in the parameters dictionary. </p>
<p>Note that the iterator $l$ starts at $0$ in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift $l$ to $l+1$ when coding.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using one step of gradient descent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters to be updated:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients to update each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span>; <span class="comment"># number of layers in the neural networks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">'db'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, learning_rate = update_parameters_with_gd_test_case();</span><br><span class="line"></span><br><span class="line">parameters = update_parameters_with_gd(parameters, grads, learning_rate);</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]));</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]));</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]));</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]));</span><br></pre></td></tr></table></figure>

<p><strong>Expected Output</strong>:</p>
<table>
<thead>
<tr>
<th><strong>variabale</strong></th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td><strong>W1</strong></td>
<td>[[ 1.63535156 -0.62320365 -0.53718766] [-1.07799357 0.85639907 -2.29470142]]</td>
</tr>
<tr>
<td><strong>b1</strong></td>
<td>[[ 1.74604067] [-0.75184921]]</td>
</tr>
<tr>
<td><strong>W2</strong></td>
<td>[[ 0.32171798 -0.25467393 1.46902454] [-2.05617317 -0.31554548 -0.3756023 ] [ 1.1404819 -1.09976462 -0.1612551 ]]</td>
</tr>
<tr>
<td><strong>b2</strong></td>
<td>[[-0.88020257] [ 0.02561572] [ 0.57539477]]</td>
</tr>
</tbody></table>
<p>A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent.</p>
<ul>
<li><p><strong>(Batch) Gradient Descent</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Stochastic Gradient Descent</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will “oscillate” toward the minimum rather than converge smoothly. Here is an illustration of this:</p>
<img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/images/2.png" >
<caption>
    <center> 
        <u> 
            **Figure 1** 
        </u> 
            **: SGD vs GD**
        <br>
         “+” denotes a minimum of the cost. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD).
    </center>
</caption>




<p><strong>Note</strong> also that implementing SGD requires 3 for-loops in total: </p>
<ol>
<li>Over the number of iterations </li>
<li>Over the m training examples </li>
<li>Over the layers (to update all parameters, from ($W^{[1]}$,$b^{[1]}$) to ($W^{[L]}$,$b^{[L]}$)</li>
</ol>
<p>In practice, you’ll often get faster results if you do not use neither the whole training set, nor only one training example, to perform each update. Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples.</p>
<img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/images/3.png" >
<caption>
    <center> 
        <u> 
            **Figure 2** 
        </u> 
            **: SGD vs Mini-Batch GD**
        <br>
         “+” denotes a minimum of the cost. Using mini-batches in your optimization algorithm often leads to faster optimization.
    </center>
</caption>




<p><strong>What you should remember:</strong> </p>
<ul>
<li>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step. </li>
<li>You have to tune a learning rate hyperparameter α. </li>
<li>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</li>
</ul>
<h2 id="2-Mini-Batch-Gradient-descent"><a href="#2-Mini-Batch-Gradient-descent" class="headerlink" title="2. Mini-Batch Gradient descent"></a>2. Mini-Batch Gradient descent</h2><p>Let’s learn how to build mini-batches from the training set $(X, Y)$.</p>
<p>There are two steps: </p>
<ul>
<li><strong>Shuffle</strong>: Create a shuffled version of the training set $(X, Y)$ as shown below. Each column of $X$ and $Y$ represents a training example. Note that the random shuffling is done synchronously between $X$ and $Y$. Such that after the shuffling the ith column of $X$ is the example corresponding to the ith label in $Y$. The shuffling step ensures that examples will be split randomly into different mini-batches.</li>
</ul>
<img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/images/4.png" style="width:600px;height:400px;">



<ul>
<li><strong>Partition</strong>: Partition the shuffled $(X, Y)$ into mini-batches of size <code>mini_batch_size</code> (here 64). Note that the number of training examples is not always divisible by <code>mini_batch_size</code>. The last mini batch might be smaller, but you don’t need to worry about this. When the final mini-batch is smaller than the full <code>mini_batch_size</code>, it will look like this:</li>
</ul>
<img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/images/5.png" >


<p><strong>Exercise</strong>: Implement <code>random_mini_batches</code>. We coded the shuffling part for you. To help you with the partitioning step, we give you the following code that selects the indexes for the 1st and 2nd mini-batches:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">first_mini_batch_X = shuffled_X[:, <span class="number">0</span> : mini_batch_size]</span><br><span class="line">second_mini_batch_X = shuffled_X[:, mini_batch_size : <span class="number">2</span> * mini_batch_size]</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p><strong>Note</strong> that the last mini-batch might end up smaller than<code>mini_batch_size=64</code>. Let $\lfloor s \rfloor$ represents $s$ rounded down to the nearest integer (this is <code>math.floor(s)</code> in Python). If the total number of examples is not a multiple of <code>mini_batch_size=64</code> then there will be $\lfloor \frac{m}{mini_batch_size}\rfloor$ mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be $(m-mini__batch__size \times \lfloor \frac{m}{mini_batch_size}\rfloor)$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: random_mini_batches</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a list of random minibatches from (X, Y)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(seed)            <span class="comment"># To make your "random" minibatches the same as ours</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                  <span class="comment"># number of training examples</span></span><br><span class="line">    mini_batches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Shuffle (X, Y)</span></span><br><span class="line">    permutation = list(np.random.permutation(m))</span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span></span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + <span class="number">1</span>) * mini_batch_size];</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + <span class="number">1</span>) * mini_batch_size];</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Handling the end case (last mini-batch &lt; mini_batch_size)</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        mini_batch_X = shuffled_X[:, mini_batch_size * num_complete_minibatches : m];</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, mini_batch_size * num_complete_minibatches : m];</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()</span><br><span class="line">mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 1st mini_batch_X: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 2nd mini_batch_X: "</span> + str(mini_batches[<span class="number">1</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 3rd mini_batch_X: "</span> + str(mini_batches[<span class="number">2</span>][<span class="number">0</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 1st mini_batch_Y: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 2nd mini_batch_Y: "</span> + str(mini_batches[<span class="number">1</span>][<span class="number">1</span>].shape)) </span><br><span class="line"><span class="keyword">print</span> (<span class="string">"shape of the 3rd mini_batch_Y: "</span> + str(mini_batches[<span class="number">2</span>][<span class="number">1</span>].shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"mini batch sanity check: "</span> + str(mini_batches[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>:<span class="number">3</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>shape of the 1st mini_batch_X: (12288, 64)
shape of the 2nd mini_batch_X: (12288, 64)
shape of the 3rd mini_batch_X: (12288, 20)
shape of the 1st mini_batch_Y: (1, 64)
shape of the 2nd mini_batch_Y: (1, 64)
shape of the 3rd mini_batch_Y: (1, 20)
mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]</code></pre><p><strong>Expected Output</strong>:</p>
<table>
<thead>
<tr>
<th><strong>variabale</strong></th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td><strong>shape of the 1st mini_batch_X</strong></td>
<td>(12288, 64)</td>
</tr>
<tr>
<td><strong>shape of the 2nd mini_batch_X</strong></td>
<td>(12288, 64)</td>
</tr>
<tr>
<td><strong>shape of the 3rd mini_batch_X</strong></td>
<td>(12288, 20)</td>
</tr>
<tr>
<td><strong>shape of the 1st mini_batch_Y</strong></td>
<td>(1, 64)</td>
</tr>
<tr>
<td><strong>shape of the 2nd mini_batch_Y</strong></td>
<td>(1, 64)</td>
</tr>
<tr>
<td><strong>shape of the 3rd mini_batch_Y</strong></td>
<td>(1, 20)</td>
</tr>
<tr>
<td><strong>mini batch sanity check</strong></td>
<td>[ 0.90085595 -0.7612069 0.2344157 ]</td>
</tr>
</tbody></table>
<p><strong>What you should remember</strong>: </p>
<ul>
<li>Shuffling and Partitioning are the two steps required to build mini-batches </li>
<li>Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.</li>
</ul>
<h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3. Momentum"></a>3. Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.</p>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the “velocity” of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill.</p>
<img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/images/6.png" >
<caption>
    <center> 
        <u> 
            **Figure 3** 
        </u> 
            : The red arrows shows the direction taken by one step of mini-batch gradient descent with momentum. The blue points show the direction of the gradient (with respect to the current mini-batch) on each step. Rather than just following the gradient, we let the gradient influence $v$ and then take a step in the direction of $v$.
    </center>
</caption>

<p><strong>Exercise</strong>: Initialize the velocity. The velocity, $v$, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the <code>grads</code> dictionary, that is: </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> l=<span class="number">1</span>,...,L:</span><br><span class="line">    v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">    v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br></pre></td></tr></table></figure>
<p><strong>Note</strong> that the iterator $l$ starts at $0$ in the for loop while the first parameters are <code>v[“dW1”]</code> and <code>v[“db1”]</code> (that’s a “one” on the superscript). This is why we are shifting <code>l</code> to <code>l + 1</code> in the <code>for</code> loop.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_velocity</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize velocity</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)].shape);</span><br><span class="line">        v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)].shape);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_velocity_test_case()</span><br><span class="line"></span><br><span class="line">v = initialize_velocity(parameters)</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>v[&quot;dW1&quot;] = [[0. 0. 0.]
 [0. 0. 0.]]
v[&quot;db1&quot;] = [[0.]
 [0.]]
v[&quot;dW2&quot;] = [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
v[&quot;db2&quot;] = [[0.]
 [0.]
 [0.]]</code></pre><p><strong>Expected Output</strong>:</p>
<table>
<thead>
<tr>
<th><strong>variabale</strong></th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td><strong>v[“dW1”]</strong></td>
<td>[[ 0. 0. 0.] [ 0. 0. 0.]]</td>
</tr>
<tr>
<td><strong>v[“db1”]</strong></td>
<td>[[ 0.] [ 0.]]</td>
</tr>
<tr>
<td><strong>v[“dW2”]</strong></td>
<td>[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]</td>
</tr>
<tr>
<td><strong>v[“db2”]</strong></td>
<td>[[ 0.] [ 0.] [ 0.]]</td>
</tr>
</tbody></table>
<p><strong>Exercise</strong>: Now, implement the parameters update with momentum. The momentum update rule is, <code>for l=1,...,L</code>:<br>$$<br>\begin{cases}<br>v_{dW^{[l]}} = \beta v_{dW^{[l]}} + (1 - \beta) dW^{[l]} \<br>W^{[l]} = W^{[l]} - \alpha v_{dW^{[l]}}<br>\end{cases}\tag{3}<br>$$<br>$$<br>\begin{cases}<br>v_{db^{[l]}} = \beta v_{db^{[l]}} + (1 - \beta) db^{[l]} \<br>b^{[l]} = b^{[l]} - \alpha v_{db^{[l]}}<br>\end{cases}\tag{4}<br>$$</p>
<p>where $L$ is the number of layers, $β$ is the momentum and $α$ is the learning rate. All parameters should be stored in the <code>parameters</code> dictionary. <strong>Note</strong> that the iterator $l$ starts at $0$ in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$ (that’s a “one” on the superscript). So you will need to shift $l$ to $l + 1$ when coding.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_momentum</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Momentum</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Momentum update for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line"></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">        <span class="comment"># compute velocities</span></span><br><span class="line">        v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = beta * v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = beta * v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta) * grads[<span class="string">'db'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        <span class="comment"># update parameters</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)] -= learning_rate * v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)] -= learning_rate * v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v = update_parameters_with_momentum_test_case()</span><br><span class="line"></span><br><span class="line">parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = <span class="number">0.9</span>, learning_rate = <span class="number">0.01</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 1.62544598 -0.61290114 -0.52907334]
 [-1.07347112  0.86450677 -2.30085497]]
b1 = [[ 1.74493465]
 [-0.76027113]]
W2 = [[ 0.31930698 -0.24990073  1.4627996 ]
 [-2.05974396 -0.32173003 -0.38320915]
 [ 1.13444069 -1.0998786  -0.1713109 ]]
b2 = [[-0.87809283]
 [ 0.04055394]
 [ 0.58207317]]
v[&quot;dW1&quot;] = [[-0.11006192  0.11447237  0.09015907]
 [ 0.05024943  0.09008559 -0.06837279]]
v[&quot;db1&quot;] = [[-0.01228902]
 [-0.09357694]]
v[&quot;dW2&quot;] = [[-0.02678881  0.05303555 -0.06916608]
 [-0.03967535 -0.06871727 -0.08452056]
 [-0.06712461 -0.00126646 -0.11173103]]
v[&quot;db2&quot;] = [[0.02344157]
 [0.16598022]
 [0.07420442]]</code></pre><p><strong>Expected Output</strong>:</p>
<table>
<thead>
<tr>
<th><strong>variable</strong></th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td><strong>W1</strong></td>
<td>[[ 1.62544598 -0.61290114 -0.52907334] [-1.07347112 0.86450677 -2.30085497]]</td>
</tr>
<tr>
<td><strong>b1</strong></td>
<td>[[ 1.74493465] [-0.76027113]]</td>
</tr>
<tr>
<td><strong>W2</strong></td>
<td>[[ 0.31930698 -0.24990073 1.4627996 ] [-2.05974396 -0.32173003 -0.38320915] [ 1.13444069 -1.0998786 -0.1713109 ]]</td>
</tr>
<tr>
<td><strong>b2</strong></td>
<td>[[-0.87809283] [ 0.04055394] [ 0.58207317]]</td>
</tr>
<tr>
<td><strong>v[“dW1”]</strong></td>
<td>[[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]]</td>
</tr>
<tr>
<td><strong>v[“db1”]</strong></td>
<td>[[-0.01228902] [-0.09357694]]</td>
</tr>
<tr>
<td><strong>v[“dW2”]</strong></td>
<td>[[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]]</td>
</tr>
<tr>
<td><strong>v[“db2”]</strong></td>
<td>[[ 0.02344157][ 0.16598022] [ 0.07420442]]</td>
</tr>
</tbody></table>
<p><strong>Note</strong> that: </p>
<ul>
<li>The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps. </li>
<li>If $β=0$, then this just becomes standard gradient descent without momentum.</li>
</ul>
<p><strong>How do you choose $β$</strong>?</p>
<p>The larger the momentum $β$ is, the smoother the update because the more we take the past gradients into account. But if $β$ is too big, it could also smooth out the updates too much.<br>Common values for $β$ range from 0.8 to 0.999. If you don’t feel inclined to tune this, $β=0.9$ is often a reasonable default.<br>Tuning the optimal $β$ for your model might need trying several values to see what works best in term of reducing the value of the cost function $J$.</p>
<p><strong>What you should remember</strong>: </p>
<ul>
<li>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent. </li>
<li>You have to tune a momentum hyperparameter $β$ and a learning rate $α$.</li>
</ul>
<h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4. Adam"></a>4. Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.</p>
<p>*<em>How does Adam work? *</em></p>
<ol>
<li>It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). </li>
<li>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). </li>
<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>
</ol>
<p>The update rule is, <code>for l=1,...,L</code>:<br>$$\begin{cases}<br>v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \<br>v^{corrected}<em>{dW^{[l]}} = \frac{v</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \<br>s^{corrected}<em>{dW^{[l]}} = \frac{s</em>{dW^{[l]}}}{1 - (\beta_2)^t} \<br>W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}<em>{dW^{[l]}}}{\sqrt{s^{corrected}</em>{dW^{[l]}}} + \varepsilon}<br>\end{cases}$$</p>
<p>where: </p>
<ul>
<li>$t$ counts the number of steps taken of Adam </li>
<li>$L$ is the number of layers </li>
<li>$β_1$ and $β_2$ are hyperparameters that control the two exponentially weighted averages. </li>
<li>$α$ is the learning rate </li>
<li>$ε$ is a very small number to avoid dividing by zero<br>As usual, we will store all parameters in the parameters dictionary</li>
</ul>
<p><strong>Exercise</strong>: Initialize the Adam variables $v,s$ which keep track of the past information.</p>
<p><strong>Instruction</strong>: The variables $v,s$ are python dictionaries that need to be initialized with arrays of zeros. Their keys are the same as for grads, that is:<br><code>for l=1,...,L</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br><span class="line">s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["W" + str(l+1)])</span></span><br><span class="line">s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = ... <span class="comment">#(numpy array of zeros with the same shape as parameters["b" + str(l+1)])</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_adam</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span> :</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes v and s as two python dictionaries with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters["W" + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters["b" + str(l)] = bl</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class="line"><span class="string">                    v["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class="line"><span class="string">                    s["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s["db" + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize v, s. Input: "parameters". Outputs: "v, s".</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">        v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)].shape);</span><br><span class="line">        v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)].shape);</span><br><span class="line">        s[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)].shape);</span><br><span class="line">        s[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)].shape);   </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_adam_test_case()</span><br><span class="line"></span><br><span class="line">v, s = initialize_adam(parameters)</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW1\"] = "</span> + str(s[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db1\"] = "</span> + str(s[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW2\"] = "</span> + str(s[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db2\"] = "</span> + str(s[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>v[&quot;dW1&quot;] = [[0. 0. 0.]
 [0. 0. 0.]]
v[&quot;db1&quot;] = [[0.]
 [0.]]
v[&quot;dW2&quot;] = [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
v[&quot;db2&quot;] = [[0.]
 [0.]
 [0.]]
s[&quot;dW1&quot;] = [[0. 0. 0.]
 [0. 0. 0.]]
s[&quot;db1&quot;] = [[0.]
 [0.]]
s[&quot;dW2&quot;] = [[0. 0. 0.]
 [0. 0. 0.]
 [0. 0. 0.]]
s[&quot;db2&quot;] = [[0.]
 [0.]
 [0.]]</code></pre><p><strong>Expected Output</strong>:</p>
<table>
<thead>
<tr>
<th align="left">variable</th>
<th align="left">value</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>v[“dW1”]</strong></td>
<td align="left">[[ 0. 0. 0.] [ 0. 0. 0.]]</td>
</tr>
<tr>
<td align="left"><strong>v[“db1”]</strong></td>
<td align="left">[[ 0.] [ 0.]]</td>
</tr>
<tr>
<td align="left"><strong>v[“dW2”]</strong></td>
<td align="left">[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]</td>
</tr>
<tr>
<td align="left"><strong>v[“db2”]</strong></td>
<td align="left">[[ 0.] [ 0.] [ 0.]]</td>
</tr>
<tr>
<td align="left"><strong>s[“dW1”]</strong></td>
<td align="left">[[ 0. 0. 0.] [ 0. 0. 0.]]</td>
</tr>
<tr>
<td align="left"><strong>s[“db1”]</strong></td>
<td align="left">[[ 0.] [ 0.]]</td>
</tr>
<tr>
<td align="left"><strong>s[“dW2”]</strong></td>
<td align="left">[[ 0. 0. 0.] [ 0. 0. 0.] [ 0. 0. 0.]]</td>
</tr>
<tr>
<td align="left"><strong>s[“db2”]</strong></td>
<td align="left">[[ 0.] [ 0.] [ 0.]]</td>
</tr>
</tbody></table>
<p><strong>Exercise</strong>: Now, implement the parameters update with Adam. Recall the general update rule is, <code>for l=1,...,L</code>:<br>$$\begin{cases}<br>v_{dW^{[l]}} = \beta_1 v_{dW^{[l]}} + (1 - \beta_1) \frac{\partial \mathcal{J} }{ \partial W^{[l]} } \<br>v^{corrected}<em>{dW^{[l]}} = \frac{v</em>{dW^{[l]}}}{1 - (\beta_1)^t} \<br>s_{dW^{[l]}} = \beta_2 s_{dW^{[l]}} + (1 - \beta_2) (\frac{\partial \mathcal{J} }{\partial W^{[l]} })^2 \<br>s^{corrected}<em>{dW^{[l]}} = \frac{s</em>{dW^{[l]}}}{1 - (\beta_2)^t} \<br>W^{[l]} = W^{[l]} - \alpha \frac{v^{corrected}<em>{dW^{[l]}}}{\sqrt{s^{corrected}</em>{dW^{[l]}}} + \varepsilon}<br>\end{cases}$$</p>
<p><strong>Note</strong> that the iterator <code>l</code> starts at <code>0</code> in the for loop while the first parameters are $W^{[1]}$ and $b^{[1]}$. You need to shift <code>l</code> to <code>l+1</code> when coding.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters_with_adam</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Adam</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                 <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v_corrected = &#123;&#125;                         <span class="comment"># Initializing first moment estimate, python dictionary</span></span><br><span class="line">    s_corrected = &#123;&#125;                         <span class="comment"># Initializing second moment estimate, python dictionary</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Perform Adam update on all parameters</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        <span class="comment"># Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = beta1 * v[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = beta1 * v[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta1) * grads[<span class="string">'db'</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        v_corrected[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = v[<span class="string">'dW'</span> +  str(l + <span class="number">1</span>)] / (<span class="number">1</span> - np.power(beta1, t));</span><br><span class="line">        v_corrected[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = v[<span class="string">'db'</span> +  str(l + <span class="number">1</span>)] / (<span class="number">1</span> - np.power(beta1, t));</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] = beta2 * s[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.power(grads[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)], <span class="number">2</span>);</span><br><span class="line">        s[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] = beta2 * s[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] + (<span class="number">1</span> - beta2) * np.power(grads[<span class="string">'db'</span> + str(l + <span class="number">1</span>)], <span class="number">2</span>);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        s_corrected[<span class="string">'dW'</span> +  str(l + <span class="number">1</span>)] = s[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] / (<span class="number">1</span> - np.power(beta2, t));</span><br><span class="line">        s_corrected[<span class="string">'db'</span> +  str(l + <span class="number">1</span>)] = s[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] / (<span class="number">1</span> - np.power(beta2, t));</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l + <span class="number">1</span>)] -= learning_rate * v_corrected[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)] / (np.sqrt(s_corrected[<span class="string">'dW'</span> + str(l + <span class="number">1</span>)]) + epsilon);</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l + <span class="number">1</span>)] -= learning_rate * v_corrected[<span class="string">'db'</span> + str(l + <span class="number">1</span>)] / (np.sqrt(s_corrected[<span class="string">'db'</span> + str(l + <span class="number">1</span>)]) + epsilon);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads, v, s = update_parameters_with_adam_test_case()</span><br><span class="line">parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW1\"] = "</span> + str(v[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db1\"] = "</span> + str(v[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"v[\"dW2\"] = "</span> + str(v[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"v[\"db2\"] = "</span> + str(v[<span class="string">"db2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW1\"] = "</span> + str(s[<span class="string">"dW1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db1\"] = "</span> + str(s[<span class="string">"db1"</span>]))</span><br><span class="line">print(<span class="string">"s[\"dW2\"] = "</span> + str(s[<span class="string">"dW2"</span>]))</span><br><span class="line">print(<span class="string">"s[\"db2\"] = "</span> + str(s[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 1.63178673 -0.61919778 -0.53561312]
 [-1.08040999  0.85796626 -2.29409733]]
b1 = [[ 1.75225313]
 [-0.75376553]]
W2 = [[ 0.32648046 -0.25681174  1.46954931]
 [-2.05269934 -0.31497584 -0.37661299]
 [ 1.14121081 -1.09244991 -0.16498684]]
b2 = [[-0.88529979]
 [ 0.03477238]
 [ 0.57537385]]
v[&quot;dW1&quot;] = [[-0.11006192  0.11447237  0.09015907]
 [ 0.05024943  0.09008559 -0.06837279]]
v[&quot;db1&quot;] = [[-0.01228902]
 [-0.09357694]]
v[&quot;dW2&quot;] = [[-0.02678881  0.05303555 -0.06916608]
 [-0.03967535 -0.06871727 -0.08452056]
 [-0.06712461 -0.00126646 -0.11173103]]
v[&quot;db2&quot;] = [[0.02344157]
 [0.16598022]
 [0.07420442]]
s[&quot;dW1&quot;] = [[0.00121136 0.00131039 0.00081287]
 [0.0002525  0.00081154 0.00046748]]
s[&quot;db1&quot;] = [[1.51020075e-05]
 [8.75664434e-04]]
s[&quot;dW2&quot;] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]
 [1.57413361e-04 4.72206320e-04 7.14372576e-04]
 [4.50571368e-04 1.60392066e-07 1.24838242e-03]]
s[&quot;db2&quot;] = [[5.49507194e-05]
 [2.75494327e-03]
 [5.50629536e-04]]</code></pre><p><strong>Expected Output</strong>:</p>
<table>
<thead>
<tr>
<th align="left">variable</th>
<th align="left">value</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>W1</strong></td>
<td align="left">[[ 1.63178673 -0.61919778 -0.53561312] [-1.08040999 0.85796626 -2.29409733]]</td>
</tr>
<tr>
<td align="left"><strong>b1</strong></td>
<td align="left">[[ 1.75225313] [-0.75376553]]</td>
</tr>
<tr>
<td align="left"><strong>W2</strong></td>
<td align="left">[[ 0.32648046 -0.25681174 1.46954931] [-2.05269934 -0.31497584 -0.37661299] [ 1.14121081 -1.09245036 -0.16498684]]</td>
</tr>
<tr>
<td align="left"><strong>b2</strong></td>
<td align="left">[[-0.88529978] [ 0.03477238] [ 0.57537385]]</td>
</tr>
<tr>
<td align="left"><strong>v[“dW1”]</strong></td>
<td align="left">[[-0.11006192 0.11447237 0.09015907] [ 0.05024943 0.09008559 -0.06837279]]</td>
</tr>
<tr>
<td align="left"><strong>v[“db1”]</strong></td>
<td align="left">[[-0.01228902] [-0.09357694]]</td>
</tr>
<tr>
<td align="left"><strong>v[“dW2”]</strong></td>
<td align="left">[[-0.02678881 0.05303555 -0.06916608] [-0.03967535 -0.06871727 -0.08452056] [-0.06712461 -0.00126646 -0.11173103]]</td>
</tr>
<tr>
<td align="left"><strong>v[“db2”]</strong></td>
<td align="left">[[ 0.02344157] [ 0.16598022] [ 0.07420442]]</td>
</tr>
<tr>
<td align="left"><strong>s[“dW1”]</strong></td>
<td align="left">[[ 0.00121136 0.00131039 0.00081287] [ 0.0002525 0.00081154 0.00046748]]</td>
</tr>
<tr>
<td align="left"><strong>s[“db1”]</strong></td>
<td align="left">[[ 1.51020075e-05] [ 8.75664434e-04]]</td>
</tr>
<tr>
<td align="left"><strong>s[“dW2”]</strong></td>
<td align="left">[[ 7.17640232e-05 2.81276921e-04 4.78394595e-04] [ 1.57413361e-04 4.72206320e-04 7.14372576e-04] [ 4.50571368e-04 1.60392066e-07 1.24838242e-03]]</td>
</tr>
<tr>
<td align="left"><strong>s[“db2”]</strong></td>
<td align="left">[[ 5.49507194e-05] [ 2.75494327e-03] [ 5.50629536e-04]]</td>
</tr>
</tbody></table>
<p>You now have three working optimization algorithms (mini-batch gradient descent, Momentum, Adam). Let’s implement a model with each of these optimizers and observe the difference.</p>
<h2 id="5-Model-with-different-optimization-algorithms"><a href="#5-Model-with-different-optimization-algorithms" class="headerlink" title="5 - Model with different optimization algorithms"></a>5 - Model with different optimization algorithms</h2><p>Lets use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y = load_dataset()</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/output_21_0.png" alt="png"></p>
<p>We have already implemented a 3-layer neural network. You will train it with: </p>
<ul>
<li>Mini-batch Gradient Descent: it will call your function: <code>update_parameters_with_gd()</code> </li>
<li>Mini-batch Momentum: it will call your functions: <code>initialize_velocity()</code> and <code>update_parameters_with_momentum()</code> </li>
<li>Mini-batch Adam: it will call your functions: <code>initialize_adam()</code> and <code>update_parameters_with_adam()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate = <span class="number">0.0007</span>, mini_batch_size = <span class="number">64</span>, beta = <span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- python list, containing the size of each layer</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    mini_batch_size -- the size of a mini batch</span></span><br><span class="line"><span class="string">    beta -- Momentum hyperparameter</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(layers_dims)             <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    costs = []                       <span class="comment"># to keep track of the cost</span></span><br><span class="line">    t = <span class="number">0</span>                            <span class="comment"># initializing the counter required for Adam update</span></span><br><span class="line">    seed = <span class="number">10</span>                        <span class="comment"># For grading purposes, so that your "random" minibatches are the same as ours</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the optimizer</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># no initialization required for gradient descent</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a minibatch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost</span></span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward propagation</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># Adam counter</span></span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line">                                                               t, learning_rate, beta1, beta2,  epsilon)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 1000 epoch</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<p>You will now run this 3 layer neural network with each of the 3 optimization methods.</p>
<h3 id="5-1-Mini-batch-Gradient-descent"><a href="#5-1-Mini-batch-Gradient-descent" class="headerlink" title="5.1 Mini-batch Gradient descent"></a>5.1 Mini-batch Gradient descent</h3><p>Run the following code to see how the model does with mini-batch gradient descent.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>

<pre><code>Cost after epoch 0: 0.690736
Cost after epoch 1000: 0.685273
Cost after epoch 2000: 0.647072
Cost after epoch 3000: 0.619525
Cost after epoch 4000: 0.576584
Cost after epoch 5000: 0.607243
Cost after epoch 6000: 0.529403
Cost after epoch 7000: 0.460768
Cost after epoch 8000: 0.465586
Cost after epoch 9000: 0.464518</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/output_25_1.png" alt="png"></p>
<pre><code>Accuracy: 0.7966666666666666</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/output_25_3.png" alt="png"></p>
<h3 id="5-2-Mini-batch-gradient-descent-with-momentum"><a href="#5-2-Mini-batch-gradient-descent-with-momentum" class="headerlink" title="5.2 Mini-batch gradient descent with momentum"></a>5.2 Mini-batch gradient descent with momentum</h3><p>Run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>

<pre><code>Cost after epoch 0: 0.690741
Cost after epoch 1000: 0.685341
Cost after epoch 2000: 0.647145
Cost after epoch 3000: 0.619594
Cost after epoch 4000: 0.576665
Cost after epoch 5000: 0.607324
Cost after epoch 6000: 0.529476
Cost after epoch 7000: 0.460936
Cost after epoch 8000: 0.465780
Cost after epoch 9000: 0.464740</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/output_27_1.png" alt="png"></p>
<pre><code>Accuracy: 0.7966666666666666</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/output_27_3.png" alt="png"></p>
<h3 id="5-3-Mini-batch-with-Adam-mode"><a href="#5-3-Mini-batch-with-Adam-mode" class="headerlink" title="5.3 Mini-batch with Adam mode"></a>5.3 Mini-batch with Adam mode</h3><p>Run the following code to see how the model does with Adam.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>

<pre><code>Cost after epoch 0: 0.690552
Cost after epoch 1000: 0.185567
Cost after epoch 2000: 0.150852
Cost after epoch 3000: 0.074454
Cost after epoch 4000: 0.125936
Cost after epoch 5000: 0.104235
Cost after epoch 6000: 0.100552
Cost after epoch 7000: 0.031601
Cost after epoch 8000: 0.111709
Cost after epoch 9000: 0.197648</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/output_29_1.png" alt="png"></p>
<pre><code>Accuracy: 0.94</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week2/output_29_3.png" alt="png"></p>
<h3 id="5-4-Summary"><a href="#5-4-Summary" class="headerlink" title="5.4 Summary"></a>5.4 Summary</h3><table>
<thead>
<tr>
<th><strong>optimization method</strong></th>
<th><strong>accuracy</strong></th>
<th><strong>cost shape</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Gradient descent</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Momentum</td>
<td>79.7%</td>
<td>oscillations</td>
</tr>
<tr>
<td>Adam</td>
<td>94%</td>
<td>smoother</td>
</tr>
</tbody></table>
<p>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.</p>
<p>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</p>
<p>Some advantages of Adam include:<br>- Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum)<br>- Usually works well even with little tuning of hyperparameters (except $α$)</p>
<p><strong>References</strong>:</p>
<ul>
<li>Adam paper: <a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1412.6980.pdf</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/03/02/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/02/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/" class="post-title-link" itemprop="url">03_hyperparameter-tuning-batch-normalization-and-programming-frameworks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-02 00:00:00" itemprop="dateCreated datePublished" datetime="2018-03-02T00:00:00+05:30">2018-03-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>74k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:07</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the first week after studying the course <a href="https://www.coursera.org/learn/deep-neural-network/" target="_blank" rel="noopener">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="01-hyperparameter-tuning"><a href="#01-hyperparameter-tuning" class="headerlink" title="01_hyperparameter-tuning"></a>01_hyperparameter-tuning</h2><h3 id="01-tuning-process"><a href="#01-tuning-process" class="headerlink" title="01_tuning-process"></a>01_tuning-process</h3><p>Hi, and welcome back. You’ve seen by now that changing neural nets can involve setting a lot of different hyperparameters. Now, how do you go about finding a good setting for these hyperparameters? In this video, I want to share with you some guidelines, some tips for how to systematically organize your hyperparameter tuning process, which hopefully will make it more efficient for you to converge on a good setting of the hyperparameters. </p>
<p>One of the painful things about training deepness is the sheer number of hyperparameters you have to deal with, ranging from the learning rate alpha to the momentum term beta, if using momentum, or the hyperparameters for the Adam Optimization Algorithm which are beta one, beta two, and epsilon. Maybe you have to pick the number of layers, maybe you have to pick the number of hidden units for the different layers, and maybe you want to use learning rate decay, so you don’t just use a single learning rate alpha. And then of course, you might need to choose the mini-batch size. So it turns out, some of these hyperparameters are more important than others.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/1.png" alt=""><br>The most learning applications I would say, alpha, <strong>the learning rate is the most important hyperparameter to tune</strong>. Other than alpha, <strong>a few other hyperparameters I tend to would maybe tune next, would be maybe the momentum term, say, 0.9 is a good default. I’d also tune the mini-batch size to make sure that the optimization algorithm is running efficiently. Often I also fiddle around with the hidden units. Of the ones I’ve circled in orange, these are really the three that I would consider second in importance to the learning rate alpha and then third in importance after fiddling around with the others, the number of layers can sometimes make a huge difference, and so can learning rate decay. And then when using the Adam algorithm I actually pretty much never tuned beta one, beta two, and epsilon. Pretty much I always use 0.9, 0.999 and tenth minus eight although you can try tuning those as well if you wish. But hopefully it does give you some rough sense of what hyperparameters might be more important than others, alpha, most important, for sure, followed maybe by the ones I’ve circle in orange, followed maybe by the ones I circled in purple. But this isn’t a hard and fast rule and I think other deep learning practitioners may well disagree with me or have different intuitions on these.</strong> </p>
<p>Now, if you’re trying to tune some set of hyperparameters, how do you select a set of values to explore? In earlier generations of machine learning algorithms, if you had two hyperparameters, which I’m calling hyperparameter one and hyperparameter two here, it was common practice to <strong>sample the points in a grid</strong> like so and systematically explore these values. Here I am placing down a five by five grid (Tip: as the left image on the following slide). In practice, it could be more or less than the five by five grid but you try out in this example all 25 points and then pick whichever hyperparameter works best. <strong>And this practice works okay when the number of hyperparameters was relatively small</strong>.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/2.png" alt=""><br><strong>In deep learning, what we tend to do, and what I recommend you do instead, is choose the points at random</strong>. So go ahead and choose maybe of same number of points, right? 25 points and then try out the hyperparameters on this randomly chosen set of points. <strong>And the reason you do that is that it’s difficult to know in advance which hyperparameters are going to be the most important for your problem</strong>. And as you saw in the previous slide, some hyperparameters are actually much more important than others. <strong>So to take an example, let’s say hyperparameter one turns out to be alpha, the learning rate. And to take an extreme example, let’s say that hyperparameter two was that value epsilon that you have in the denominator of the Adam algorithm</strong>. So your choice of alpha matters a lot and your choice of epsilon hardly matters. <strong>So if you sample in the grid then you’ve really tried out five values of alpha and you might find that all of the different values of epsilon give you essentially the same answer. So you’ve now trained 25 models and only got into trial five values for the learning rate alpha, which I think is really important. Whereas in contrast, if you were to sample at random, then you will have tried out 25 distinct values of the learning rate alpha and therefore you be more likely to find a value that works really well</strong>. I’ve explained this example, using just two hyperparameters. In practice, you might be searching over many more hyperparameters than these, so if you have, say, three hyperparameters, I guess instead of searching over a square, you’re searching over a cube where this third dimension is hyperparameter three and then by sampling within this three-dimensional cube you get to try out a lot more values of each of your three hyperparameters.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/3.png" alt=""><br><strong>And in practice you might be searching over even more hyperparameters than three and sometimes it’s just hard to know in advance which ones turn out to be the really important hyperparameters for your application and sampling at random rather than in the grid shows that you are more richly exploring set of possible values for the most important hyperparameters, whatever they turn out to be</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/4.png" alt=""><br>When you sample hyperparameters, <strong>another common practice is to use a coarse to fine sampling scheme</strong>. So let’s say in this two-dimensional example that you sample these points, and maybe you found that this point work the best and maybe a few other points(circled in blue color on the right down the above slide) around it tended to work really well, then in the course of the final scheme what you might do is zoom in to a smaller region of the hyperparameters and then sample more density within this space. Or maybe again at random, but to then focus more resources on searching within this blue square if you’re suspecting that the best setting, the hyperparameters, may be in this region. So after doing a coarse sample of this entire square, that tells you to then focus on a smaller square. You can then sample more densely into smaller square. So this type of a coarse to fine search is also frequently used. And by trying out these different values of the hyperparameters you can then pick whatever value allows you to do best on your training set objective or does best on your development set or whatever you’re trying to optimize in your hyperparameter search process. </p>
<p>So I hope this gives you a way to more systematically organize your hyperparameter search process. <strong>The two key takeaways are, use random sampling and adequate search and optionally consider implementing a coarse to fine search process.</strong> But there’s even more to hyperparameter search than this. Let’s talk more in the next video about how to choose the right scale on which to sample your hyperparameters.</p>
<h3 id="02-using-an-appropriate-scale-to-pick-hyperparameters"><a href="#02-using-an-appropriate-scale-to-pick-hyperparameters" class="headerlink" title="02_using-an-appropriate-scale-to-pick-hyperparameters"></a>02_using-an-appropriate-scale-to-pick-hyperparameters</h3><p>In the last video, you saw how sampling at random, over the range of hyperparameters, can allow you to search over the space of hyperparameters more efficiently. But it turns out that sampling at random doesn’t mean sampling uniformly at random, over the range of valid values. Instead, it’s important to pick the appropriate scale on which to explore the hyperparamaters. In this video, I want to show you how to do that. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/5.png" alt=""><br>Let’s say that you’re trying to choose the number of hidden units, n[l], for a given layer l. And let’s say that you think a good range of values is somewhere from 50 to 100. In that case, if you look at the number line from 50 to 100, maybe picking some number values at random within this number line. There’s a pretty visible way to search for this particular hyperparameter. Or if you’re trying to decide on the number of layers in your neural network, we’re calling that capital L. Maybe you think the total number of layers should be somewhere between 2 to 4. Then sampling uniformly at random, along 2, 3 and 4, might be reasonable. Or even using a grid search, where you explicitly evaluate the values 2, 3 and 4 might be reasonable. <strong>So these were a couple examples where sampling uniformly at random over the range you’re contemplating, might be a reasonable thing to do. But this is not true for all hyperparameters</strong>. </p>
<p>Let’s look at another example. Say your searching for the hyperparameter alpha, the learning rate. And let’s say that you suspect 0.0001 might be on the low end, or maybe it could be as high as 1. Now if you draw the number line from 0.0001 to 1, and sample values uniformly at random over this number line. Well about 90% of the values you sample would be between 0.1 and 1. So you’re using 90% of the resources to search between 0.1 and 1, and only 10% of the resources to search between 0.0001 and 0.1. So that doesn’t seem right.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/6.png" alt=""><br>Instead, it seems more reasonable to search for hyperparameters on a log scale. Where instead of using a linear scale, you’d have 0.0001 here, and then 0.001, 0.01, 0.1, and then 1. And you instead sample uniformly, at random, on this type of <strong>logarithmic</strong> scale. Now you have more resources dedicated to searching between 0.0001 and 0.001, and between 0.001 and 0.01, and so on. So in Python, the way you implement this, is let r = -4 * np.random.rand(). And then a randomly chosen value of alpha, would be alpha = 10 to the power of r. So after this first line, r will be a random number between -4 and 0. And so alpha here will be between 10 to the -4 and 10 to the 0. So 10 to the -4 is this left thing, this 10 to the -4. And 1 is 10 to the 0. In a more general case, if you’re trying to sample between 10 to the a, to 10 to the b, on the log scale. And in this example, this is 10 to the a. And you can figure out what a is by taking the log base 10 of 0.0001, which is going to tell you a is -4. And this value on the right, this is 10 to the b. And you can figure out what b is, by taking log base 10 of 1, which tells you b is equal to 0. So what you do, is then sample r uniformly, at random, between a and b. So in this case, r would be between -4 and 0. And you can set alpha, on your randomly sampled hyperparameter value, as 10 to the r, okay? So just to recap, to sample on the log scale, you take the low value, take logs to figure out what is a. Take the high value, take a log to figure out what is b. So now you’re trying to sample, from 10 to the a to the b, on a log scale. So you set r uniformly, at random, between a and b. And then you set the hyperparameter to be 10 to the r. So that’s how you implement sampling on this logarithmic scale. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/7.png" alt=""><br><strong>Finally, one other tricky case is sampling the hyperparameter beta, used for computing exponentially weighted averages</strong>. So let’s say you suspect that beta should be somewhere between 0.9 to 0.999. Maybe this is the range of values you want to search over. So remember, that when computing exponentially weighted averages, using 0.9 is like averaging over the last 10 values. kind of like taking the average of 10 days temperature, whereas using 0.999 is like averaging over the last 1,000 values. So similar to what we saw on the last slide, if you want to search between 0.9 and 0.999, it doesn’t make sense to sample on the linear scale, right? Uniformly, at random, between 0.9 and 0.999. So the best way to think about this, is that we want to explore the range of values for 1 minus beta, which is going to now range from 0.1 to 0.001. And so we’ll sample the between beta, taking values from 0.1, to maybe 0.1, to 0.001. So using the method we have figured out on the previous slide, this is 10 to the -1, this is 10 to the -3. Notice on the previous slide, we had the small value on the left, and the large value on the right, but here we have reversed. We have the large value on the left, and the small value on the right. So what you do, is you sample r uniformly, at random, from -3 to -1. And you set 1- beta = 10 to the r, and so beta = 1- 10 to the r. And this becomes your randomly sampled value of your hyperparameter, chosen on the appropriate scale. And hopefully this makes sense, in that this way, you spend as much resources exploring the range 0.9 to 0.99, as you would exploring 0.99 to 0.999. So if you want to study more formal mathematical justification for why we’re doing this, right, <strong>why is it such a bad idea to sample in a linear scale? It is that, when beta is close to 1, the sensitivity of the results you get changes, even with very small changes to beta</strong>. So if beta goes from 0.9 to 0.9005, it’s no big deal, this is <strong>hardly any change</strong> in your results. But if beta goes from 0.999 to 0.9995, this will have <strong>a huge impact</strong> on exactly what your algorithm is doing, right? <strong>In both of these cases, it’s averaging over roughly 10 values. But here it’s gone from an exponentially weighted average over about the last 1,000 examples, to now, the last 2,000 examples</strong>. And it’s because that formula we have, 1 / 1- beta, this is <strong>very sensitive</strong> to small changes in beta, when beta is close to 1. So what this whole sampling process does, is it causes you to sample more densely in the region of when beta is close to 1. Or, alternatively, when 1- beta is close to 0. So that you can be more efficient in terms of how you distribute the samples, to explore the space of possible outcomes more efficiently. </p>
<p>So I hope this helps you select the right scale on which to sample the hyperparameters. In case you don’t end up making the right scaling decision on some hyperparameter choice, don’t worry to much about it. Even if you sample on the uniform scale, where sum of the scale would have been superior, you might still get okay results. Especially if you use a coarse to fine search, so that in later iterations, you focus in more on the most useful range of hyperparameter values to sample. I hope this helps you in your hyperparameter search. In the next video, I also want to share with you some thoughts of how to organize your hyperparameter search process. That I hope will make your workflow a bit more efficient. </p>
<h3 id="03-hyperparameters-tuning-in-practice-pandas-vs-caviar"><a href="#03-hyperparameters-tuning-in-practice-pandas-vs-caviar" class="headerlink" title="03_hyperparameters-tuning-in-practice-pandas-vs-caviar"></a>03_hyperparameters-tuning-in-practice-pandas-vs-caviar</h3><p>You have now heard a lot about how to search for good hyperparameters. Before wrapping up our discussion on hyperparameter search, I want to share with you just a couple of final tips and tricks for how to organize your hyperparameter search process. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/8.png" alt=""><br>Deep learning today is applied to many different application areas and that <strong>intuitions about hyperparameter settings from one application area may or may not transfer to a different one. There is a lot of cross-fertilization among different applications’ domains</strong>, so for example, I’ve seen ideas developed in the <strong>computer vision community</strong>, such as <strong>Confonets</strong> or <strong>ResNets</strong>, which we’ll talk about in a later course, successfully applied to speech. I’ve seen ideas that were first developed in speech successfully applied in <strong>NLP</strong>, and so on. <strong>So one nice development in deep learning is that people from different application domains do read increasingly research papers from other application domains to look for inspiration for cross-fertilization</strong>. In terms of your settings for the hyperparameters, though, I’ve seen that intuitions do get stale. So even if you work on just one problem, say logistics, you might have found a good setting for the hyperparameters and kept on developing your algorithm, or maybe seen your data gradually change over the course of several months, or maybe just upgraded servers in your data center. And because of those changes, the best setting of your hyperparameters can get stale. So I recommend maybe just retesting or reevaluating your hyperparameters at least once every several months to make sure that you’re still happy with the values you have. </p>
<p><strong>Finally, in terms of how people go about searching for hyperparameters, I see maybe two major schools of thought, or maybe two major different ways in which people go about it.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/9.png" alt=""><br>One way is if you <strong>babysit one model</strong>. And usually you do this <strong>if you have maybe a huge data set but not a lot of computational resources, not a lot of CPUs and GPUs, so you can basically afford to train only one model or a very small number of models at a time.</strong> In that case you might gradually babysit that model even as it’s training. So, for example, on Day 0 you might initialize your parameter as random and then start training. And you gradually watch your learning curve, maybe the cost function J or your dataset error or something else, gradually decrease over the first day. Then at the end of day one, you might say, gee, looks it’s learning quite well, I’m going to try increasing the learning rate a little bit and see how it does. And then maybe it does better. And then that’s your Day 2 performance. And after two days you say, okay, it’s still doing quite well. Maybe I’ll fill the momentum term a bit or decrease the learning variable a bit now, and then you’re now into Day 3. And every day you kind of look at it and try nudging up and down your parameters. And maybe on one day you found your learning rate was too big. So you might go back to the previous day’s model, and so on. But you’re kind of babysitting the model one day at a time even as it’s training over a course of many days or over the course of several different weeks. So that’s one approach, and people that babysit one model, that is watching performance and patiently nudging the learning rate up or down. But that’s usually what happens if you don’t have enough computational capacity to train a lot of models at the same time. </p>
<p><strong>The other approach would be if you train many models in parallel</strong>. So you might have some setting of the hyperparameters and just let it run by itself ,either for a day or even for multiple days, and then you get some learning curve like that; and this could be a plot of the cost function J or cost of your training error or cost of your dataset error, but some metric in your tracking. And then <strong>at the same time you might start up a different model with a different setting of the hyperparameters</strong>. And so, your second model might generate a different learning curve, maybe one that looks like that. I will say that one looks better. And at the same time, you might train a third model, which might generate a learning curve that looks like that, and another one that, maybe this one diverges so it looks like that, and so on. Or you might train many different models in parallel, where these orange lines are different models, right, and so this way you can try a lot of different hyperparameter settings and then just maybe quickly at the end pick the one that works best. Looks like in this example it was, maybe this curve that look best. </p>
<p>So to make an analogy, I’m going to call the approach on the left the panda approach. <strong>When pandas have children, they have very few children, usually one child at a time, and then they really put a lot of effort into making sure that the baby panda survives. So that’s really babysitting</strong>. One model or one baby panda. <strong>Whereas the approach on the right is more like what fish do. I’m going to call this the caviar strategy</strong>. There’s some fish that lay over 100 million eggs in one mating season. But the way fish reproduce is they lay a lot of eggs and don’t pay too much attention to any one of them but just see that hopefully one of them, or maybe a bunch of them, will do well. So I guess, this is really the difference between how mammals reproduce versus how fish and a lot of reptiles reproduce. But I’m going to call it the panda approach versus the caviar approach, since that’s more fun and memorable. So the way to choose between these two approaches is really a function of how much computational resources you have. If you have enough computers to train a lot of models in parallel, then by all means take the caviar approach and try a lot of different hyperparameters and see what works. </p>
<p>But in some application domains, I see this <strong>in some online advertising settings as well as in some computer vision applications, where there’s just so much data and the models you want to train are so big that it’s difficult to train a lot of models at the same time. It’s really application dependent of course, but I’ve seen those communities use the panda approach a little bit more, where you are kind of babying a single model along and nudging the parameters up and down and trying to make this one model work</strong>. Although, of course, even the panda approach, having trained one model and then seen it work or not work, maybe in the second week or the third week, maybe I should initialize a different model and then baby that one along just like even pandas, I guess, can have multiple children in their lifetime, even if they have only one, or a very small number of children, at any one time. </p>
<p>So hopefully this gives you a good sense of how to go about the hyperparameter search process. Now, it turns out that there’s one other technique that can make your neural network much more robust to the choice of hyperparameters. It doesn’t work for all neural networks, but when it does, it can make the hyperparameter search much easier and also make training go much faster. Let’s talk about this technique in the next video. </p>
<h2 id="02-batch-normalization"><a href="#02-batch-normalization" class="headerlink" title="02_batch-normalization"></a>02_batch-normalization</h2><h3 id="01-normalizing-activations-in-a-network"><a href="#01-normalizing-activations-in-a-network" class="headerlink" title="01_normalizing-activations-in-a-network"></a>01_normalizing-activations-in-a-network</h3><p>In the rise of deep learning, one of the most important ideas has been an algorithm called <strong>batch normalization</strong>, created by two researchers, Sergey Ioffe and Christian Szegedy. Batch normalization makes your hyperparameter search problem much easier, makes your neural network much more robust. The choice of hyperparameters is a much bigger range of hyperparameters that work well, and will also enable you to much more easily train even very deep networks. Let’s see how batch normalization works. </p>
<p>When training a model, such as logistic regression, you might remember that normalizing the input features can speed up learnings in compute the means, subtract off the means from your training sets. Compute the variances. The sum of $x^{(i)}$ squared. This is an element-wise squaring. And then normalize your data set according to the variances. And we saw in an earlier video how this can turn the contours of your learning problem from something that might be very elongated to something that is more round, and easier for an algorithm like gradient descent to optimize. So this works, in terms of normalizing the input feature values to a neural network, alter the regression.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/10.png" alt=""><br>Now, how about a deeper model? You have not just input features x, but in this layer you have activations a1, in this layer, you have activations a2 and so on. So if you want to train the parameters, say w3, b3, then wouldn’t it be nice if you can normalize the mean and variance of a2 to make the training of w3, b3 more efficient? In the case of logistic regression, we saw how normalizing x1, x2, x3 maybe helps you train w and b more efficiently. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/12.png" alt=""><br><strong>So here, the question is, for any hidden layer, can we normalize</strong>, The values of a, let’s say a2, in this example but really any hidden layer, so as to train w3 b3 faster, right? Since a2 is the input to the next layer, that therefore affects your training of w3 and b3. So this is what batch norm does, batch normalization, or batch norm for short, does. Although technically, we’ll actually normalize the values of not a2 but z2. <strong>There are some debates in the deep learning literature about whether you should normalize the value before the activation function, so z2, or whether you should normalize the value after applying the activation function, a2. In practice, normalizing z2 is done much more often. So that’s the version I’ll present and what I would recommend you use as a default choice</strong>. </p>
<p>So here is how you will implement batch norm. Given some intermediate values, In your neural net, Let’s say that you have some hidden unit values $z^{[1]}$ up to $z^{[m]}$, and this is really from some hidden layer, so it’d be more accurate to write this as $z$ for some hidden layer i for i equals 1 through m. But to reduce writing, I’m going to omit this [l], just to simplify the notation on this line. So given these values, what you do is compute the mean as follows. Okay, and all this is specific to some layer l, but I’m omitting the [l].<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/11.png" alt=""><br>And then you compute the variance using pretty much the formula you would expect and then you would take each the zis and normalize it. So you get $z^{[i]}$ normalized by subtracting off the mean and dividing by the standard deviation. <strong>For numerical stability, we usually add epsilon to the denominator like that just in case sigma squared turns out to be zero in some estimate.</strong> And so now we’ve taken these values z and normalized them to have mean 0 and standard unit variance. So every component of z has mean 0 and variance 1. <strong>But we don’t want the hidden units to always have mean 0 and variance 1. Maybe it makes sense for hidden units to have a different distribution, so what we’ll do instead is compute, I’m going to call this z tilde = gamma zi norm + beta, $\tilde{z}=\gamma z^{[i]}_{norm}+\beta$. And here, $\gamma$ and $\beta$ are learnable parameters of your model.</strong> </p>
<p>So we’re using gradient descent, or some other algorithm, like the gradient descent of momentum, or RMSprop Adam, you would update the parameters $\gamma$ and $\beta$, just as you would update the weights of your neural network. <strong>Now, notice that the effect of gamma and beta is that it allows you to set the mean of z tilde to be whatever you want it to be. In fact, if $\gamma$ equals square root $\sigma$ squared plus $\epsilon$, $\gamma = \sqrt{\sigma^2+\epsilon}$, so if $\gamma$ were equal to this denominator term. And if $\beta$ were equal to $\mu$, so this value up here, then the effect of $\gamma z_{norm} + \beta$ is that it would exactly invert this equation. So if this is true, then actually z tilde i is equal to zi. And so by an appropriate setting of the parameters gamma and beta, this normalization step, that is, these four equations is just computing essentially the identity function</strong>. But by choosing other values of $\gamma$ and $\beta$, this allows you to make the hidden unit values have other means and variances as well. And so the way you fit this into your neural network is, whereas previously you were using these values $z^{[1]}, z^{[2]}$, and so on, you would now use $\tilde{z}^{[i]}$, Instead of $z^{[i]}$ for the later computations in your neural network. And you want to put back in this [l] to explicitly denote which layer it is in, you can put it back there. So the intuition I hope you’ll take away from this is that we saw how normalizing the input features $x$ can help learning in a neural network. And what batch norm does is it applies that normalization process not just to the input layer, but to the values even deep in some hidden layer in the neural network. So it will apply this type of normalization to normalize the mean and variance of some of your hidden units’ values, $z$. <strong>But one difference between the training input and these hidden unit values is you might not want your hidden unit values be forced to have mean 0 and variance 1.</strong> For example, if you have a sigmoid activation function, you don’t want your values to always be clustered here.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/13.png" alt=""><br>You might want them to have a larger variance or have a mean that’s different than 0, in order to better take advantage of the nonlinearity of the sigmoid function rather than have all your values be in just this linear regime. <strong>So that’s why with the parameters gamma and beta, you can now make sure that your $z^{[i]}$ values have the range of values that you want. But what it does really is it then shows that your hidden units have standardized mean and variance, where the mean and variance are controlled by two explicit parameters $\gamma$ and $\beta$ which the learning algorithm can set to whatever it wants. So what it really does is it normalizes in mean and variance of these hidden unit values, really the $z^{[i]}$s, to have some fixed mean and variance. And that mean and variance could be 0 and 1, or it could be some other value, and it’s controlled by these parameters $\gamma$ and $\beta$.</strong></p>
<p>So I hope that gives you a sense of the mechanics of how to implement batch norm, at least for a single layer in the neural network. In the next video, I’m going to show you how to fit batch norm into a neural network, even a deep neural network, and how to make it work for the many different layers of a neural network. And after that, we’ll get some more intuition about why batch norm could help you train your neural network. So in case why it works still seems a little bit mysterious, stay with me, and I think in two videos from now we’ll really make that clearer. </p>
<h3 id="02-fitting-batch-norm-into-a-neural-network"><a href="#02-fitting-batch-norm-into-a-neural-network" class="headerlink" title="02_fitting-batch-norm-into-a-neural-network"></a>02_fitting-batch-norm-into-a-neural-network</h3><p>So you have seen the equations for how to invent Batch Norm for maybe a single hidden layer. Let’s see how it fits into the training of a deep network. </p>
<p>So, let’s say you have a neural network like this, you’ve seen me say before that you can view each of the unit as computing two things.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/14.png" alt=""></p>
<p>First, it computes Z and then it applies the activation function to compute A. And so we can think of each of these circles as representing a two step computation. And similarly for the next layer, that is Z2 1, and A2 1, and so on. So, if you were not applying Batch Norm, you would have an input X fit into the first hidden layer, and then first compute Z1, and this is governed by the parameters W1 and B1. And then ordinarily, you would fit Z1 into the activation function to compute A1. But what would do in Batch Norm is take this value Z1, and apply Batch Norm, sometimes abbreviated BN to it, and that’s going to be governed by parameters, Beta 1 and Gamma 1, and this will give you this new normalize value Z1. And then you fit that to the activation function to get A1, which is G1 applied to Z tilde 1. Now, you’ve done the computation for the first layer, where this Batch Norms that really occurs in between the computation from Z and A. Next, you take this value A1 and use it to compute Z2, and so this is now governed by W2, B2. And similar to what you did for the first layer, you would take Z2 and apply it through Batch Norm, and we abbreviate it to BN now. This is governed by Batch Norm parameters specific to the next layer. So Beta 2, Gamma 2, and now this gives you Z tilde 2, and you use that to compute A2 by applying the activation function, and so on. So once again, the Batch Norms that happens between computing Z and computing A. And the intuition is that, instead of using the un-normalized value Z, you can use the normalized value Z tilde, that’s the first layer. </p>
<p>The second layer as well, instead of using the un-normalized value Z2, you can use the mean and variance normalized values Z tilde 2. So the parameters of your network are going to be W1, B1. It turns out we’ll get rid of the parameters but we’ll see why in the next slide. But for now, imagine the parameters are the usual W1. B1, WL, BL, and we have added to this new network, additional parameters Beta 1, Gamma 1, Beta 2, Gamma 2, and so on, for each layer in which you are applying Batch Norm. <strong>For clarity, note that these Betas here, these have nothing to do with the hyperparameter beta that we had for momentum over the computing the various exponentially weighted averages</strong>. The authors of the Adam paper use Beta on their paper to denote that hyperparameter, the authors of the Batch Norm paper had used Beta to denote this parameter, <strong>but these are two completely different Betas. I decided to stick with Beta in both cases, in case you read the original papers. But the Beta 1, Beta 2, and so on, that Batch Norm tries to learn is a different Beta than the hyperparameter Beta used in momentum and the Adam and RMSprop algorithms.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/15.png" alt=""><br>So now that these are the new parameters of your algorithm, you would then use whether optimization you want, such as creating descent in order to implement it. For example, you might compute $d\beta^{[L]}$ for a given layer, and then update the parameters $\beta$, gets updated as $\beta-\alpha d\beta^{[L]}$. And you can also use Adam or RMSprop or momentum in order to update the parameters $\beta$ and $\gamma$, not just creating descent. And even though in the previous video, I had explained what the Batch Norm operation does, computes mean and variances and subtracts and divides by them. If they are using a Deep Learning Programming Framework, usually you won’t have to implement the Batch Norm step on Batch Norm layer yourself. So theprogramming  frameworks, that can be something like one line of code. So for example, in terms of flow framework, you can implement Batch Normalization with this function, <code>tf.nn.batch_normlization()</code>. We’ll talk more about programming frameworks later, but in practice you might not end up needing to implement all these details yourself, <strong>knowing how it works so that you can get a better understanding of what your code is doing. But implementing Batch Norm is often one line of code in the deep learning frameworks</strong>.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/16.png" alt=""><br>Now, so far, we’ve talked about Batch Norm as if you were training on your entire training set at the time as if you are using Batch gradient descent. <strong>In practice, Batch Norm is usually applied with mini-batches of your training set</strong>. So the way you actually apply Batch Norm is you take your first mini-batch and compute Z1. Same as we did on the previous slide using the parameters W1, B1 and then you take just this mini-batch and computer mean and variance of the Z1 on just this mini batch and then Batch Norm would subtract by the mean and divide by the standard deviation and then re-scale by Beta 1, Gamma 1, to give you Z1, and all this is on the first mini-batch, then you apply the activation function to get A1, and then you compute Z2 using W2, B2, and so on. So you do all this in order to perform one step of gradient descent on the first mini-batch and then goes to the second mini-batch X2, and you do something similar where you will now compute Z1 on the second mini-batch and then use Batch Norm to compute Z1 tilde. And so here in this Batch Norm step, You would be normalizing Z tilde using just the data in your second mini-batch, so does Batch Norm step here. Let’s look at the examples in your second mini-batch, computing the mean and variances of the Z1’s on just that mini-batch and re-scaling by Beta and Gamma to get Z tilde, and so on. And you do this with a third mini-batch, and keep training. </p>
<p>Now, there’s one detail to the parameterization that I want to clean up, which is previously, I said that the parameters was WL, BL, for each layer as well as $\beta^{[L]}$, and $\gamma^{[L]}$. Now notice that the way $Z$ was computed is as follows, $Z^{[L]} = W^{[L]} A^{[L-1]} + B^{[L]}$. But what Batch Norm does, is it is going to look at the mini-batch and normalize $Z^{[L]}$ to first of mean 0 and standard variance, and then a rescale by $\beta$ and $\gamma$. <strong>But what that means is that, whatever is the value of $B^{[L]}$ is actually going to just get subtracted out, because during that Batch Normalization step, you are going to compute the means of the $Z^{[L]}$’s and subtract the mean. And so adding any constant to all of the examples in the mini-batch, it doesn’t change anything. Because any constant you add will get cancelled out by the mean subtractions step. So, if you’re using Batch Norm, you can actually eliminate that parameter, or if you want, think of it as setting it permanently to 0</strong>. So then the parameterization becomes ZL is just WL x AL - 1, And then you compute ZL normalized, and we compute Z tilde = Gamma ZL + Beta, you end up using this parameter Beta L in order to decide whats that mean of Z tilde L. Which is why guess post in this layer. So just to recap, because Batch Norm zeroes out the mean of these ZL values in the layer, there’s no point having this parameter BL, and so you must get rid of it, and instead is sort of replaced by Beta L, which is a parameter that controls that ends up affecting the shift or the biased terms. </p>
<p>Finally, remember that the dimension of ZL, because if you’re doing this on one example, it’s going to be NL by 1, and so BL, a dimension, NL by one, if NL was the number of hidden units in layer L. And so the dimension of Beta L and Gamma L is also going to be NL by 1 because that’s the number of hidden units you have. You have NL hidden units, and so Beta L and Gamma L are used to scale the mean and variance of each of the hidden units to whatever the network wants to set them to. </p>
<p><strong>So, let’s pull all together and describe how you can implement gradient descent using Batch Norm</strong>.</p>
<ul>
<li><p>for t = 1 to number of mini-batches</p>
<ul>
<li><p>Compute forward propagation on the min-batch $X^{[t]}$ </p>
<ul>
<li>In each hidden layer, use Batch Norm to replace $Z^{[l]}$ with $\tilde{Z}^{[l]}$</li>
</ul>
</li>
<li><p>Use back propagation to compute: $dw^{[l]}, dγ^{[l]}, dβ^{[l]}, dw^{[l]}, dγ^{[l]}, dβ^{[l]}$</p>
</li>
<li><p>Update: </p>
<ul>
<li>$W^{[l]}:=W^{[l]}−αdW^{[l]}$</li>
<li>$\Gamma^{[l]}:=\Gamma^{[l]}−αd\Gamma^{[l]}$</li>
<li>$\beta^{[l]}:=\beta^{[l]}−αd\beta^{[l]}$</li>
</ul>
</li>
</ul>
</li>
<li><p>As well as mini-batch gradient descent, Batch Norm is used to momentum, RMSprop, Adam gradient descent to update the parameters.</p>
</li>
</ul>
<p>Assuming you’re using mini-batch gradient descent, it rates for T = 1 to the number of many batches. You would implement forward prop on mini-batch XT and doing forward prop in each hidden layer, use Batch Norm to replace ZL with Z tilde L. And so then it shows that within that mini-batch, the value Z end up with some normalized mean and variance and the values and the version of the normalized mean that and variance is Z tilde L. And then, you use back prop to compute DW, DB, for all the values of L, D Beta, D Gamma. Although, technically, since you have got to get rid of B, this actually now goes away. And then finally, you update the parameters. So, W gets updated as W minus the learning rate times, as usual, Beta gets updated as Beta minus learning rate times DB, and similarly for Gamma. And if you have computed the gradient as follows, you could use gradient descent. That’s what I’ve written down here, but this also works with gradient descent with momentum, or RMSprop, or Adam. Where instead of taking this gradient descent update, mini-batch you could use the updates given by these other algorithms as we discussed in the previous week’s videos. Some of these other optimization algorithms as well can be used to update the parameters $\beta$ and $\gamma$ that Batch Norm added to algorithm. </p>
<p>So, I hope that gives you a sense of how you could implement Batch Norm from scratch if you wanted to. If you’re using one of the Deep Learning Programming frameworks which we will talk more about later, hopefully you can just call someone else’s implementation in the Programming framework which will make using Batch Norm much easier. Now, in case Batch Norm still seems a little bit mysterious if you’re still not quite sure why it speeds up training so dramatically, let’s go to the next video and talk more about why Batch Norm really works and what it is really doing.</p>
<h3 id="03-why-does-batch-norm-work"><a href="#03-why-does-batch-norm-work" class="headerlink" title="03_why-does-batch-norm-work"></a>03_why-does-batch-norm-work</h3><p>So, why does batch norm work? <strong>Here’s one reason, you’ve seen how normalizing the input features, the X’s, to mean zero and variance one, how that can speed up learning. So rather than having some features that range from zero to one, and some from one to a 1,000, by normalizing all the features, input features X, to take on a similar range of values that can speed up learning. So, one intuition behind why batch norm works is, this is doing a similar thing, but further values in your hidden units and not just for your input there</strong>. Now, this is just a partial picture for what batch norm is doing. There are a couple of further intuitions, that will help you gain a deeper understanding of what batch norm is doing. Let’s take a look at those in this video. </p>
<p><strong>A second reason why batch norm works, is it makes weights, later or deeper than your network, say the weight on layer 10, more robust to changes to weights in earlier layers of the neural network, say, in layer one.</strong> To explain what I mean, let’s look at this most vivid example. Let’s see a training on network, maybe a shallow network, like logistic regression or maybe a neural network, maybe a shallow network like this regression or maybe a deep network, on our famous cat detection toss.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/17.png" alt=""><br><strong>But let’s say that you’ve trained your data sets on all images of black cats. If you now try to apply this network to data with colored cats where the positive examples are not just black cats like on the left, but to color cats like on the right, then your cosfa might not do very well</strong>. So in pictures, if your training set looks like this (Tip: to see on the left of the following image), where you have positive examples here and negative examples here, but you were to try to generalize it, to a data set where maybe positive examples are here and the negative examples are here, then you might not expect a module trained on the data on the left to do very well on the data on the right. Even though there might be the same function that actually works well, but you wouldn’t expect your learning algorithm to discover that green decision boundary, just looking at the data on the left.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/18.png" alt=""><br>So, this idea of your data distribution changing goes by the somewhat fancy name, <strong>covariate shift</strong>. And the idea is that, if you’ve learned some X to Y mapping, if the distribution of X changes, then you might need to retrain your learning algorithm. And this is true even if the function, the ground true function, mapping from X to Y, remains unchanged, which it is in this example, because the ground true function is, is this picture a cat or not. And the need to retain your function becomes even more acute or it becomes even worse if the ground true function shifts as well.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/19.png" alt=""><br>So, how does this problem of covariate shift apply to a neural network? Consider a deep network like this, and let’s look at the learning process from the perspective of this certain layer, the third hidden layer. So this network has learned the parameters W3 and B3. And from the perspective of the third hidden layer, it gets some set of values from the earlier layers, and then it has to do some stuff to hopefully make the output Y-hat close to the ground true value Y.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/20.png" alt=""><br>So let me cover up the nose on the left for a second. So from the perspective of this third hidden layer, it gets some values, let’s call them A_2_1, A_2_2, A_2_3, and A_2_4. But these values might as well be features X1, X2, X3, X4, and the job of the third hidden layer is to take these values and find a way to map them to Y-hat. So you can imagine doing great intercepts, so that these parameters W_3_B_3 as well as maybe W_4_B_4, and even W_5_B_5, maybe try and learn those parameters, <strong>so the network does a good job, mapping from the values I drew in black on the left to the output values Y-hat</strong>.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/21.png" alt=""><br>But now let’s uncover the left of the network again. <strong>The network is also adapting parameters $W^{[2]}, B^{[2]}$ and $W^{[1]}, B^{[1]}$, and so as these parameters change, these values, $A^{[2]}$, will also change. So from the perspective of the third hidden layer, these hidden unit values are changing all the time, and so it’s suffering from the problem of covariate shift that we talked about on the previous slide. So what batch norm does, is it reduces the amount that the distribution of these hidden unit values shifts around.</strong><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/22.png" alt=""><br>And if it were to plot the distribution of these hidden unit values, maybe this is technically renormalizer $Z$, so this is actually $Z^{[2]}_1$ and $Z^{[2]}_2$, and I also plot two values instead of four values, so we can visualize in 2D. What batch norm is saying is that, the values for $Z^{[2]}_1$ Z and $Z^{[2]}_2$  can change, and indeed they will change when the neural network updates the parameters in the earlier layers. But what batch norm ensures is that no matter how it changes, the mean and variance of $Z^{[2]}_1$ and $Z^{[2]}_2$ will remain the same. So even if the exact values of $Z^{[2]}_1$ and $Z^{[2]}_2$ change, their mean and variance will at least stay same mean zero and variance one. Or, not necessarily mean zero and variance one, but whatever value is governed by $\beta^{[2]}$ and $\gamma^{[2]}$. Which, if the neural networks chooses, can force it to be mean zero and variance one. Or, really, any other mean and variance. But what this does is, it limits the amount to which updating the parameters in the earlier layers can affect the distribution of values that the third layer now sees and therefore has to learn on. <strong>And so, batch norm reduces the problem of the input values changing, it really causes these values to become more stable, so that the later layers of the neural network has more firm ground to stand on. And even though the input distribution changes a bit, it changes less, and what this does is, even as the earlier layers keep learning, the amounts that this forces the later layers to adapt to as early as layer changes is reduced or, if you will, it weakens the coupling between what the early layers parameters has to do and what the later layers parameters have to do. And so it allows each layer of the network to learn by itself, a little bit more independently of other layers, and this has the effect of speeding up of learning in the whole network</strong>.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/23.png" alt=""><br>So I hope this gives some better intuition, but the takeaway is that batch norm means that, especially from the perspective of one of the later layers of the neural network, <strong>the earlier layers don’t get to shift around as much, because they’re constrained to have the same mean and variance. And so this makes the job of learning on the later layers easier. It turns out batch norm has a second effect, it has a slight regularization effect</strong>. So one non-intuitive thing of a batch norm is that each mini-batch, I will say mini-batch X_t, has the values Z_t, has the values Z_l, scaled by <strong>the mean and variance computed on just that one mini-batch</strong>. Now, because the mean and variance computed on just that mini-batch as opposed to computed on the entire data set, that mean and variance has a little bit of noise in it, because it’s computed just on your mini-batch of, say, 64, or 128, or maybe 256 or larger training examples. <strong>So because the mean and variance is a little bit noisy because it’s estimated with just a relatively small sample of data</strong>, the scaling process, going from Z_l to Z_2_l, that process is a little bit noisy as well, because it’s computed, using a slightly noisy mean and variance. So similar to dropout, it adds some noise to each hidden layer’s activations. The way dropout has noises, it takes a hidden unit and it multiplies it by zero with some probability. And multiplies it by one with some probability. And so your dropout has multiple of noise because it’s multiplied by zero or one, whereas batch norm has multiples of noise because of scaling by the standard deviation, as well as additive noise because it’s subtracting the mean. <strong>Well, here the estimates of the mean and the standard deviation are noisy. And so, similar to dropout, batch norm therefore has a slight regularization effect. Because by adding noise to the hidden units, it’s forcing the downstream hidden units not to rely too much on any one hidden unit. And so similar to dropout, it adds noise to the hidden layers and therefore has a very slight regularization effect. Because the noise added is quite small, this is not a huge regularization effect, and you might choose to use batch norm together with dropout, and you might use batch norm together with dropouts if you want the more powerful regularization effect of dropout</strong>. </p>
<p>And maybe one other slightly non-intuitive effect is that, if you use a bigger mini-batch size, right, so if you use use a mini-batch size of, say, 512 instead of 64, by using a larger mini-batch size, you’re reducing this noise and therefore also reducing this regularization effect. <strong>So that’s one strange property of dropout which is that by using a bigger mini-batch size, you reduce the regularization effect. Having said this, I wouldn’t really use batch norm as a regularizer, that’s really not the intent of batch norm, but sometimes it has this extra intended or unintended effect on your learning algorithm. But, really, don’t turn to batch norm as a regularization. Use it as a way to normalize your hidden units activations and therefore speed up learning. And I think the regularization is an almost unintended side effect</strong>. </p>
<p>So I hope that gives you better intuition about what batch norm is doing. Before we wrap up the discussion on batch norm, <strong>there’s one more detail I want to make sure you know, which is that batch norm handles data one mini-batch at a time. It computes mean and variances on mini-batches. So at test time, you try and make predictors, try and evaluate the neural network, you might not have a mini-batch of examples, you might be processing one single example at the time. So, at test time you need to do something slightly differently to make sure your predictions make sense. Like in the next and final video on batch norm</strong>, let’s talk over the details of what you need to do in order to take your neural network trained using batch norm to make predictions.</p>
<h3 id="04-batch-norm-at-test-time"><a href="#04-batch-norm-at-test-time" class="headerlink" title="04_batch-norm-at-test-time"></a>04_batch-norm-at-test-time</h3><p>Batch norm processes your data one mini batch at a time, but the test time you might need to process the examples one at a time. Let’s see how you can adapt your network to do that. </p>
<p>Recall that during training, here are the equations you’d use to implement batch norm. <strong>Within a single mini batch</strong>, you’d sum over that mini batch of the ZI values to compute the mean. So here, you’re just summing over the examples in one mini batch. I’m using M to denote the number of examples in the mini batch not in the whole training set. Then, you compute the variance and then you compute Z norm by scaling by the mean and standard deviation with Epsilon added for numerical stability. And then Z total is taking Z norm and rescaling by gamma and beta. So, notice that mu and sigma squared which you need for this scaling calculation are computed on the entire mini batch. <strong>But the test time you might not have a mini batch of 6428 or 2056 examples to process at the same time. So, you need some different way of coming up with $\mu$ and $\sigma$ squared. And if you have just one example, taking the mean and variance of that one example, doesn’t make sense.</strong><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/24.png" alt=""><br>So what’s actually done? In order to apply your neural network and test time is to come up with some separate estimate of mu and sigma squared. <strong>And in typical implementations of batch norm, what you do is estimate this using a exponentially weighted average where the average is across the mini batches.</strong> So, to be very concrete here’s what I mean. Let’s pick some layer L and let’s say you’re going through mini batches X1, X2 together with the corresponding values of Y and so on. So, when training on X1 for that layer L, you get some mu L. And in fact, I’m going to write this as mu for the first mini batch and that layer. And then when you train on the second mini batch for that layer and that mini batch,you end up with some second value of mu. And then for the fourth mini batch in this hidden layer, you end up with some third value for mu. So just as we saw how to use a exponentially weighted average to compute the mean of Theta one, Theta two, Theta three when you were trying to compute a exponentially weighted average of the current temperature, you would do that to keep track of what’s the latest average value of this mean vector you’ve seen. So that exponentially weighted average becomes your estimate for what the mean of the Zs is for that hidden layer and similarly, you use an exponentially weighted average to keep track of these values of sigma squared that you see on the first mini batch in that layer, sigma square that you see on second mini batch and so on. So you keep a running average of the mu and the sigma squared that you’re seeing for each layer as you train the neural network across different mini batches. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/25.png" alt=""></p>
<p>Then finally at test time, what you do is in place of this equation(Tip: the equations in green color above the slide), you would just compute $Z$ norm using whatever value your $Z$ have, and using your exponentially weighted average of the $\mu$ and $\sigma$ square whatever was the latest value you have to do the scaling here. And then you would compute $\tilde{Z}$ on your one test example using that $Z_{norm}$ that we just computed on the left and using the $\beta$ and $\gamma$ parameters that you have learned during your neural network training process. </p>
<p><strong>So the takeaway from this is that during training time $\mu$ and $\sigma$ squared are computed on an entire mini batch of say 64, 28 or some number of examples. But that test time, you might need to process a single example at a time. So, the way to do that is to estimate $\mu$ and $\sigma^2$ from your training set and there are many ways to do that. You could in theory run your whole training set through your final network to get $\mu$ and $\sigma^2$. But in practice, what people usually do is implement and exponentially weighted average where you just keep track of the $\mu$ and $\sigma^2$ values you’re seeing during training and use and exponentially the weighted average, also sometimes called the running average, to just get a rough estimate of $\mu$ and $\sigma^2$ and then you use those values of $\mu$ and $\sigma^2$ that test time to do the scale and you need the head and unit values Z</strong>. </p>
<p>In practice, this process is <strong>pretty robust</strong> to the exact way you used to estimate mu and sigma squared. So, I wouldn’t worry too much about exactly how you do this and <strong>if you’re using a deep learning framework, they’ll usually have some default way to estimate the mu and sigma squared that should work reasonably well as well.</strong> But in practice, any reasonable way to estimate the mean and variance of your head and unit values Z should work fine at test. </p>
<p>So, that’s it for batch norm and using it. I think you’ll be able to train much deeper networks and get your learning algorithm to run much more quickly. Before we wrap up for this week, I want to share with you some thoughts on deep learning frameworks as well. Let’s start to talk about that in the next video. </p>
<h2 id="03-multi-class-classification"><a href="#03-multi-class-classification" class="headerlink" title="03_multi-class-classification"></a>03_multi-class-classification</h2><h3 id="01-softmax-regression"><a href="#01-softmax-regression" class="headerlink" title="01_softmax-regression"></a>01_softmax-regression</h3><p>So far, the classification examples we’ve talked about have used binary classification, where you had two possible labels, 0 or 1. Is it a cat, is it not a cat? What if we have multiple possible classes? There’s a generalization of logistic regression called Softmax regression. Let’s you make predictions where you’re trying to recognize one of C or one of multiple classes, rather than just recognize two classes. Let’s take a look. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/26.png" alt=""><br>Let’s say that instead of just recognizing cats you want to recognize cats, dogs, and baby chicks. So I’m going to call cats class 1, dogs class 2, baby chicks class 3. And if none of the above, then there’s an other or a none of the above class, which I’m going to call class 0. So here’s an example of the images and the classes they belong to. That’s a picture of a baby chick, so the class is 3. Cats is class 1, dog is class 2, I guess that’s a koala, so that’s none of the above, so that is class 0, class 3 and so on. So the notation we’re going to use is, I’m going to use capital C to denote the number of classes you’re trying to categorize your inputs into. And in this case, you have four possible classes, including the other or the none of the above class. So when you have four classes, the numbers indexing your classes would be 0 through capital C minus one. So in other words, that would be zero, one, two or three. In this case, we’re going to build a new XY, where the upper layer has four, or in this case the variable capital alphabet C upward units. So N, the number of units upper layer which is layer L is going to equal to 4 or in general this is going to equal to C. And what we want is for the number of units in the upper layer to tell us what is the probability of each of these four classes. So the first node here is supposed to output, or we want it to output the probability that is the other class, given the input x, this will output probability there’s a cat. Give an x, this will output probability as a dog. Give an x, that will output the probability. I’m just going to abbreviate baby chick to baby C, given the input x. <strong>So here, the output labels $y$ hat is going to be a four by one dimensional vector, because it now has to output four numbers, giving you these four probabilities. And because probabilities should sum to one, the four numbers in the output $\hat{y}$, they should sum to one.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/27.png" alt=""><br>The standard model for getting your network to do this uses what’s called <strong>a Softmax layer</strong>, and the output layer in order to generate these outputs. Then write down the map, then you can come back and get some intuition about what the Softmax there is doing. So in the final layer of the neural network, you are going to compute as usual the linear part of the layers. So z, capital L, that’s the z variable for the final layer. So remember this is layer capital L. So as usual you compute that as wL times the activation of the previous layer plus the biases for that final layer. Now having computer z, you now need to apply what’s called the Softmax activation function. So that activation function is a bit unusual for the Softmax layer, but this is what it does. First, we’re going to computes a temporary variable, which we’re going to call t, which is e to the z L. So this is a part element-wise. So zL here, in our example, zL is going to be four by one. This is a four dimensional vector. So t Itself e to the zL, that’s an element wise exponentiation. T will also be a 4 by 1 dimensional vector. Then the output aL, is going to be basically the vector t will normalized to sum to 1. So aL is going to be e to the zL divided by sum from J equal 1 through 4, because we have four classes of t substitute i. So in other words we’re saying that aL is also a four by one vector, and the i element of this four dimensional vector. Let’s write that, aL substitute i that’s going to be equal to ti over sum of ti, okay? In case this math isn’t clear, we’ll do an example in a minute that will make this clearer. So in case this math isn’t clear, let’s go through a specific example that will make this clearer. </p>
<p>Let’s say that your computer $Z^{[L]}$, and $Z^{[L]}$ is a four dimensional vector, let’s say is 5, 2, -1, 3. What we’re going to do is use this element-wise exponentiation to compute this vector t. So t is going to be e to the 5, e to the 2, e to the -1, e to the 3. And if you plug that in the calculator, these are the values you get. E to the 5 is 1484, e squared is about 7.4, e to the -1 is 0.4, and e cubed is 20.1. And so, the way we go from the vector t to the vector aL is just to normalize these entries to sum to one. So if you sum up the elements of t, if you just add up those 4 numbers you get 176.3. So finally, aL is just going to be this vector t, as a vector, divided by 176.3. So for example, this first node here, this will output e to the 5 divided by 176.3. And that turns out to be 0.842. So saying that, for this image, if this is the value of z you get, the chance of it being called zero is 84.2%. And then the next nodes outputs e squared over 176.3, that turns out to be 0.042, so this is 4.2% chance. The next one is e to -1 over that, which is 0.042. And the final one is e cubed over that, which is 0.114. So it is 11.4% chance that this is class number three, which is the baby C class, right? So there’s a chance of it being class zero, class one, class two, class three. So the output of the neural network aL, this is also y hat. This is a 4 by 1 vector where the elements of this 4 by 1 vector are going to be these four numbers. Then we just compute it. So this algorithm takes the vector zL and is four probabilities that sum to 1. And if we summarize what we just did to math from zL to aL, this whole computation confusing exponentiation to get this temporary variable t and then normalizing, we can summarize this into a Softmax activation function and say aL equals the activation function g applied to the vector zL. The unusual thing about this particular activation function is that, this activation function g, it takes a input a 4 by 1 vector and it outputs a 4 by 1 vector. <strong>So previously, our activation functions used to take in a single row value input. So for example, the sigmoid and the value activation functions input the real number and output a real number. The unusual thing about the Softmax activation function is, because it needs to normalized across the different possible outputs, and needs to take a vector and puts in outputs of vector.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/28.png" alt=""><br>So what other things that a Softmax cross layer can represent, I’m going to show you some examples where you have inputs x1, x2. And these feed directly to a Softmax layer that has three or four, or more output nodes that then output y hat. So I’m going to show you a new network with no hidden layer, and all it does is compute z1 equals w1 times the input x plus b. And then the output a1, or y hat is just the Softmax activation function applied to z1. So in this neural network with no hidden layers, it should give you a sense of the types of things a Softmax function can represent. So here’s one example with just raw inputs x1 and x2. A Softmax layer with C equals 3 upper classes can represent this type of decision boundaries. Notice this kind of several linear decision boundaries, but this allows it to separate out the data into three classes. And in this diagram, what we did was we actually took the training set that’s kind of shown in this figure and train the Softmax cross fire with the upper labels on the data. And then the color on this plot shows fresh holding the upward of the Softmax cross fire, and coloring in the input base on which one of the three outputs have the highest probability. So we can maybe we kind of see that this is like a generalization of logistic regression with sort of linear decision boundaries, but with more than two classes [INAUDIBLE] class 0, 1, the class could be 0, 1, or 2. Here’s another example of the decision boundary that a Softmax cross fire represents when three normal datasets with three classes. And here’s another one, rIght, so this is a, but one intuition is that the decision boundary between any two classes will be more linear. That’s why you see for example that decision boundary between the yellow and the various classes, that’s the linear boundary where the purple and red linear in boundary between the purple and yellow and other linear decision boundary. But able to use these different linear functions in order to separate the space into three classes. Let’s look at some examples with more classes. So it’s an example with C equals 4, so that the green class and Softmax can continue to represent these types of linear decision boundaries between multiple classes. So here’s one more example with C equals 5 classes, and here’s one last example with C equals 6. So this shows the type of things the Softmax crossfire can do when there is no hidden layer of class, even much deeper neural network with x and then some hidden units, and then more hidden units, and so on. Then you can learn even more complex non-linear decision boundaries to separate out multiple different classes. </p>
<p>So I hope this gives you a sense of what a Softmax layer or the Softmax activation function in the neural network can do. In the next video, let’s take a look at how you can train a neural network that uses a Softmax layer. </p>
<h3 id="02-training-a-softmax-classifier"><a href="#02-training-a-softmax-classifier" class="headerlink" title="02_training-a-softmax-classifier"></a>02_training-a-softmax-classifier</h3><p>In the last video, you learned about the softmax, the softmax activation function. In this video, you deepen your understanding of softmax classification, and also learn how the training model that uses a softmax layer. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/29.png" alt=""><br>Recall our earlier example where the output layer computes z[L] as follows. So we have four classes, $C = 4$ then $Z^{[L]}$ can be (4,1) dimensional vector and we said we compute t which is this temporary variable that performs element y’s exponentiation. And then finally, if the activation function for your output layer, g[L] is the softmax activation function, then your outputs will be this. It’s basically taking the temporarily variable t and normalizing it to sum to 1. So this then becomes a(L). So you notice that in the z vector, the biggest element was 5, and the biggest probability ends up being this first probability. The name softmax comes from contrasting it to what’s called a hard max which would have taken the vector Z and matched it to this vector. So hard max function will look at the elements of Z and just put a 1 in the position of the biggest element of Z and then 0s everywhere else. And so this is a very hard max where the biggest element gets a output of 1 and everything else gets an output of 0. Whereas in contrast, a softmax is a more gentle mapping from Z to these probabilities. So, I’m not sure if this is a great name but at least, that was the intuition behind why we call it a softmax, all this in contrast to the hard max. And one thing I didn’t really show but had alluded to is that softmax regression or the softmax identification function generalizes the logistic activation function to C classes rather than just two classes. And it turns out that if C = 2, then softmax with C = 2 essentially reduces to logistic regression. And I’m not going to prove this in this video but the rough outline for the proof is that if C = 2 and if you apply softmax, then the output layer, a[L], will output two numbers if C = 2, so maybe it outputs 0.842 and 0.158, right? And these two numbers always have to sum to 1. And because these two numbers always have to sum to 1, they’re actually redundant. And maybe you don’t need to bother to compute two of them, maybe you just need to compute one of them. And it turns out that the way you end up computing that number reduces to the way that logistic regression is computing its single output. So that wasn’t much of a proof but the takeaway from this is that softmax regression is a generalization of logistic regression to more than two classes. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/30.png" alt=""><br>Now let’s look at how you would actually train a neural network with a softmax output layer. So in particular, let’s define the loss functions you use to train your neural network. Let’s take an example. Let’s see of an example in your training set where the target output, the ground true label is 0 1 0 0. So the example from the previous video, this means that this is an image of a cat because it falls into Class 1. And now let’s say that your neural network is currently outputting y hat equals, so y hat would be a vector probability is equal to sum to 1. 0.1, 0.4, so you can check that sums to 1, and this is going to be a[L]. So the neural network’s not doing very well in this example because this is actually a cat and assigned only a 20% chance that this is a cat. So didn’t do very well in this example. So what’s the last function you would want to use to train this neural network? In softmax classification, they’ll ask me to produce this negative sum of j=1 through 4. And it’s really sum from 1 to C in the general case. We’re going to just use 4 here, of yj log y hat of j. So let’s look at our single example above to better understand what happens. Notice that in this example, y1 = y3 = y4 = 0 because those are 0s and only y2 = 1. So if you look at this summation, all of the terms with 0 values of yj were equal to 0. And the only term you’re left with is -y2 log y hat 2, because we use sum over the indices of j, all the terms will end up 0, except when j is equal to 2. And because y2 = 1, this is just -log y hat 2. So what this means is that, if your learning algorithm is trying to make this small because you use gradient descent to try to reduce the loss on your training set. Then the only way to make this small is to make this small. And the only way to do that is to make y hat 2 as big as possible. And these are probabilities, so they can never be bigger than 1. But this kind of makes sense because x for this example is the picture of a cat, then you want that output probability to be as big as possible. So more generally, what this loss function does is it looks at whatever is the ground true class in your training set, and it tries to make the corresponding probability of that class as high as possible. If you’re familiar with maximum likelihood estimation statistics, this turns out to be a form of maximum likelyhood estimation. But if you don’t know what that means, don’t worry about it. The intuition we just talked about will suffice. Now this is the loss on a single training example. How about the cost J on the entire training set. So, the class of setting of the parameters and so on, of all the ways and biases, you define that as pretty much what you’d guess, sum of your entire training sets are the loss, your learning algorithms predictions are summed over your training samples. And so, what you do is use gradient descent in order to try to minimize this class.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/31.png" alt=""><br>Finally, one more implementation detail. Notice that because C is equal to 4, y is a 4 by 1 vector, and y hat is also a 4 by 1 vector. So if you’re using a vectorized limitation, the matrix capital Y is going to be y(1), y(2), through y(m), stacked horizontally. And so for example, if this example up here is your first training example then the first column of this matrix Y will be 0 1 0 0 and then maybe the second example is a dog, maybe the third example is a none of the above, and so on. And then this matrix Y will end up being a 4 by m dimensional matrix. And similarly, Y hat will be y hat 1 stacked up horizontally going through y hat m, so this is actually y hat 1. All the output on the first training example then y hat will these 0.3, 0.2, 0.1, and 0.4, and so on. And y hat itself will also be 4 by m dimensional matrix. Finally, let’s take a look at how you’d implement gradient descent when you have a softmax output layer. So this output layer will compute z[L] which is C by 1 in our example, 4 by 1 and then you apply the softmax attribution function to get a[L], or y hat. And then that in turn allows you to compute the loss. So with talks about how to implement the forward propagation step of a neural network to get these outputs and to compute that loss. How about the back propagation step, or gradient descent? Turns out that the key step or the key equation you need to initialize back prop is this expression, that the derivative with respect to z at the loss layer, this turns out, you can compute this y hat, the 4 by 1 vector, minus y, the 4 by 1 vector. So you notice that all of these are going to be 4 by 1 vectors when you have 4 classes and C by 1 in the more general case. And so this going by our usual definition of what is dz, this is the partial derivative of the class function with respect to z[L]. If you are an expert in calculus, you can derive this yourself. Or if you’re an expert in calculus, you can try to derive this yourself, but using this formula will also just work fine, if you have a need to implement this from scratch. With this, you can then compute dz[L] and then sort of start off the back prop process to compute all the derivatives you need throughout your neural network. But it turns out that in this week’s primary exercise, we’ll start to use one of the deep learning program frameworks and for those primary frameworks, usually it turns out you just need to focus on getting the forward prop right. And so long as you specify it as a primary framework, the forward prop pass, the primary framework will figure out how to do back prop, how to do the backward pass for you. So this expression is worth keeping in mind for if you ever need to implement softmax regression, or softmax classification from scratch. Although you won’t actually need this in this week’s primary exercise because the primary framework you use will take care of this derivative computation for you. </p>
<p>So that’s it for softmax classification, with it you can now implement learning algorithms to characterized inputs into not just one of two classes, but one of C different classes. Next, I want to show you some of the deep learning programming frameworks which can make you much more efficient in terms of implementing deep learning algorithms. Let’s go on to the next video to discuss that. </p>
<h3 id="Personal-Tip"><a href="#Personal-Tip" class="headerlink" title="Personal Tip"></a>Personal Tip</h3><p>if you want to go over the details of Softmax regression, please refer to <a href="http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression" target="_blank" rel="noopener"> Softmax regression UFLDL Tutorial</a> </p>
<h2 id="04-introduction-to-programming-frameworks"><a href="#04-introduction-to-programming-frameworks" class="headerlink" title="04_introduction-to-programming-frameworks"></a>04_introduction-to-programming-frameworks</h2><p>You’ve learned to implement deep learning algorithms more or less from scratch using Python and NumPY. And I’m glad you did that because I wanted you to understand what these deep learning algorithms are really doing. But you find unless you implement more complex models, such as convolutional neural networks or recurring neural networks, or as you start to implement very large models that is increasingly not practical, at least for most people, is not practical to implement everything yourself from scratch. Fortunately, there are now many good deep learning software frameworks that can help you implement these models. To make an analogy, I think that hopefully you understand how to do a matrix multiplication and you should be able to implement how to code, to multiply two matrices yourself. But as you build very large applications, you’ll probably not want to implement your own matrix multiplication function but instead you want to call a numerical linear algebra library that could do it more efficiently for you. But this still helps that you understand how multiplying two matrices work. So I think deep learning has now matured to that point where it’s actually more practical you’ll be more efficient doing some things with some of the deep learning frameworks. So let’s take a look at the frameworks out there. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/03_hyperparameter-tuning-batch-normalization-and-programming-frameworks/32.png" alt=""><br>Today, there are many deep learning frameworks that makes it easy for you to implement neural networks, and here are some of the leading ones. Each of these frameworks has a dedicated user and developer community and I think each of these frameworks is a credible choice for some subset of applications. There are lot of people writing articles comparing these deep learning frameworks and how well these deep learning frameworks changes. And because these frameworks are often evolving and getting better month to month, I’ll leave you to do a few internet searches yourself, if you want to see the arguments on the pros and cons of some of these frameworks. But I think many of these frameworks are evolving and getting better very rapidly. So rather than too strongly endorsing any of these frameworks I want to share with you the criteria I would recommend you use to choose frameworks. One important criteria is the ease of programming, and that means both developing the neural network and iterating on it as well as deploying it for production, for actual use, by thousands or millions or maybe hundreds of millions of users, depending on what you’re trying to do. A second important criteria is running speeds, especially training on large data sets, some frameworks will let you run and train your neural network more efficiently than others. And then, one criteria that people don’t often talk about but I think is important is whether or not the framework is truly open. And for a framework to be truly open, it needs not only to be open source but I think it needs good governance as well. Unfortunately, in the software industry some companies have a history of open sourcing software but maintaining single corporation control of the software. And then over some number of years, as people start to use the software, some companies have a history of gradually closing off what was open source, or perhaps moving functionality into their own proprietary cloud services. So one thing I pay a bit of attention to is how much you trust that the framework will remain open source for a long time rather than just being under the control of a single company, which for whatever reason may choose to close it off in the future even if the software is currently released under open source. But at least in the short term depending on your preferences of language, whether you prefer Python or Java or C++ or something else, and depending on what application you’re working on, whether this can be division or natural language processing or online advertising or something else, I think multiple of these frameworks could be a good choice. So that said on programming frameworks by providing a higher level of abstraction than just a numerical linear algebra library, any of these program frameworks can make you more efficient as you develop machine learning applications.</p>
<h3 id="02-tensorflow"><a href="#02-tensorflow" class="headerlink" title="02_tensorflow"></a>02_tensorflow</h3><p>Welcome to the last video for this week. There are many great, deep learning programming frameworks. One of them is TensorFlow. I’m excited to help you start to learn to use TensorFlow. What I want to do in this video is show you the basic structure of a TensorFlow program, and then leave you to practice, learn more details, and practice them yourself in this week’s problem exercise. This week’s problem exercise will take some time to do so please be sure to leave some extra time to do it. </p>
<p>As a motivating problem, let’s say that you have some cost function J that you want to minimize. And for this example, I’m going to use this highly simple cost function J(w) = w squared- 10w + 25. So that’s the cost function. You might notice that this function is actually (w- 5) squared. If you expand out this quadratic, you get the expression above, and so the value of w that minimizes this is w = 5. But let’s say we didn’t know that, and you just have this function. Let’s see how you can implement something in TensorFlow to minimize this. Because a very similar structure of program can be used to train neural networks where you can have some complicated cost function J(w, b) depending on all the parameters of your neural network. And the, similarly, you’ll be able to use TensorFlow so automatically try to find values of w and b that minimize this cost function. But let’s start with the simpler example on the left. So, I’m running Python in my Jupyter notebook and to start up TensorFlow, you import numpy as np and it’s idiomatic to use import tensorflow as tf. Next, let me define the parameter w. So in TensorFlow, you’re going to use tf.Variable to define a parameter. Dtype=tf.float32. And then let’s define the cost function. So remember the cost function was w squared- 10w + 25. So let me use tf.add. So I’m going to have w squared + tf.multiply. So the second term was -10.w. And then I’m going to add that 25. So let me put another tf.add over there. So that defines the cost J that we had. And then, I’m going to write train = tf.train.GradientDescentOptimizer. Let’s use a learning rate of 0.01 and the goal is to minimize the cost. And finally, the following few lines are quite idiomatic. Init = tf.global_variables_initializer and then session = tf.Sessions. So it starts a TensorFlow session. Session.run init to initialize global variables. And then, for TensorFlow’s evaluative variable, we’re going to use sess.run w. We haven’t done anything yet. So with this line above, initialize w to zero and define a cost function. We define train to be our learning algorithm which uses a GradientDescentOptimizer to minimize the cost function. But we haven’t actually run the learning algorithm yet, so session.run, we evaluate w, and let me print(session.run(w). So if we run that, it evaluates w to be equal to 0 because we haven’t run anything yet. Now, let’s do session.run train. So what this will do is run one step of GradientDescent. And then let’s evaluate the value of w after one step of GradientDescent and print that. So we do that of the one set of GradientDescent, w is now 0.1. Let’s now run 1000 iterations of GradientDescent so .run(train). And lets then print(session.run w). So this would run a 1,000 iterations of GradientDescent, and at the end w ends up being 4.9999. Remember, we said that we’re minimizing w- 5 squared so the absolute value of w is 5 and it got very close to this. So hope this gives you a sense of the broad structure of a TensorFlow program. And as you do the following exercise and play with more TensorFlow course yourself, some of these functions that I’m using here will become more familiar. </p>
<p>Some things to notice about this, <strong>w is the parameter</strong> which I optimize so we’re going to declare that as a variable. And notice that all <strong>we had to do was define a cost function</strong> using these add and multiply and so on functions. And TensorFlow knows automatically how to take derivatives with respect to the add and multiply as was other functions. Which is why you only had to implement basically four prop and it can figure out how to do the back problem or the grading computation. Because that’s already built in to the add and multiply as well as the squaring functions. By the way, in cases notation seems really ugly, TensorFlow actually has overloaded the computation for the usual plus, minus, and so on. So you could also just write this nicer format for the cost and comment that out and rerun this and get the same result. <strong>So once w is declared to be a TensorFlow variable, the squaring, multiplication, adding, and subtraction operations are overloaded</strong>. So you don’t need to use this uglier syntax that I had above. </p>
<p>Now, there’s just one more feature of TensorFlow that I want to show you, which is this example minimize a fix function of w. One of the function you want to minimize is the function of your training set. So whether you have some training data, x and when you’re training a neural network the training data x can change. So how do you get training data into a TensorFlow program? So I’m going to define t and x which is think of this as train a relevant training data or really the training data with both x and y, but we only have x in this example. So just going to define x with placeholder and it’s going to be a type float32 and let’s make this a [3,1] array. And what I’m going to do is whereas the cost here have fixed coefficients in front of the three terms in this quadratic was 1 times w squared- 10<em>w + 25. We could turn these numbers 1- 10 and 25 into data. So what I’m going to do is replace the cost with cost = x[0][0]</em>w squared + x[1][0]*w + x[2][0]. Well, times 1. So now x becomes sort of like data that controls the coefficients of this quadratic function. And this placeholder function tells TensorFlow that x is something that you provide the values for later. So let’s define another array, coefficient = np.array, [1.], [-10.] and yes, the loss value was [25.]. So that’s going to be the data that we’re going to plug into x. So finally we need a way to get this array coefficients into the variable x and the syntax to do that is just doing the training step. That the values for will need to be provided for x, I’m going to set here, feed_dict = x:coefficients, And I’m going to change this, I’m going to copy and paste put that there as well. All right, hopefully, I didn’t have any syntax errors. Let’s try re-running this and we get the same results hopefully as before. And now, if you want to change the coefficients of this quadratic function, let’s say you take this [-10.] and change it to 20, [-20]. And let’s change this to 100. So this is now a function x- 10 squared. And if I re-run this, hopefully, I find that the value that minimizes x- 10 squared is w = 10. Let’s see, cool, great and we get w very close to 10 after running 1,000 integrations of GradientDescent. So what you see more of when you do that from exercise is that a placeholder in TensorFlow is a variable whose value you assign later. And this is a convenient way to get your training data into the cost function. And the way you get your data into the cost function is with this syntax when you’re running a training iteration to use the feed_dict to set x to be equal to the coefficients here. And if you are doing mini batch GradientDescent where on each iteration, you need to plug in a different mini batch, then on different iterations you use the feed_dict to feed in different subsets of your training sets. Different mini batches into where your cost function is expecting to see data. So hopefully this gives you a sense of what TensorFlow can do. And the thing that makes this so powerful is all you need to do is specify how to compute the cost function. And then, it takes derivatives and it can apply a gradient optimizer or an add-on optimizer or some other optimizer with just pretty much one or two lines of codes. </p>
<p>So here’s the code again. I’ve cleaned this up just a little bit. And in case some of these functions or variables seem a little bit mysterious to use, they will become more familiar after you’ve practiced with it a couple times by working through their problem exercise. Just one last thing I want to mention. These three lines of code are quite idiomatic in TensorFlow, and what some programmers will do is use this alternative format. Which basically does the same thing. Set session to tf.Session() to start the session, and then use the session to run init, and then use the session to evaluate, say, w and then print the result. But this with construction is used in a number of TensorFlow programs as well. It more or less means the same thing as the thing on the left. <strong>But the with command in Python is a little bit better at cleaning up in cases an error in exception while executing this inner loop</strong>. So you see this is the following exercise as well. So what is this code really doing? Let’s focus on this equation. <strong>The heart of a TensorFlow program is something to compute at cost, and then TensorFlow automatically figures out the derivatives in how to minimize that costs. So what this equation or what this line of code is doing is allowing TensorFlow to construct a computation draft</strong>. And a computation draft does the following, it takes x[0][0], it takes w and then it goes w gets squared. And then x[0][0] gets multiplied with w squared, so you have x[0][0]<em>w squared, and so on, right? And eventually, you know, this gets built up to compute this xw, x[0][0]</em>w squared + x[1][0]<em>w + and so on. And so eventually, you get the cost function. And so the last term to be added would be x [2][0] where it gets added to be the cost. I won’t write other format for the cost. *</em>And the nice thing about TensorFlow is that by implementing basically four prop applications through this computation draft, the computed cost, TensorFlow already has that built in. All the necessary backward functions. So remember how training a deep neural network has a set of forward functions instead of backward functions. Programming frameworks like Tensor Flow have already built-in the necessary backward functions. Which is why by using the built-in functions to compute the forward function, it can automatically do the backward functions as well to implement back propagation through even very complicated functions and compute derivatives for you. So that’s why you don’t need to explicitly implement back prop. This is one of the things that makes the programming frameworks help you become really efficient.** If you look at the TensorFlow documentation, I just want to point out that the TensorFlow documentation uses a slightly different notation than I did for drawing the computation draft. So it uses x[0][0] w. And then, rather than writing the value, like w squared, the TensorFlow documentation tends to just write the operation. So this would be a, square operation, and then these two get combined in the multiplication operation and so on. And then, a final note, I guess that would be an addition operation where you add x to 0 to find the final value. So for the purposes of this class, I thought that this notation for the computation draft would be easier for you to understand. But if you look at the TensorFlow documentation, if you look at the computation drafts in the documentation, you see this alternative convention where the notes are labeled with the operations rather than with the value. But both of these representations represent basically the same computation draft. </p>
<p>And there are a lot of things that you can with just one line of code in programming frameworks. For example, if you don’t want to use GradientDescent, but instead you want to use the add-on Optimizer by changing this line of code, you can very quickly swap it, swap in a better optimization algorithm. So all the modern deep learning programming framework support things like this and makes it really easy for you to code up even pretty complex neural networks. </p>
<p>So I hope this is helpful for giving you a sense of the typical structure of a TensorFlow program. </p>
<p>To recap the material from this week, you saw how to systematically organize the hyper parameter search process. We also talked about batch normalization and how you can use that to speed up training of your neural networks. And finally, we talked about programming frameworks of deep learning. There are many great programming frameworks. And we had this last video focusing on TensorFlow. With that, I hope you enjoyed this week’s programming exercise and that helps you gain even more familiarity with these ideas. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Karan"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Karan</p>
  <div class="site-description" itemprop="description">Refuse to Fall</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">91</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/yourname" title="YouTube → https:&#x2F;&#x2F;youtube.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/yourname" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="/www.massivefile.com" title="www.massivefile.com">DataBases</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Karan</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">2.2m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">34:01</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


    </div>
</body>
</html>
