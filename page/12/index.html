<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">

<script>
    (function(){
        if(''){
                         If (prompt('Please enter the article password') !== ''){
                                 Alert('Password error!');
                history.back();
            }
        }
    })();
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"snakecoding.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":"flat","style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":false},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Refuse to Fall">
<meta property="og:type" content="website">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://snakecoding.com/page/12/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="Refuse to Fall">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Karan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://snakecoding.com/page/12/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Machine Learning</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Machine Learning</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Data Science</p>
      <a>
        <img class="custom-logo-image" src="/images/custom-logo.jpg" alt="Machine Learning">
      </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">87</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="#" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/14/14_dimensionality-reduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/14/14_dimensionality-reduction/" class="post-title-link" itemprop="url">14_dimensionality-reduction note14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-14 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-14T00:00:00+05:30">2018-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 14:57:12" itemprop="dateModified" datetime="2020-04-09T14:57:12+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>56k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>51 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This personal note is written after studying the opening course on <a href="https://www.coursera.org" target="_blank" rel="noopener">the coursera website</a>, <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew NG</a> . And images, audios of this note all comes from the opening course. </p>
<h2 id="01-motivation"><a href="#01-motivation" class="headerlink" title="01_motivation"></a>01_motivation</h2><h3 id="01-motivation-i-data-compression"><a href="#01-motivation-i-data-compression" class="headerlink" title="01_motivation-i-data-compression"></a>01_motivation-i-data-compression</h3><p>In this video, I’d like to start talking about <em>a second type of unsupervised learning problem called</em> <strong>dimensionality reduction</strong>. </p>
<p><strong>There are a couple of different reasons why one might want to do dimensionality reduction. One is data compression, and as we’ll see later, a few videos later, data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms.</strong></p>
<p> But first, let’s start by talking about <strong>what is dimensionality reduction</strong>. As a motivating example, let’s say that we’ve collected a data set with many, many, many features, and I’ve plotted just two of them here. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/1.png" alt="2D-1D-1"></p>
<p> And let’s say that unknown to us two of the features were actually the length of something in centimeters, and a different feature, x2, is the length of the same thing in inches. So, this gives us a <strong>highly redundant representation</strong> and maybe instead of having two separate features x1 then x2, both of which basically measure the length, maybe what we want to do is reduce the data to one-dimensional and just have one number measuring this length. In case this example seems a bit contrived, this centimeter and inches example is actually not that unrealistic, and not that different from things that I see happening in industry. If you have hundreds or thousands of features, it is often this easy to lose track of exactly what features you have. And sometimes may have a few different engineering teams, maybe one engineering team gives you two hundred features, a second engineering team gives you another three hundred features, and a third engineering team gives you five hundred features so you have a thousand features all together, and it actually becomes hard to keep track of you know, exactly which features you got from which team, and it’s actually not that want to have highly redundant features like these. And so if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch. Then, that’s why these examples don’t lie perfectly on a straight line, because of, you know, round-off error to the nearest centimeter or the nearest inch. And if we can reduce the data to one dimension instead of two dimensions, that reduces the redundancy.</p>
<p> For a different example, again maybe when there seems fairly less contrives. For may years I’ve been working with autonomous helicopter pilots. Or I’ve been working with pilots that fly helicopters. And so. If you were to measure–if you were to, you know, do a survey or do a test of these different pilots–you might have one feature, x1, which is maybe the skill of these helicopter pilots, and maybe “x2” could be the pilot enjoyment. That is, you know, how much they enjoy flying, and maybe these two features will be highly correlated. And what you really care about might be this sort of this sort of, this direction, a different feature that really measures pilot aptitude. And I’m making up the name aptitude of course, but again, if you highly correlated features, maybe you really want to reduce the dimension.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/2.png" alt="autonomous_helicopter_pilots"></p>
<p> So, let me say a little bit more about what it really means to reduce the dimension of the data from 2 dimensions down from 2D to 1 dimensional or to 1D. Let me color in these examples by using different colors.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/3.png" alt="Data_compression"></p>
<p> And in this case by reducing the dimension what I mean is that I would like to find maybe this line, this, you know, direction on which most of the data seems to lie and project all the data onto that line which is true, and by doing so, what I can do is just measure the position of each of the examples on that line. And what I can do is come up with a new feature, z1, and to specify the position on the line I need only one number, so it says z1 is a new feature that specifies the location of each of those points on this green line. And what this means, is that where as previously if i had an example x1, maybe this was my first example, x1. So in order to represent x1 originally x1. I needed a two dimensional number, or a two dimensional feature vector. Instead now I can represent z1. I could use just z1 to represent my first example, and that’s going to be a real number. And similarly x2 you know, if x2 is my second example there, then previously, whereas this required two numbers to represent if I instead compute the projection of that black cross onto the line. And now I only need one real number which is z2 to represent the location of this point z2 on the line. And so on through my M examples. So, just to summarize, if we allow ourselves to approximate the original data set by projecting all of my original examples onto this green line over here, then I need only one number, I need only real number to specify the position of a point on the line, and so what I can do is therefore use just one number to represent the location of each of my training examples after they’ve been projected onto that green line. So this is an approximation to the original training self because I have projected all of my training examples onto a line. But now, I need to keep around only one number for each of my examples. And so this halves the memory requirement, or a space requirement, or what have you, for how to store my data. And perhaps more interestingly, more importantly, what we’ll see later, in the later video as well is that this will allow us to make our learning algorithms run more quickly as well. And that is actually, perhaps, even the more interesting application of this data compression rather than reducing the memory or disk space requirement for storing the data. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/4.png" alt="Data_compression_3D-2D"></p>
<p>On the previous slide we showed an example of reducing data from 2D to 1D. On this slide, I’m going to show another example of reducing data from three dimensional 3D to two dimensional 2D. By the way, in the more typical example of dimensionality reduction we might have a thousand dimensional data or 1000D data that we might want to reduce to let’s say a hundred dimensional or 100D, but because of the limitations of what I can plot on the slide. I’m going to use examples of 3D to 2D, or 2D to 1D. So, let’s have a data set like that shown here. And so, I would have a set of examples x(i) which are points in r3. So, I have three dimension examples. I know it might be a little bit hard to see this on the slide, but I’ll show a 3D point cloud in a little bit. And it might be hard to see here, but all of this data maybe lies roughly on the plane, like so. And so what we can do with dimensionality reduction, is take all of this data and project the data down onto a two dimensional plane. So, here what I’ve done is, I’ve taken all the data and I’ve projected all of the data, so that it all lies on the plane. Now, finally, in order to specify the location of a point within a plane, we need two numbers, right? We need to, maybe, specify the location of a point along this axis, and then also specify it’s location along that axis. So, we need two numbers, maybe called z1 and z2 to specify the location of a point within a plane. And so, what that means, is that we can now represent each example, each training example, using two numbers that I’ve drawn here, z1, and z2. So, our data can be represented using vector z which are in r2. And these subscript, z subscript 1, z subscript 2, what I just mean by that is that my vectors here, z, you know, are two dimensional vectors, z1, z2. And so if I have some particular examples, z(i), or that’s the two dimensional vector, z(i)1, z(i)2. And on the previous slide when I was reducing data to one dimensional data then I had only z1, right? And that is what a z1 subscript 1 on the previous slide was, but here I have two dimensional data, so I have z1 and z2 as the two components of the data. </p>
<p>Now, let me just make sure that these figures make sense. So let me just reshow these exact three figures again but with 3D plots. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/5.png" alt="3d_plots"></p>
<p>So the process we went through was that shown in the lab is the optimal data set, in the middle the data set projects on the 2D, and on the right the 2D data sets with z1 and z2 as the axis. Let’s look at them a little bit further. Here’s my original data set, shown on the left, and so I had started off with a 3D point cloud like so, where the axis are labeled x1, x2, x3, and so there’s a 3D point but most of the data, maybe roughly lies on some, you know, not too far from some 2D plain. So, what we can do is take this data and here’s my middle figure. I’m going to project it onto 2D. So, I’ve projected this data so that all of it now lies on this 2D surface. As you can see all the data lies on a plane, ‘cause we’ve projected everything onto a plane, and so what this means is that now I need only two numbers, z1 and z2, to represent the location of point on the plane. And so that’s the process that we can go through to reduce our data from three dimensional to two dimensional. So that’s dimensionality reduction and how we can use it to compress our data. And as we’ll see later this will allow us to make some of our learning algorithms run much later as well, but we’ll get to that only in a later video.</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><ul>
<li>We may want to reduce the dimension of our features if we have a lot of redundant data. </li>
<li>To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line. </li>
</ul>
<p>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.</p>
<p>Note: in dimensionality reduction, we are reducing our features rather than our number of examples. Our variable m will stay the same size; n, the number of features each example from $x^{(1)}$ to $x^{(m)}$ carries, will be reduced. </p>
<h3 id="02-motivation-ii-visualization"><a href="#02-motivation-ii-visualization" class="headerlink" title="02_motivation-ii-visualization"></a>02_motivation-ii-visualization</h3><p>In the last video, we talked about dimensionality reduction for the purpose of compressing the data. In this video, I’d like to tell you about <strong>a second application of dimensionality reduction and that is to visualize the data</strong>. For a lot of machine learning applications, it really helps us to develop effective learning algorithms, if we can understand our data better. If there is some way of visualizing the data better, and so, dimensionality reduction offers us, often, another useful tool to do so. </p>
<p>Let’s start with an example. Let’s say we’ve collected a large data set of many statistics and facts about different countries around the world. So, maybe the first feature, X1 is the country’s GDP, or the Gross Domestic Product, and X2 is a per capita, meaning the per person GDP, X3 human development index, life expectancy, X5, X6 and so on. And we may have a huge data set like this, where, you know, maybe 50 features for every country, and we have a huge set of countries. So is there something we can do to try to understand our data better? I’ve given this huge table of numbers. How do you visualize this data? If you have 50 features, it’s very difficult to plot 50-dimensional data. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/6.png" alt="50-features_of_GDP_of_various_contries"></p>
<p>What is a good way to examine this data? Using dimensionality reduction, what we can do is, instead of having each country represented by this featured vector, xi, which is 50-dimensional, so instead of, say, having a country like Canada, instead of having 50 numbers to represent the features of Canada, let’s say we can come up with a different feature representation that is these z vectors, that is in R2. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/7.png" alt="50-features_tp_2-features_of_GDP_of_various_contries"></p>
<p>If that’s the case, if we can have just a pair of numbers, z1 and z2 that somehow, summarizes my 50 numbers, maybe what we can do  [xx] is to plot these countries in R2 and use that to try to understand the space in [xx] of features of different countries [xx]  the better and so, here, what you can do is reduce the data from 50 D, from 50 dimensions to 2D, so you can plot this as a 2 dimensional plot, and, when you do that, it turns out that, if you look at the output of the Dimensionality Reduction algorithms, It usually doesn’t astride a physical meaning to these new features you want $z_1,z_2$. It’s often up to us to figure out you know, roughly what these features means. But, And if you plot those features, here is what you might find. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/8.png" alt="a_plot_of_2-features_of_GDP_of_various_contries"></p>
<p>So, here, every country is represented by a point ZI, which is an R2 and so each of those. Dots, and this figure represents a country, and so, here’s Z1 and here’s Z2, and a  couple of these. So, you might find, for example, That the horizontial axis the Z1 axis corresponds roughly to the overall country size, or the overall economic activity of a country. So the overall GDP, overall economic size of a country. Whereas the vertical axis in our data might correspond to the per person GDP. Or the per person well being, or the per person economic activity, and, you might find that, given these 50 features, you know, these are really the 2 main dimensions of the deviation, and so, out here you may have a country like the U.S.A., which is a relatively large GDP, you know, is a very large GDP and a relatively high per-person GDP as well. Whereas here you might have a country like Singapore, which actually has a very high per person GDP as well, but because Singapore is a much smaller country the overall economy size of Singapore is much smaller than the US. And, over here, you would have countries where individuals are unfortunately some are less well off, maybe shorter life expectancy, less health care, less economic maturity that’s why smaller countries, whereas a point like this will correspond to a country that has a fair, has a substantial amount of economic activity, but where individuals tend to be somewhat less well off. So you might find that the axes Z1 and Z2 can help you to most succinctly capture really what are the two main dimensions of the variations amongst different countries. Such as the overall economic activity of the country projected by the size of the country’s overall economy as well as the per-person individual well-being, measured by per-person GDP, per-person healthcare, and things like that. So that’s how you can use dimensionality reduction, in order to reduce data from 50 dimensions or whatever, down to two dimensions, or maybe down to three dimensions, so that you can plot it and understand your data better. </p>
<p>In the next video, we’ll start to develop a specific algorithm, called PCA, or Principal Component Analysis, which will allow us to do this and also do the earlier application I talked about of compressing the data.</p>
<h4 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h4><p><strong>Motivation II: Visualization</strong><br>It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.<br>We need to find new features, $z_1,z_2$ (and perhaps $z_3$ ) that can effectively <strong>summarize</strong> all the other features.<br>Example: hundreds of features related to a country’s economic system may all be combined into one feature that you call “Economic Activity.” </p>
<h2 id="02-principal-component-analysis"><a href="#02-principal-component-analysis" class="headerlink" title="02_principal-component-analysis"></a>02_principal-component-analysis</h2><h3 id="01-principal-component-analysis-problem-formulation"><a href="#01-principal-component-analysis-problem-formulation" class="headerlink" title="01_principal-component-analysis-problem-formulation"></a>01_principal-component-analysis-problem-formulation</h3><p>For the problem of dimensionality reduction, by far the most popular, by far the most commonly used algorithm is something called <strong>principle components analysis, or PCA</strong>. </p>
<p>In this video, I’d like to start talking about the problem formulation for PCA. In other words, let’s try to formulate, precisely, exactly what we would like PCA to do. Let’s say we have a data set like this. So, this is a data set of examples x and R2 and let’s say I want to reduce the dimension of the data from two-dimensional to one-dimensional. In other words, I would like to find a line onto which to project the data. So what seems like a good line onto which to project the data, it’s a line like this, might be a pretty good choice. And the reason we think this might be a good choice is that if you look at where the projected versions of the point scales, so I take this point and project it down here. Get that, this point gets projected here, to here, to here, to here. What we find is that the distance between each point and the projected version is pretty small. <strong>That is, these blue line segments are pretty short. So what PCA does formally is it tries to find a lower dimensional surface, really a line in this case, onto which to project the data so that the sum of squares of these little blue line segments is minimized. The length of those blue line segments, that’s sometimes also called the projection error. And so what PCA does is it tries to find a surface onto which to project the data so as to minimize that.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/9.png" alt=""></p>
<p><strong>As an aside, before applying PCA, it’s standard practice to first perform mean normalization at feature scaling so that the features x1 and x2 should have zero mean, and should have comparable ranges of values</strong>. I’ve already done this for this example, but I’ll come back to this later and talk more about feature scaling and the normalization in the context of PCA later. But coming back to this example, in contrast to the red line that I just drew, here’s a different line onto which I could project my data, which is this magenta line. And, as we’ll see, this magenta line is a much worse direction onto which to project my data, right? So if I were to project my data onto the magenta line, we’d get a set of points like that. And the projection errors, that is these blue line segments, will be huge. So these points have to move a huge distance in order to get projected onto the magenta line. And so that’s why PCA, principal components analysis, will choose something like the red line rather than the magenta line down here. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/10.png" alt="PCA_1-2"></p>
<p>Let’s write out the PCA problem a little more formally. The goal of PCA, if we want to reduce data from two-dimensional to one-dimensional is, we’re going to try find a vector that is a vector u1, which is going to be an Rn, so that would be an R2 in this case. I’m gonna find the direction onto which to project the data, so it’s to minimize the projection error. So, in this example I’m hoping that PCA will find this vector, which l wanna call u(1), so that when I project the data onto the line that I define by extending out this vector, I end up with pretty small reconstruction errors. And that reference of data that looks like this. And by the way, I should mention that where the PCA gives me u(1) or -u(1), doesn’t matter. So if it gives me a positive vector in this direction, that’s fine. If it gives me the opposite vector facing in the opposite direction, so that would be like minus u(1). Let’s draw that in blue instead, right? But it gives a positive u(1) or negative u(1), it doesn’t matter because each of these vectors defines the same red line onto which I’m projecting my data. So this is a case of reducing data from two-dimensional to one-dimensional. In the more general case we have n-dimensional data and we’ll want to reduce it to k-dimensions. In that case we want to find not just a single vector onto which to project the data but we want to find k-dimensions onto which to project the data. So as to minimize this projection error. So here’s the example. If I have a 3D point cloud like this, then maybe what I want to do is find vectors. So find a pair of vectors. And I’m gonna call these vectors. Let’s draw these in red. I’m going to find a pair of vectors, sustained from the origin. Here’s u(1), and plane, or they define a 2D surface, right? Like this with a 2D surface onto which I am going to project my data. For those of you that are familiar with linear algebra, for this year they’re really experts in linear algebra, the formal definition of this is that we are going to find the set of vectors u(1), u(2), maybe up to u(k). And what we’re going to do is project the data onto the linear subspace spanned by this set of k vectors. But if you’re not familiar with linear algebra, just think of it as finding k directions instead of just one direction onto which to project the data. So finding a k-dimensional surface is really finding a 2D plane in this case, shown in this figure, where we can define the position of the points in a plane using k directions. And that’s why for PCA we want to find k vectors onto which to project the data. And so more formally in PCA, what we want to do is find this way to project the data so as to minimize the sort of projection distance, which is the distance between the points and the projections. And so in this 3D example too. Given a point we would take the point and project it onto this 2D surface. We are done with that. And so the projection error would be, the distance between the point and where it gets projected down to my 2D surface. And so what PCA does is I try to find the line, or a plane, or whatever, onto which to project the data, to try to minimize that square projection, that 90 degree or that orthogonal projection error. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/11.png" alt=""></p>
<p>Finally, one question I sometimes get asked is how does PCA relate to linear regression? Because when explaining PCA, I sometimes end up drawing diagrams like these and that looks a little bit like linear regression. <strong>It turns out PCA is not linear regression and despite some cosmetic similarity, these are actually totally different algorithms.</strong> If we were doing linear regression, what we would do would be, <em>on the left we would be trying to predict the value of some variable y given some info features x</em>. <strong>And so linear regression, what we’re doing is we’re fitting a straight line so as to minimize the square error between point and this straight line.</strong> And so what we’re minimizing would be the squared magnitude of these blue lines. And notice that I’m drawing these blue lines vertically. That these blue lines are the vertical distance between the point and the value predicted by the hypothesis. <strong>Whereas in contrast, in PCA</strong>, what it does is it tries to minimize the magnitude of these blue lines, which are drawn at an angle. These are really the shortest <strong>orthogonal distances</strong>. The shortest distance between the point x and this red line. And this gives very different effects depending on the dataset. And more generally, when you’re doing linear regression, there is this distinguished variable y they we’re trying to predict. All that linear regression as well as taking all the values of x and try to use that to predict y. Whereas in PCA, there is no distinguish, or there is no special variable y that we’re trying to predict. And instead, we have a list of features, x1, x2, and so on, up to xn, and all of these features are treated equally,so no one of them is special. </p>
<p>As one last example, if I have three-dimensional data and I want to reduce data from 3D to 2D, so maybe I wanna find two directions, u(1) and u(2), onto which to project my data. Then what I have is I have three features, x1, x2, x3, and all of these are treated alike. All of these are treated symmetrically and there’s no special variable y that I’m trying to predict. And so PCA is not a linear regression, and even though at some cosmetic level they might look related, these are actually very different algorithms. So hopefully you now understand what PCA is doing. It’s trying to find a lower dimensional surface onto which to project the data, so as to minimize this squared projection error. To minimize the square distance between each point and the location of where it gets projected. In the next video, we’ll start to talk about how to actually find this lower dimensional surface onto which to project the data.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/12.png" alt="PCA_is_not_linear_regression"></p>
<h4 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h4><p><strong>Principal Component Analysis Problem Formulation</strong></p>
<p>The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)<br><strong><em>Problem formulation</em></strong><br>Given two features, $x_1$ and $x_2$ we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.<br>The same can be done with three features, where we map them to a plane.<br><strong>The goal of PCA</strong> is to reduce the average of all the distances of every feature to the projection line. This is the <strong>projection error</strong>.<br>Reduce from 2d to 1d: find a direction (a vector $u^{(1)} \in \mathbb{R}^n$) onto which to project the data so as to minimize the projection error.<br>The more general case is as follows:<br>Reduce from n-dimension to k-dimension: Find k vectors $u^{(1)}, u^{(2)}, \dots, u^{(k)}$ onto which to project the data so as to minimize the projection error.<br>If we are converting from 3d to 2d, we will project our data onto two directions (a plane), so k will be 2.<br><strong><em>PCA is not linear regression</em></strong><br>In linear regression, we are minimizing the <strong>squared error</strong> from every point to our predictor line. These are vertical distances.<br>In PCA, we are minimizing the <strong>shortest distance</strong> , or shortest orthogonal distances, to our data points.<br>More generally, in linear regression we are taking all our examples in x and applying the parameters in Θ to predict y.<br>In PCA, we are taking a number of features $x_1, x_2, \dots, x_n$, and finding a closest common dataset among them. We aren’t trying to predict any result and we aren’t applying any theta weights to the features. </p>
<h3 id="02-principal-component-analysis-algorithm"><a href="#02-principal-component-analysis-algorithm" class="headerlink" title="02_principal-component-analysis-algorithm"></a>02_principal-component-analysis-algorithm</h3><p>In this video I’d like to tell you about the <strong>principle components analysis algorithm</strong>. And by the end of this video you know to implement PCA for yourself.</p>
<p>And use it reduce the dimension of your data. Before applying PCA,there is a data pre-processing step which you should always do.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/13.png" alt="Data_preprocessing_of_PCA"></p>
<p>Given the trading sets of the examples is important to always perform mean normalization, and then depending on your data, maybe perform feature scaling as well. this is very similar to the mean normalization and feature scaling process that we have for supervised learning. In fact it’s exactly the same procedure except that we’re doing it now to our unlabeled data, X1 through Xm. So for mean normalization we first compute the mean of each feature and then we replace each feature, X, with X minus its mean, and so this makes each feature now have exactly zero mean The different features have very different scales. So for example, if x1 is the size of a house, and x2 is the number of bedrooms, to use our earlier example, we then also scale each feature to have a comparable range of values. And so, similar to what we had with supervised learning, we would take x, i substitute j, that’s the j feature and so we would subtract of the mean, now that’s what we have on top, and then divide by sj. Here, sj is some measure of the beta values of feature j.  So, it could be the max minus min value, or more commonly, it is the standard deviation of feature j. Having done this sort of data pre-processing, here’s what the PCA algorithm does.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/14.png" alt="an_example_of_PCA"></p>
<p>We saw from the previous video that what PCA does is, it tries to find a lower dimensional sub-space onto which to project the data, so as to minimize the squared projection errors, sum of the squared projection errors, as the square of the length of those blue lines that and so what we wanted to do specifically is find a vector, u1, which specifies that direction or in the 2D case we want to find two vectors, u1 and u2, to define this surface onto which to project the data. So, just as a quick reminder of what reducing the dimension of the data means, for this example on the left we were given the examples xI, which are in r2. And what we like to do is find a set of numbers zI in r push to represent our data. So that’s what from reduction from 2D to 1D means. So specifically by projecting data onto this red line there. We need only one number to specify the position of the points on the line. So i’m going to call that number z or z1. Z here  [xx] real number, so that’s like a one dimensional vector. So z1 just refers to the first component of this, you know, one by one matrix, or this one dimensional vector. And so we need only one number to specify the position of a point. So if this example here was my example X1, then maybe that gets mapped here. And if this example was X2 maybe that example gets mapped And so this point here will be Z1 and this point here will be Z2, and similarly we would have those other points for These, maybe X3, X4, X5 get mapped to Z1, Z2, Z3. So What PCA has to do is we need to come up with a way to compute two things. One is to compute these vectors, u1, and in this case u1 and u2. And the other is how do we compute these numbers, Z. So on the example on the left we’re reducing the data from 2D to 1D. In the example on the right, we would be reducing data from 3 dimensional as in r3, to zi, which is now two dimensional. So these z vectors would now be two dimensional. So it would be z1 z2 like so, and so we need to give away to compute these new representations, the z1 and z2 of the data as well. So how do you compute all of these quantities? It turns out that a mathematical derivation, also the mathematical proof, for what is the right value U1, U2, Z1, Z2, and so on. That mathematical proof is very complicated and beyond the scope of the course. But once you’ve done  [xx] it turns out that the procedure to actually find the value of u1 that you want is not that hard, even though so that the mathematical proof that this value is the correct value is someone more involved and more than i want to get into. But let me just describe the specific procedure that you have to implement in order to compute all of these things, the vectors, u1, u2, the vector z.  </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/15.png" alt="svd_of_PCA"></p>
<p>Here’s the procedure. Let’s say we want to reduce the data to n dimensions to k dimension What we’re going to do is first compute something called the covariance matrix, and the covariance matrix is commonly denoted by this Greek alphabet which is the capital Greek alphabet sigma. It’s a bit unfortunate that the Greek alphabet sigma looks exactly like the summation symbols. So this is the Greek alphabet Sigma is used to denote a matrix and this here is a summation symbol. So hopefully in these slides there won’t be ambiguity about which is Sigma Matrix, the matrix, which is a summation symbol, and hopefully it will be clear from context when I’m using each one. How do you compute this matrix let’s say we want to store it in an octave variable called sigma. What we need to do is compute something called the eigenvectors of the matrix sigma. And an octave, the way you do that is you use this command, u s v equals s v d of sigma. SVD, by the way, stands for singular value decomposition. This is a Much more advanced single value composition. It is much more advanced linear algebra than you actually need to know but now It turns out that when sigma is equal to matrix there is a few ways to compute these are high in vectors and If you are an expert in linear algebra and if you’ve heard of high in vectors before you may know that there is another octet function called I, which can also be used to compute the same thing. and It turns out that the SVD function and the I function it will give you the same vectors, although SVD is a little more numerically stable. So I tend to use SVD, although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix sigma it gives you the same thing. This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite You really don’t need to know what that means, but the SVD and I-functions are different functions but when they are applied to a covariance matrix which can be proved to always satisfy this mathematical property; they’ll always give you the same thing. Okay, that was probably much more linear algebra than you needed to know. In case none of that made sense, don’t worry about it. All you need to know is that this system command you should implement in Octave. And if you’re implementing this in a different language than Octave or MATLAB, what you should do is find the numerical linear algebra library that can compute the SVD or singular value decomposition, and there are many such libraries for probably all of the major programming languages. People can use that to compute the matrices u, s, and d of the covariance matrix sigma. So just to fill in some more details, this covariance matrix sigma will be an n by n matrix. And one way to see that is if you look at the definition this is an n by 1 vector and this here I transpose is 1 by N so the product of these two things is going to be an N by N matrix. 1xN transfers, 1xN, so there’s an NxN matrix and when we add up all of these you still have an NxN matrix. And what the SVD outputs three matrices, u, s, and v.  <strong>The thing you really need out of the SVD is the u matrix. The u matrix will also be a NxN matrix. And if we look at the columns of the U matrix it turns out that the columns of the U matrix will be exactly those vectors, u1, u2 and so on. So u, will be matrix. And if we want to reduce the data from n dimensions down to k dimensions, then what we need to do is take the first k vectors. that gives us u1 up to uK which gives us the K direction onto which we want to project the data.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/16.png" alt="Ureduce-matrix_of_PCA"></p>
<p>The rest of the procedure from this SVD numerical linear algebra routine we get this matrix u.  We’ll call these columns u1-uN. So, just to wrap up the description of the rest of the procedure, from the SVD numerical linear algebra routine we get these matrices u, s, and d.  we’re going to use the first K columns of this matrix to get u1-uK. Now the other thing we need to is take my original data set, X which is an RN And find a lower dimensional representation Z, which is a R K for this data. So the way we’re going to do that is take the first K Columns of the U matrix. Construct this matrix. Stack up U1, U2 and so on up to U K in columns. It’s really basically taking, you know, this part of the matrix, the first K columns of this matrix. And so this is going to be an N by K matrix. I’m going to give this matrix a name. I’m going to call this matrix U, subscript “reduce,” sort of a reduced version of the U matrix maybe. I’m going to use it to reduce the dimension of my data. And the way I’m going to compute Z is going to let Z be equal to this U reduce matrix transpose times X. Or alternatively, you know, to write down what this transpose means. When I take this transpose of this U matrix, what I’m going to end up with is these vectors now in rows. I have U1 transpose down to UK transpose. Then take that times X, and that’s how I get my vector Z. Just to make sure that these dimensions make sense, this matrix here is going to be k by n and x here is going to be n by 1 and so the product here will be k by 1. And so z is k dimensional, is a k dimensional vector, which is exactly what we wanted. And of course these x’s here right, can be Examples in our training set can be examples in our cross validation set, can be examples in our test set, and for example if you know, I wanted to take training example i, I can write this as xi XI and that’s what will give me ZI over there. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/19.png" alt="implementation_of_PCA_in_octave_code"></p>
<p>So, to summarize, here’s the PCA algorithm on one slide. After mean normalization, to ensure that every feature is zero mean and optional feature scaling whichYou really should do feature scaling if your features take on very different ranges of values. After this pre-processing we compute the carrier matrix Sigma like so by the way if your data is given as a matrix like hits if you have your data Given in rows like this. If you have a matrix X which is your time trading sets written in rows where x1 transpose down to x1 transpose, this covariance matrix sigma actually has a nice vectorizing implementation. You can implement in octave, you can even run sigma equals 1 over m, times x, which is this matrix up here, transpose times x and this simple expression, that’s the vectorize implementation of how to compute the matrix sigma. I’m not going to prove that today. This is the correct vectorization whether you want, you can either numerically test this on yourself by trying out an octave and making sure that both this and this implementations give the same answers or you Can try to prove it yourself mathematically. Either way but this is the correct vectorizing implementation, without compusingnext we can apply the SVD routine to get u, s, and d. And then we grab the first k columns of the u matrix you reduce and finally this defines how we go from a feature vector x to this reduce dimension representation z. And similar to k Means if you’re apply PCA, they way you’d apply this is with vectors X and RN. So, this is not done with X-0 1. So that was the PCA algorithm. One thing I didn’t do is give a mathematical proof that this There it actually give the projection of the data onto the K dimensional subspace onto the K dimensional surface that actually minimizes the square projection error Proof of that is beyond the scope of this course. Fortunately the PCA algorithm can be implemented in not too many lines of code. and if you implement this in octave or algorithm, you actually get a very effective dimensionality reduction algorithm. </p>
<p>So, that was the PCA algorithm. One thing I didn’t do was give a mathematical proof that the U1 and U2 and so on and the Z and so on you get out of this procedure is really the choices that would minimize these squared projection error. Right, remember we said What PCA tries to do is try to find a surface or line onto which to project the data so as to minimize to square projection error. So I didn’t prove that this that, and the mathematical proof of that is beyond the scope of this course. But fortunately the PCA algorithm can be implemented in not too many lines of octave code. And if you implement this, this is actually what will work, or this will work well, and if you implement this algorithm, you get a very effective dimensionality reduction algorithm. That does do the right thing of minimizing this square projection error</p>
<h4 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h4><p>Before we can apply PCA, there is a data pre-processing step we must perform:<br><strong>Data preprocessing</strong><br>Given training set: x(1),x(2),…,x(m)<br>Preprocess (feature scaling/mean normalization):<br>$\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}$<br>Replace each $x_j^{(i)}$ with $x_j^{(i)} - \mu_j$<br>If different features on different scales (e.g., $x_1$ = size of house, $x_2$ = number of bedrooms), scale features to have comparable range of values.<br>Above, we first subtract the mean of each feature from the original feature. Then we scale all the features $x_j^{(i)} = \dfrac{x_j^{(i)} - \mu_j}{s_j}$<br>We can define specifically what it means to reduce from 2d to 1d data as follows:<br>$$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$<br>The z values are all real numbers and are the projections of our features onto $u^{(1)}$.<br>So, PCA has two tasks: figure out $u^{(1)},\dots,u^{(k)}$ and also to find $z_1, z_2, \dots, z_m$.<br>The mathematical proof for the following procedure is complicated and beyond the scope of this course.<br><strong>1. Compute “covariance matrix”</strong><br>$$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$<br>This can be vectorized in Octave as: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sigma &#x3D; (1&#x2F;m) * X&#39; * X;</span><br></pre></td></tr></table></figure>
<p>We denote the covariance matrix with a capital sigma (which happens to be the same symbol for summation, confusingly—they represent entirely different things).<br>Note that $x^{(i)}$ is an n×1 vector, $(x^{(i)})^T$ is an 1×n vector and X is a m×n matrix (row-wise stored examples). The product of those will be an n×n matrix, which are the dimensions of Σ.<br><strong>2. Compute “eigenvectors” of covariance matrix Σ</strong><br>[U,S,V] = svd(Sigma);<br>svd() is the ‘singular value decomposition’, a built-in Octave function.<br>What we actually want out of svd() is the ‘U’ matrix of the Sigma covariance matrix: $U \in \mathbb{R}^{n \times n}$. U contains $u^{(1)},\dots,u^{(n)}$, which is exactly what we want. </p>
<p><strong>3. Take the first k columns of the U matrix and compute z</strong><br>We’ll assign the first k columns of U to a variable called ‘Ureduce’. This will be an n×k matrix. We compute z with:<br>$$z^{(i)} = Ureduce^T \cdot x^{(i)}$$<br>$UreduceZ^T$ will have dimensions k×n while x(i) will have dimensions n×1. The product $Ureduce^T \cdot x^{(i)}$ will have dimensions k×1.<br>To summarize, the whole algorithm in octave is roughly: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sigma &#x3D; (1&#x2F;m) * X&#39; * X; % compute the covariance matrix</span><br><span class="line">[U,S,V] &#x3D; svd(Sigma);   % compute our projected directions</span><br><span class="line">Ureduce &#x3D; U(:,1:k);     % take the first k directions</span><br><span class="line">Z &#x3D; X * Ureduce;        % compute the projected data</span><br></pre></td></tr></table></figure>

<h2 id="03-applying-pca"><a href="#03-applying-pca" class="headerlink" title="03_applying-pca"></a>03_applying-pca</h2><h3 id="01-reconstruction-from-compressed-representation"><a href="#01-reconstruction-from-compressed-representation" class="headerlink" title="01_reconstruction-from-compressed-representation"></a>01_reconstruction-from-compressed-representation</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/20.png" alt="reconstruction_from_compressed_representation"></p>
<p>In some of the earlier videos, I was talking about PCA as a compression  algorithm where you may have say, 1,000-dimensional data and compres  it to 100-dimensional feature vector. Or have three-dimensional data and compress it to a two-dimensiona  representation. So, if this is a compression algorithm, there should be a way to go bac  from this compressed representation back to an approximation of you  original high-dimensional data. So given zi, which may  100-dimensional, how do you go back to your original representation, xi which was maybe a 1000-dimensional. In this video, I’d like to describe how to do that. In the PCA algorithm, we may have an example like this, so maybe that’s my example x1, and maybe that’s my example x2. And what we do is we take these examples, and we project them onto this one dimensional surface. And then now we need to use a real number, say z1, to specify the location of these points after they’ve been projected onto this one dimensional surface. So, given the point z1, how can we go back to this original two dimensional space? In particular, given the point z, which is R, can we map this back to some approximate representation x and R2 of whatever the original value of the data was? So whereas z equals U reduce transpose x, if you want to go in the opposite direction, the equation for that is, we’re going to write x approx equals U reduce, times z. And again, just to check the dimensions, here U reduce is going to be an n by k dimensional vector, z is going to be k by one dimensional vector. So you multiply these out that’s going to be n by one, so x approx is going to be an n dimensional vector. And so the intent of PCA, that is if the square projection error is not too big, is that this x approx will be close to whatever was the original value of x that you have used to derive z in the first place. To show a picture of what this looks like, this is what it looks like. What you get back of this procedure are points that lie on the projection of that, onto the green line. So to take our early example, if we started off with this value of x1, and we got this value of z1, if you plug z1 through this formula to get x1 approx, then this point here, that would be x1 approx, which is going to be in R2. And similarly, if you do the same procedure, this would be x2 approx. And that’s a pretty decent approximation to the original data. So that’s how you go back from your low dimensional representation z, back to an uncompressed representation of the data. We get back an approximation to your original data x. And we also call this process reconstruction of the original data where we think of trying to reconstruct the original value of x from the compressed representation. So, given an unlabeled data set, you now know how to apply PCA and take your high dimensional features x and map that to this lower-dimensional representation z. And from this video hopefully you now also know how to take these low-representation z and map it back up to an approximation of your original high-dimensional data</p>
<p>Now that you know how to implement and apply PCA, what I’d like to do next is talk about some of the mechanics of how to actually use PCA well. And in particular in the next video, I’d like to talk about how to choose k, which is how to choose the dimension of the reduced representation vector z.</p>
<h4 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h4><p>If we use PCA to compress our data, how can we uncompress our data, or go back to our original number of features?<br>To go from 1-dimension back to 2d we do: $z \in \mathbb{R} \rightarrow x \in \mathbb{R}^2$.<br>We can do this with the equation:<br>$$x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}$$.<br>Note that we can only get approximations of our original data.<br>Note: It turns out that the U matrix has the special property that it is a Unitary Matrix. One of the special properties of a Unitary Matrix is:<br>$U^{-1} = U^<em>$ where the “</em>“ means “conjugate transpose”.<br>Since we are dealing with real numbers here, this is equivalent to:<br>$U^{-1} = U^T$ So we could compute the inverse and use that, but it would be a waste of energy and compute cycles. </p>
<h3 id="02-choosing-the-number-of-principal-components"><a href="#02-choosing-the-number-of-principal-components" class="headerlink" title="02_choosing-the-number-of-principal-components"></a>02_choosing-the-number-of-principal-components</h3><p>In the PCA algorithm we take N dimensional features and reduce them to some K dimensional feature representation. <strong>This number K is a parameter of the PCA algorithm. This number K is also called the number of principle components or the number of principle components that we’ve retained.</strong> And in this video <strong>I’d like to give you some guidelines, tell you about how people tend to think about how to choose this parameter K for PCA.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/21.png" alt="choosing_k_of_PCA"></p>
<p>In order to choose k, that is to choose the number of principal components, here are a couple of useful concepts. What PCA tries to do is it tries to minimize the average squared projection error. So it tries to minimize this quantity, which I’m writing down, which is the difference between the original data X and the projected version, X-approx-i, which was defined last video, so it tries to minimize the squared distance between x and it’s projection onto that lower dimensional surface. So that’s the average square projection error. Also let me define the total variation in the data to be the average length squared of these examples Xi so the total variation in the data is the average of my training sets of the length of each of my training examples. And this one says, “On average, how far are my training examples from the vector, from just being all zeros?” How far is, how far on average are my training examples from the origin? When we’re trying to choose k, a pretty common rule of thumb for choosing k is to choose the smaller values so that the ratio between these is less than 0.01. So in other words, a pretty common way to think about how we choose k is we want the average squared projection error. That is the average distance between x and it’s projections divided by the total variation of the data. That is how much the data varies. We want this ratio to be less than, let’s say, 0.01. Or to be less than 1%, which is another way of thinking about it. And the way most people think about choosing K is rather than choosing K directly the way most people talk about it is as what this number is, whether it is 0.01 or some other number. And if it is 0.01, another way to say this to use the language of PCA is that 99% of the variance is retained. I don’t really want to, don’t worry about what this phrase really means technically but this phrase “99% of variance is retained” just means that this quantity on the left is less than 0.01. And so, if you are using PCA and if you want to tell someone, you know, how many principle components you’ve retained it would be more common to say well, I chose k so that 99% of the variance was retained. And that’s kind of a useful thing to know, it means that you know, the average squared projection error divided by the total variation that was at most 1%. That’s kind of an insightful thing to think about, whereas if you tell someone that, “Well I had to 100 principle components” or “k was equal to 100 in a thousand dimensional data” it’s a little hard for people to interpret that. So this number 0.01 is what people often use. Other common values is 0.05, and so this would be 5%, and if you do that then you go and say well 95% of the variance is retained and, you know other numbers maybe 90% of the variance is retained, maybe as low as 85%. So 90% would correspond to say 0.10, kinda 10%. And so range of values from, you know, 90, 95, 99, maybe as low as 85% of the variables contained would be a fairly typical range in values. Maybe 95 to 99 is really the most common range of values that people use. For many data sets you’d be surprised, in order to retain 99% of the variance, you can often reduce the dimension of the data significantly and still retain most of the variance. Because for most real life data says many features are just highly correlated, and so it turns out to be possible to compress the data a lot and still retain you know 99% of the variance or 95% of the variance. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/22.png" alt="choosing_k_of_PCA_by_counting_singular_values"></p>
<p>So how do you implement this? Well, here’s one algorithm that you might use. You may start off, if you want to choose the value of k, we might start off with k equals 1. And then we run through PCA. You know, so we compute, you reduce, compute z1, z2, up to zm. Compute all of those x1 approx and so on up to xm approx and then we check if 99% of the variance is retained. Then we’re good and we use k equals 1. But if it isn’t then what we’ll do we’ll next try K equals 2. And then we’ll again run through this entire procedure and check, you know is this expression satisfied. Is this less than 0.01. And if not then we do this again. Let’s try k equals 3, then try k equals 4, and so on until maybe we get up to k equals 17 and we find 99% of the data have is retained and then we use k equals 17, right? That is one way to choose the smallest value of k, so that and 99% of the variance is retained. But as you can imagine, this procedure seems horribly inefficient we’re trying k equals one, k equals two, we’re doing all these calculations. Fortunately when you implement PCA it actually, in this step, it actually gives us a quantity that makes it much easier to compute these things as well. Specifically when you’re calling SVD to get these matrices u, s, and d, when you’re calling usvd on the covariance matrix sigma, it also gives us back this matrix S and what S is, is going to be a square matrix an N by N matrix in fact, that is diagonal. So is diagonal entries s one one, s two two, s three three down to s n n are going to be the only non-zero elements of this matrix, and everything off the diagonals is going to be zero. Okay? So those big O’s that I’m drawing, by that what I mean is that everything off the diagonal of this matrix all of those entries there are going to be zeros. And so, what is possible to show, and I won’t prove this here, and it turns out that for a given value of k, this quantity over here can be computed much more simply. And that quantity can be computed as one minus sum from i equals 1 through K of Sii divided by sum from I equals 1 through N of Sii. So just to say that it words, or just to take another view of how to explain that, if K equals 3 let’s say. What we’re going to do to compute the numerator is sum from one–  I equals 1 through 3 of of Sii, so just compute the sum of these first three elements. So that’s the numerator. And then for the denominator, well that’s the sum of all of these diagonal entries. And one minus the ratio of that, that gives me this quantity over here, that I’ve circled in blue. And so, what we can do is just test if this is less than or equal to 0.01. Or equivalently, we can test if the sum from i equals 1 through k, s-i-i divided by sum from i equals 1 through n, s-i-i if this is greater than or equal to 4.99, if you want to be sure that 99% of the variance is retained. And so what you can do is just slowly increase k, set k equals one, set k equals two, set k equals three and so on, and just test this quantity to see what is the smallest value of k that ensures that 99% of the variance is retained. And if you do this, then you need to call the SVD function only once. Because that gives you the S matrix and once you have the S matrix, you can then just keep on doing this calculation by increasing the value of K in the numerator and so you don’t need keep to calling SVD over and over again to test out the different values of K. So this procedure is much more efficient, and this can allow you to select the value of K without needing to run PCA from scratch over and over. You just run SVD once, this gives you all of these diagonal numbers, all of these numbers S11, S22 down to SNN, and then you can just you know, vary K in this expression to find the smallest value of K, so that 99% of the variance is retained. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/23.png" alt="choosing_k_of_PCA_by_counting_singular_values2"></p>
<p>So to summarize, the way that I often use, the way that I often choose K when I am using PCA for compression is I would call SVD once in the covariance matrix, and then I would use this formula and pick the smallest value of K for which this expression is satisfied. And by the way, even if you were to pick some different value of K, even if you were to pick the value of K manually, you know maybe you have a thousand dimensional data and I just want to choose K equals one hundred. Then, if you want to explain to others what you just did, a good way to explain the performance of your implementation of PCA to them, is actually to take this quantity and compute what this is, and that will tell you what was the percentage of variance retained. And if you report that number, then, you know, people that are familiar with PCA, and people can use this to get a good understanding of how well your hundred dimensional representation is approximating your original data set, because there’s 99% of variance retained. That’s really a measure of your square of construction error, that ratio being 0.01, just gives people a good intuitive sense of whether your implementation of PCA is finding a good approximation of your original data set. </p>
<p>So hopefully, that gives you an efficient procedure for choosing the number K. For choosing what dimension to reduce your data to, and if you apply PCA to very high dimensional data sets, you know, to like a thousand dimensional data, very often, just because data sets tend to have highly correlated features, this is just a property of most of the data sets you see, you often find that PCA will be able to retain ninety nine percent of the variance or say, ninety five ninety nine, some high fraction of the variance, even while compressing the data by a very large factor.</p>
<h4 id="summary-5"><a href="#summary-5" class="headerlink" title="summary"></a>summary</h4><p>How do we choose k, also called <em>the number of principal components</em> ? Recall that k is the dimension we are reducing to.<br>One way to choose k is by using the following formula: </p>
<ul>
<li>Given the average squared projection error: $\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$ </li>
<li>Also given the total variation in the data: $\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2$ </li>
<li>Choose k to be the smallest value such that: $\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01$ </li>
</ul>
<p>In other words, the squared projection error divided by the total variation should be less than one percent, so that 99% of the variance is retained .<br><strong>Algorithm for choosing k</strong></p>
<ol>
<li>Try PCA with k=1,2,… </li>
<li>Compute $U_{reduce}, z, x$ </li>
<li>Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k. </li>
</ol>
<p>This procedure would actually be horribly inefficient. In Octave, we will call svd: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] &#x3D; svd(Sigma)</span><br></pre></td></tr></table></figure>
<p>Which gives us a matrix S. We can actually check for 99% of retained variance using the S matrix as follows: </p>
<p>$$\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99$$ </p>
<h3 id="03-advice-for-applying-pca"><a href="#03-advice-for-applying-pca" class="headerlink" title="03_advice-for-applying-pca"></a>03_advice-for-applying-pca</h3><p>In an earlier video, I had said that <strong>PCA can be sometimes used to speed up the running time of a learning algorithm</strong>. In this video, I’d like to explain <strong>how to actually do that</strong>, and also say some, just try to give <strong>some advice about how to apply PCA.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/24.png" alt="speeding_up_supervised_learning_by_PCA"></p>
<p>Here’s how you can use PCA to speed up a learning algorithm, and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA. Let’s say you have a supervised learning problem, note this is a supervised learning problem with inputs X and labels Y, and let’s say that your examples xi are very high dimensional. So, lets say that your examples, xi are 10,000 dimensional feature vectors. One example of that, would be, if you were doing some computer vision problem, where you have a 100x100 images, and so if you have 100x100, that’s 10000 pixels, and so if xi are, you know, feature vectors that contain your 10000 pixel intensity values, then you have 10000 dimensional feature vectors. So with very high-dimensional feature vectors like this, running a learning algorithm can be slow, right? Just, if you feed 10,000 dimensional feature vectors into logistic regression, or a new network, or support vector machine or what have you, just because that’s a lot of data, that’s 10,000 numbers, it can make your learning algorithm run more slowly. Fortunately with PCA we’ll be able to reduce the dimension of this data and so make our algorithms run more efficiently. Here’s how you do that. We are going first check our labeled training set and extract just the inputs, we’re just going to extract the X’s and temporarily put aside the Y’s. So this will now give us an unlabelled training set x1 through xm which are maybe there’s a ten thousand dimensional data, ten thousand dimensional examples we have. So just extract the input vectors x1 through xm. Then we’re going to apply PCA and this will give me a reduced dimension representation of the data, so instead of 10,000 dimensional feature vectors I now have maybe one thousand dimensional feature vectors. So that’s like a 10x savings. So this gives me, if you will, a new training set. So whereas previously I might have had an example x1, y1, my first training input, is now represented by z1. And so we’ll have a new sort of training example, which is Z1 paired with y1. And similarly Z2, Y2, and so on, up to ZM, YM. Because my training examples are now represented with this much lower dimensional representation Z1, Z2, up to ZM. Finally, I can take this reduced dimension training set and feed it to a learning algorithm maybe a neural network, maybe logistic regression, and I can learn the hypothesis H, that takes this input, these low-dimensional representations Z and tries to make predictions. So if I were using logistic regression for example, I would train a hypothesis that outputs, you know, one over one plus E to the negative-theta transpose Z, that takes this input to one these z vectors, and tries to make a prediction. And finally, if you have a new example, maybe a new test example X. What you do is you would take your test example x, map it through the same mapping that was found by PCA to get you your corresponding z. And that z then gets fed to this hypothesis, and this hypothesis then makes a prediction on your input x. One final note, what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running PCA only on the training sets. And in particular, this mapping that PCA is learning, right, this mapping, what that does is it computes the set of parameters. That’s the feature scaling and mean normalization. And there’s also computing this matrix U reduced. But all of these things that U reduce, that’s like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced so on, that should be obtained by running PCA only on your training set. And then having found U reduced, or having found the parameters for feature scaling where the mean normalization and scaling the scale that you divide the features by to get them on to comparable scales. Having found all those parameters on the training set, you can then apply the same mapping to other examples that may be In your cross-validation sets or in your test sets, OK? Just to summarize, when you’re running PCA, run your PCA only on the training set portion of the data not the cross-validation set or the test set portion of your data. And that defines the mapping from x to z and you can then apply that mapping to your cross-validation set and your test set and by the way in this example I talked about reducing the data from ten thousand dimensional to one thousand dimensional, this is actually not that unrealistic.<strong>For many problems we actually reduce the dimensional data. You know by 5x maybe by 10x and still retain most of the variance and we can do this barely effecting the performance, in terms of classification accuracy, let’s say, barely affecting the classification accuracy of the learning algorithm. And by working with lower dimensional data our learning algorithm can often run much much faster</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/25.png" alt="application_of_PCA"></p>
<p>To summarize, we’ve so far talked about the following applications of PCA. First is the compression application where we might do so to reduce the memory or the disk space needed to store data and we just talked about how to use this to speed up a learning algorithm. In these applications, in order to choose K, often we’ll do so according to, figuring out what is the percentage of variance retained, and so for this learning algorithm, speed up application often will retain 99%  of the variance. That would be a very typical choice for how to choose k. So that’s how you choose k for these compression applications. Whereas for visualization applications while usually we know how to plot only two dimensional data or three dimensional data, and so for visualization applications, we’ll usually choose k equals 2 or k equals 3, because we can plot only 2D and 3D data sets. So that summarizes the main applications of PCA, as well as how to choose the value of k for these different applications. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/26.png" alt="application_of_PCA"></p>
<p>I should mention that there is often <strong>one frequent misuse of PCA</strong> and you sometimes hear about others doing this hopefully not too often. I just want to mention this so that you know not to do it. And there is one bad use of PCA, which iss to try to use it to prevent over-fitting. Here’s the reasoning. This is not a great way to use PCA, but here’s the reasoning behind this method, which is,you know if we have Xi, then maybe we’ll have n features, but if we compress the data, and use Zi instead and that reduces the number of features to k, which could be much lower dimensional. And so if we have a much smaller number of features, if k is 1,000 and n is 10,000, then if we have only 1,000 dimensional data, maybe we’re less likely to over-fit than if we were using 10,000-dimensional data with like a thousand features. So some people think of PCA as a way to prevent over-fitting. But just to emphasize this is a bad application of PCA and I do not recommend doing this. And it’s not that this method works badly. If you want to use this method to reduce the dimensional data, to try to prevent over-fitting, it might actually work OK. But this just is not a good way to address over-fitting and instead, if you’re worried about over-fitting, there is a much better way to address it, to use regularization instead of using PCA to reduce the dimension of the data. <strong>And the reason is, if you think about how PCA works, it does not use the labels y. You are just looking at your inputs xi, and you’re using that to find a lower-dimensional approximation to your data. So what PCA does, is it throws away some information. It throws away or reduces the dimension of your data without knowing what the values of y is, so this is probably okay using PCA this way is probably okay if, say 99 percent of the variance is retained, if you’re keeping most of the variance, but it might also throw away some valuable information. And it turns out that if you’re retaining 99% of the variance or 95% of the variance or whatever, it turns out that just using regularization will often give you at least as good a method for preventing over-fitting and regularization will often just work better, because when you are applying linear regression or logistic regression or some other method with regularization, well, this minimization problem actually knows what the values of y are, and so is less likely to throw away some valuable information, whereas PCA doesn’t make use of the labels and is more likely to throw away valuable information</strong>.</p>
<p>So, to summarize, it is a good use of PCA, if your main motivation to speed up your learning algorithm, but using PCA to prevent over-fitting, that is not a good use of PCA, and using regularization instead is really what many people would recommend doing instead. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/27.png" alt="last_bad_use_of_PCA"></p>
<p>Finally, <strong>one last misuse of PCA.</strong> And so I should say PCA is a very useful algorithm, I often use it for the compression on the visualization purposes. But, what I sometimes see, is also people sometimes use PCA where it shouldn’t be. So, here’s a pretty common thing that I see, which is if someone is designing a machine-learning system, they may write down the plan like this: let’s design a learning system. Get a training set and then, you know, what I’m going to do is run PCA, then train logistic regression and then test on my test data. So often at the very start of a project, someone will just write out a project plan than says lets do these four steps with PCA inside. Before writing down a project plan the incorporates PCA like this, one very good question to ask is, well, what if we were to just do the whole without using PCA. And often people do not consider this step before coming up with a complicated project plan and implementing PCA and so on. And sometime, and so specifically, what I often advise people is, before you implement PCA, I would first suggest that, you know, do whatever it is, take whatever it is you want to do and first consider doing it with your original raw data xi, and only if that doesn’t do what you want, then implement PCA before using Zi. So, before using PCA you know, instead of reducing the dimension of the data, I would consider well, let’s ditch this PCA step, and I would consider, let’s just train my learning algorithm on my original data. <strong>Let’s just use my original raw inputs xi, and I would recommend, instead of putting PCA into the algorithm, just try doing whatever it is you’re doing with the xi first. And only if you have a reason to believe that doesn’t work, so that only if your learning algorithm ends up running too slowly, or only if the memory requirement or the disk space requirement is too large, so you want to compress your representation, but if only using the xi doesn’t work, only if you have evidence or strong reason to believe that using the xi won’t work, then implement PCA and consider using the compressed representation</strong>. Because what I do see, is sometimes people start off with a project plan that incorporates PCA inside, and sometimes they, whatever they’re doing will work just fine, even without using PCA instead. So, just consider that as an alternative as well, before you go to spend a lot of time to get PCA in, figure out what k is and so on. </p>
<p><strong>So, that’s it for PCA. Despite these last sets of comments, PCA is an incredibly useful algorithm, when you use it for the appropriate applications and I’ve actually used PCA pretty often and for me, I use it mostly to speed up the running time of my learning algorithms. But I think, just as common an application of PCA, is to use it to compress data, to reduce the memory or disk space requirements, or to use it to visualize data. And PCA is one of the most commonly used and one of the most powerful unsupervised learning algorithms.</strong> And with what you’ve learned in these videos, I think hopefully you’ll be able to implement PCA and use them through all of these purposes as well.</p>
<h4 id="summary-6"><a href="#summary-6" class="headerlink" title="summary"></a>summary</h4><p>The most common use of PCA is to speed up supervised learning.<br>Given a training set with a large number of features (e.g. $x^{(1)},\dots,x^{(m)} \in \mathbb{R}^{10000}$ ) we can use PCA to reduce the number of features in each example of the training set (e.g. $z^{(1)},\dots,z^{(m)} \in \mathbb{R}^{1000}$).<br>Note that we should define the PCA reduction from $x^{(i)}$ to $z^{(i)}$ only on the training set and not on the cross-validation or test sets. You can apply the mapping z(i) to your cross-validation and test sets after it is defined on the training set.<br>Applications </p>
<ul>
<li>Compressions </li>
</ul>
<p>Reduce space of data<br>Speed up algorithm </p>
<ul>
<li>Visualization of data </li>
</ul>
<p>Choose k = 2 or k = 3<br><strong>Bad use of PC A:</strong> trying to prevent overfitting. We might think that reducing the features with PCA would be an effective way to address overfitting. It might work, but is not recommended because it does not consider the values of our results y. Using just regularization will be at least as effective.<br>Don’t assume you need to do PCA. <strong>Try your full machine learning algorithm without PCA first.</strong> Then use PCA if you find that you need it.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/13/13_unsupervised-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/13/13_unsupervised-learning/" class="post-title-link" itemprop="url">13_unsupervised-learning note13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-13 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-13T00:00:00+05:30">2018-01-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 14:57:11" itemprop="dateModified" datetime="2020-04-09T14:57:11+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>33 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This personal note is written after studying the opening course on <a href="https://www.coursera.org" target="_blank" rel="noopener">the coursera website</a>, <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew NG</a> . And images, audios of this note all comes from the opening course. </p>
<h2 id="01-clustering"><a href="#01-clustering" class="headerlink" title="01_clustering"></a>01_clustering</h2><h3 id="01-unsupervised-learning-introduction"><a href="#01-unsupervised-learning-introduction" class="headerlink" title="01_unsupervised-learning-introduction"></a>01_unsupervised-learning-introduction</h3><p>In this video, I’d like to start to talk about <strong>clustering</strong>. This will be exciting, because this is our first unsupervised learning algorithm, where we learn from unlabeled data instead from labelled data. </p>
<p>So, what is unsupervised learning? I briefly talked about unsupervised learning at the beginning of the class but it’s useful to contrast it with supervised learning. So, here’s a typical supervised learning problem where we’re given a labeled training set and the goal is to find the decision boundary that separates the positive label examples and the negative label examples. So, the supervised learning problem in this case is given a set of labels to fit a hypothesis to it.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/1.png" alt="supervised_learning_vs_unsupervised_learning1"></p>
<p>In contrast, in the unsupervised learning problem we’re given data that does not have any labels associated with it. So, we’re given data that looks like this.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/2.png" alt="supervised_learning_vs_unsupervised_learning2"></p>
<p>Here’s a set of points add in no labels, and so, our training set is written just x1, x2, and so on up to xm and we don’t get any labels y. And that’s why the points plotted up on the figure don’t have any labels with them. <strong>So, in unsupervised learning what we do is we give this sort of unlabeled training set to an algorithm and we just ask the algorithm find some structure in the data for us.</strong> </p>
<p>Given this data set one type of structure we might have an algorithm find is that it looks like this data set has points grouped into two separate clusters and so an algorithm that finds clusters like the ones I’ve just circled is called <strong><em>a clustering algorithm</em></strong>. And <strong>this would be our first type of unsupervised learning, although there will be other types of unsupervised learning algorithms that we’ll talk about later that finds other types of structure or other types of patterns in the data other than clusters.</strong> We’ll talk about this after we’ve talked about clustering. </p>
<p>So, what is clustering good for? Early in this class I already mentioned a few applications. One is market segmentation where you may have a database of customers and want to group them into different marker segments so you can sell to them separately or serve your different market segments better. Social network analysis. There are actually groups have done this things like looking at a group of people’s social networks. So, things like Facebook, Google+, or maybe information about who other people that you email the most frequently and who are the people that they email the most frequently and to find coherence in groups of people. So, this would be another maybe clustering algorithm where you know want to find who are the coherent groups of friends in the social network? Here’s something that one of my friends actually worked on which is, use clustering to organize computer clusters or to organize data centers better. Because if you know which computers in the data center in the cluster tend to work together, you can use that to reorganize your resources and how you layout the network and how you design your data center communications. And lastly, something that actually another friend worked on using clustering algorithms to understand galaxy formation and using that to understand astronomical data. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/3.png" alt="Applications_of_clustering"></p>
<p>So, that’s clustering which is our first example of an unsupervised learning algorithm. In the next video we’ll start to talk about a specific clustering algorithm.</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p>Unsupervised learning is contrasted from supervised learning because it uses an <strong>unlabeled</strong> training set rather than a labeled one.<br>In other words, we don’t have the vector y of expected results, we only have a dataset of features where we can find structure.<br>Clustering is good for:</p>
<ul>
<li>Market segmentation </li>
<li>Social network analysis </li>
<li>Organizing computer clusters </li>
<li>Astronomical data analysis</li>
</ul>
<h3 id="02-k-means-algorithm"><a href="#02-k-means-algorithm" class="headerlink" title="02_k-means-algorithm"></a>02_k-means-algorithm</h3><p>In the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us. <strong>The K Means algorithm is by far the most popular, by far the most widely used clustering algorithm</strong>, and in this video I would like to tell you what the K Means Algorithm is and how it works. The K means clustering algorithm is best illustrated in pictures. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/4.png" alt=""></p>
<p>Let’s say I want to take an unlabeled data set like the one shown here, and I want to group the data into two clusters. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/5.png" alt=""></p>
<p>If I run the K Means clustering algorithm, here is what I’m going to do. The first step is to randomly initialize two points, called the cluster centroids. So, these two crosses here, these are called the <strong>Cluster Centroids</strong> and I have two of them because I want to group my data into two clusters. <strong>K Means is an iterative algorithm and it does two things. First is a cluster assignment step, and second is a move centroid step.</strong> </p>
<p>So, let me tell you what those things mean. <strong>The first of the two steps</strong> in the loop of K means, is this <strong><em>cluster assignment step</em></strong>. What that means is that, it’s going through each of the examples, each of these green dots shown here and depending on whether it’s closer to the red cluster centroid or the blue cluster centroid, it is going to assign each of the data points to one of the two cluster centroids. Specifically, what I mean by that, is to go through your data set and color each of the points either red or blue, depending on whether it is closer to the red cluster centroid or the blue cluster centroid, and I’ve done that in this diagram here. So, that was the cluster assignment step. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/6.png" alt=""></p>
<p>The other part of K means, in the loop of K means, is the <strong>move centroid step</strong>, and what we are going to do is, we are going to take the two cluster centroids, that is, the red cross and the blue cross, and we are going to move them to the average of the points colored the same colour. So what we are going to do is look at all the red points and compute the average, really the mean of the location of all the red points, and we are going to move the red cluster centroid there. And the same things for the blue cluster centroid, look at all the blue dots and compute their mean, and then move the blue cluster centroid there. So, let me do that now. We’re going to move the cluster centroids as follows and I’ve now moved them to their new means. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/7.png" alt=""></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/8.png" alt=""></p>
<p>The red one moved like that and the blue one moved like that and the red one moved like that. And then we go back to another cluster assignment step, so we’re again going to look at all of my unlabeled examples and depending on whether it’s closer the red or the blue cluster centroid, I’m going to color them either red or blue. I’m going to assign each point to one of the two cluster centroids, so let me do that now. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/9.png" alt=""></p>
<p>And so the colors of some of the points just changed. And then I’m going to do another move centroid step. So I’m going to compute the average of all the blue points, compute the average of all the red points and move my cluster centroids like this, and so, let’s do that again. Let me do one more cluster assignment step. So colour each point red or blue, based on what it’s closer to and then do another move centroid step and we’re done.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/10.png" alt=""></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/11.png" alt=""></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/12.png" alt=""></p>
<p>And in fact if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further. And so, this is the, at this point, K means has converged and it’s done a pretty good job finding the two clusters in this data. </p>
<p>Let’s write out the K means algorithm more formally. <strong>The K means algorithm takes two inputs. One is a parameter K,</strong> which is the number of clusters you want to find in the data. I’ll later say how we might go about trying to choose k, but for now let’s just say that we’ve decided we want a certain number of clusters and we’re going to tell the algorithm how many clusters we think there are in the data set. <strong>And then K means also takes as input this sort of unlabeled training set of just the Xs and because this is unsupervised learning, we don’t have the labels Y anymore.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/13.png" alt=""></p>
<p>And <strong>for unsupervised learning of the K means I’m going to use the <em>* *convention</em> **that $X^{(i)}$ is an $R^N$ dimensional vector. And that’s why my training examples are now N dimensional rather N plus one dimensional vectors.</strong> This is what the K means algorithm does. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/14.png" alt=""></p>
<p><strong>The first step is that it randomly initializes k cluster centroids which we will call mu 1, mu 2, up to mu k.</strong> And so in the earlier diagram, the cluster centroids corresponded to the location of the red cross and the location of the blue cross. So there we had two cluster centroids, so maybe the red cross was mu 1 and the blue cross was mu 2, and more generally we would have k cluster centroids rather than just 2. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/15.png" alt=""></p>
<p><strong>Then the inner loop of k means does the following</strong>, we’re going to repeatedly do the following. First for each of my training examples, I’m going to set this variable $c^{(i)}$ to be the index 1 through K of the cluster centroid closest to $x^{(i)}$. So this was my <strong>cluster assignment step</strong>, where we took each of my examples and coloured it either red or blue, depending on which cluster centroid it was closest to. So $c^{(i)}$ is going to be a number from 1 to K that tells us, you know, is it closer to the red cross or is it closer to the blue cross, and another way of writing this is I’m going to, to compute $c^{(i)}$, I’m going to take my $i_{th}$ example $x^{(i)}$ and and I’m going to <strong>measure it’s distance to each of my cluster centroids</strong>, this is mu and then lower-case k, right, so capital K is the total number centroids and I’m going to use lower case k here to index into the different centroids. But so, <strong>$c^{(i)}$ is going to, I’m going to minimize over my values of k and find the value of K that minimizes this distance between Xi and the cluster centroid, and then, you know, the value of k that minimizes this, that’s what gets set in $c^{(i)}$.</strong> So, here’s another way of writing out what Ci is. If I write the norm between Xi minus Mu-k, then this is the distance between my ith training example Xi and the cluster centroid Mu subscript K, this is–this here, that’s a lowercase K. So uppercase K is going to be used to denote the total number of cluster centroids, and this lowercase K’s a number between one and capital K. I’m just using lower case K to index into my different cluster centroids. Next is lower case k. So that’s the distance between the example and the cluster centroid and so what I’m going to do is find the value of K, of lower case k that minimizes this, and so the value of k that minimizes you know, that’s what I’m going to set as Ci, and by convention here I’ve written the distance between Xi and the cluster centroid, by convention people actually tend to write this as the squared distance. So we think of Ci as picking the cluster centroid with the smallest squared distance to my training example Xi. But of course minimizing squared distance, and minimizing distance that should give you the same value of Ci, but we usually put in the square there, just as the convention that people use for K means. So that was the cluster assignment step.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/16.png" alt=""></p>
<p>The other in the loop of K means does the <strong>move centroid step</strong>. And what that does is for each of my cluster centroids, so for lower case k equals 1 through K, <strong>it sets Mu-k equals to the average of the points assigned to the cluster</strong>. So as a concrete example, let’s say that one of my cluster centroids, let’s say cluster centroid two, has training examples, you know, 1, 5, 6, and 10 assigned to it. And what this means is, really this means that C1 equals to C5 equals to C6 equals to and similarly well c10 equals, too, right? If we got that from the cluster assignment step, then that means examples 1,5,6 and 10 were assigned to the cluster centroid two. Then in this move centroid step, what I’m going to do is just compute the average of these four things. So X1 plus X5 plus X6 plus X10. And now I’m going to average them so here I have four points assigned to this cluster centroid, just take one quarter of that. And now Mu2 is going to be an n-dimensional vector. Because each of these example x1, x5, x6, x10 each of them were an n-dimensional vector, and I’m going to add up these things and, you know, divide by four because I have four points assigned to this cluster centroid, I end up with my move centroid step, for my cluster centroid mu-2. This has the effect of moving mu-2 to the average of the four points listed here. One thing that I’ve asked is, well here we said, let’s let mu-k be the average of the points assigned to the cluster. </p>
<p><strong>But what if there is a cluster centroid no points with zero points assigned to it. In that case the more common thing to do is to just eliminate that cluster centroid. And if you do that, you end up with K minus one clusters instead of k clusters. Sometimes if you really need k clusters, then the other thing you can do if you have a cluster centroid with no points assigned to it is you can just randomly reinitialize that cluster centroid, but it’s more common to just eliminate a cluster if somewhere during K means it with no points assigned to that cluster centroid, and that can happen, altthough in practice it happens not that often.</strong> So that’s the K means Algorithm. </p>
<p>Before wrapping up this video I just want to tell you about one other common application of K Means and that’s to the problems with non well separated clusters. Here’s what I mean. So far we’ve been picturing K Means and applying it to data sets like that shown here where we have three pretty well separated clusters, and we’d like an algorithm to find maybe the 3 clusters for us. <strong>But it turns out that very often K Means is also applied to data sets that look like this where there may not be several very well separated clusters.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/17.png" alt=""></p>
<p>Here is an example application, to t-shirt sizing. Let’s say you are a t-shirt manufacturer you’ve done is you’ve gone to the population that you want to sell t-shirts to, and you’ve collected a number of examples of the height and weight of these people in your population and so, well I guess height and weight tend to be positively highlighted so maybe you end up with a data set like this, you know, with a sample or set of examples of different peoples heights and weight. Let’s say you want to size your t shirts. Let’s say I want to design and sell t shirts of three sizes, small, medium and large. So how big should I make my small one? How big should I my medium? And how big should I make my large t-shirts. One way to do that would to be to run my k means clustering logarithm on this data set that I have shown on the right and maybe what K Means will do is group all of these points into one cluster and group all of these points into a second cluster and group all of those points into a third cluster. So, even though the data, you know, before hand it didn’t seem like we had 3 well separated clusters, K Means will kind of separate out the data into multiple pluses for you. And what you can do is then look at this first population of people and look at them and, you know, look at the height and weight, and try to design a small t-shirt so that it kind of fits this first population of people well and then design a medium t-shirt and design a large t-shirt. And this is in fact kind of an example of market segmentation where you’re using K Means to separate your market into 3 different segments. So you can design a product separately that is a small, medium, and large t-shirts, that tries to suit the needs of each of your 3 separate sub-populations well. So that’s the K Means algorithm. And by now you should know how to implement the K Means Algorithm and kind of get it to work for some problems. </p>
<p>But in the next few videos what I want to do is really get more deeply into the nuts and bolts of K means and to talk a bit about how to actually get this to work really well.</p>
<h4 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h4><p>The K-Means Algorithm is the most popular and widely used algorithm for automatically grouping data into coherent subsets. </p>
<ol>
<li>Randomly initialize two points in the dataset called the cluster centroids . </li>
<li>Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to. </li>
<li>Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages. </li>
<li>Re-run (2) and (3) until we have found our clusters. </li>
</ol>
<p>Our main variables are: </p>
<ul>
<li>K (number of clusters) </li>
<li>Training set ${x^{(1)}, x^{(2)}, \dots,x^{(m)}}$ </li>
<li>Where $x^{(i)} \in \mathbb{R}^n$ </li>
<li>Note that we will not use the $x_0=1$ convention. </li>
</ul>
<p><strong>The algorithm:</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)</span><br><span class="line">Repeat:</span><br><span class="line">   <span class="keyword">for</span> i = <span class="number">1</span> to m:</span><br><span class="line">      c(i):= index (from <span class="number">1</span> to K) of cluster centroid closest to x(i)</span><br><span class="line">   <span class="keyword">for</span> k = <span class="number">1</span> to K:</span><br><span class="line">      mu(k):= average (mean) of points assigned to cluster k</span><br></pre></td></tr></table></figure>

<p>The <strong>first for-loop</strong> is the ‘Cluster Assignment’ step. We make a vector c where $c^{(i)}$ represents the centroid assigned to example $x^{(i)}$ .<br>We can write the operation of the Cluster Assignment step more mathematically as follows:<br>$c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2$<br>That is, each $c^{(i)}$ contains the index of the centroid that has minimal distance to $x^{(i)}$.<br>By convention, we square the right-hand-side, which makes the function we are trying to minimize more sharply increasing. It is mostly just a convention. But a convention that helps reduce the computation load because the Euclidean distance requires a square root but it is canceled.<br>Without the square:<br>$$||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + …}\quad||$$<br>With the square:<br>$$||x^{(i)} - \mu_k||^2 = ||\quad(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + …\quad||$$<br>so the square convention serves two purposes, minimize more sharply and less computation.<br>The <strong>second for-loop</strong> is the ‘Move Centroid’ step where we move each centroid to the average of its group.<br>More formally, the equation for this loop is as follows:<br>$$\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n$$<br>Where each of $x^{(k_1)}, x^{(k_2)}, \dots, x^{(k_n)}$ are the training examples assigned to group $mμ_k$.<br>If you have a cluster centroid with 0 points assigned to it, you can randomly <strong>re-initialize</strong> that centroid to a new point. You can also simply <strong>eliminate</strong> that cluster group.<br>After a number of iterations the algorithm will <strong>converge</strong> , where new iterations do not affect the clusters.<br>Note on non-separated clusters: some datasets have no real inner separation or natural structure. K-means can still evenly segment your data into K subsets, so can still be useful in this case. </p>
<h3 id="03-optimization-objective"><a href="#03-optimization-objective" class="headerlink" title="03_optimization-objective"></a>03_optimization-objective</h3><p>Most of the supervised learning algorithms we’ve seen, things like linear regression, logistic regression, and so on, all of those algorithms have an optimization objective or some cost function that the algorithm was trying to minimize. <strong>It turns out that k-means also has an optimization objective or a cost function that it’s trying to minimize</strong>. </p>
<p>And in this video I’d like to tell you what that optimization objective is. And the reason I want to do so is because this will be useful to us for two purposes. <strong>First, knowing what is the optimization objective of k-means will help us to debug the learning algorithm and just make sure that k-means is running correctly. And second, and perhaps more importantly, in a later video we’ll talk about how we can use this to help k-means find better costs for this and avoid the local optima.</strong> </p>
<p>But we do that in a later video that follows this one. <strong>Just as a quick reminder while k-means is running we’re going to be keeping track of two sets of variables. First is the $c^{(i)}$’s and that keeps track of the index or the number of the cluster, to which an example $x^{(i)}$ is currently assigned. And then the other set of variables we use is $/mu_k$, which is the location of cluster centroid k.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/18.png" alt=""></p>
<p>Again, for k-means we use capital K to denote the total number of clusters. And here lower case k is going to be an index into the cluster centroids and so, lower case k is going to be a number between one and capital K. Now here’s one more bit of notation, which is gonna use mu subscript ci ($/mu_{c^{(i)}}$) to denote the cluster centroid of the cluster to which example $x^{(i)}$ has been assigned, right? And to explain that notation a little bit more, lets say that xi has been assigned to cluster number five. What that means is that ci, that is the index of xi, that that is equal to five. Right? Because having ci equals five, if that’s what it means for the example xi to be assigned to cluster number five. And so mu subscript ci is going to be equal to mu subscript 5. Because ci is equal to five. And so this mu subscript ci is the cluster centroid of cluster number five, which is the cluster to which my example xi has been assigned. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/19.png" alt="K-means_optimization_objective"></p>
<p>Out with this notation, we’re now ready to write out what is the optimization objective of the k-means clustering algorithm and here it is. The cost function that k-means is minimizing is a function J of all of these parameters, $c^{(1)}$ through $c^{(m)}$ and $/mu_1$ through $/gmu_K$. That k-means is varying as the algorithm runs. And the optimization objective is shown to the right, is the average of 1 over m of sum from i equals 1 through m of this term here. That I’ve just drawn the red box around, right? The square distance between each example xi and the location of the cluster centroid to which xi has been assigned. So let’s draw this and just let me explain this. Right, so here’s the location of training example xi and here’s the location of the cluster centroid to which example xi has been assigned. So to explain this in pictures, if here’s x1, x2, and if a point here is my example xi, so if that is equal to my example xi, and if xi has been assigned to some cluster centroid, I’m gonna denote my cluster centroid with a cross, so if that’s the location of mu 5, let’s say. If x i has been assigned cluster centroid five as in my example up there, then this square distance, that’s the square of the distance between the point xi and this cluster centroid to which xi has been assigned. And what k-means can be shown to be doing is that it is trying to define parameters ci and mu i. Trying to find c and mu to try to minimize this cost function J. This cost function is sometimes also called the <strong>distortion cost function</strong>, or the distortion of the k-means algorithm. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/20.png" alt="K-means-algorithm_in_optimization_view"></p>
<p>And just to provide a little bit more detail, here’s the k-means algorithm. Here’s exactly the algorithm as we have written it out on the earlier slide.  <strong>And what this first step of this algorithm is, this was the cluster assignment step where we assigned each point to the closest centroid. And it’s possible to show mathematically that what the cluster assignment step is doing is exactly Minimizing J, with respect to the variables c1, c2 and so on, up to cm, while holding the cluster centroids mu 1 up to mu K, fixed.</strong> So what the cluster assignment step does is it doesn’t change the cluster centroids, but what it’s doing is this is exactly picking the values of c1, c2, up to cm. That minimizes the cost function, or the distortion function J. And it’s possible to prove that mathematically, but I won’t do so here. But it has a pretty intuitive meaning of just well, let’s assign each point to a cluster centroid that is closest to it, because that’s what minimizes the square of distance between the points in the cluster centroid. <strong>And then the second step of k-means, this second step over here. The second step was the move centroid step. And once again I won’t prove it, but it can be shown mathematically that what the move centroid step does is it chooses the values of mu that minimizes J, so it minimizes the cost function J with respect to, wrt is my abbreviation for, with respect to, when it minimizes J with respect to the locations of the cluster centroids mu 1 through mu K.</strong> So if is really is doing is this taking the two sets of variables and partitioning them into two halves right here. First the c sets of variables and then you have the mu sets of variables. <strong><em>And what it does is it first minimizes J with respect to the variable c and then it minimizes J with respect to the variables mu and then it keeps on. And, so all that’s all that k-means does.</em></strong> And now that we understand k-means as trying to minimize this cost function J, we can also use this to try to debug other any algorithm and just kind of make sure that our implementation of k-means is running correctly. </p>
<p>So, we now understand the k-means algorithm as trying to optimize this cost function J, which is also called the distortion function. We can use that to debug k means and help make sure that k-means is converging and is running properly. And in the next video we’ll also see how we can use this to help k-means find better clusters and to help k-means to avoid local optima.</p>
<h4 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h4><p>Recall some of the parameters we used in our algorithm:<br>$c^{(i)}$ = index of cluster (1,2,…,K) to which example $x^{(i)}$ is currently assigned<br>$\mu_k $= cluster centroid k (μk∈ℝn)<br>$\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned<br>Using these variables we can define our <strong>cost function</strong> :<br>$$J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2$$<br>Our <strong>optimization objective</strong> is to minimize all our parameters using the above cost function:<br>$$min_{c,\mu}\ J(c,\mu)$$<br>That is, we are finding all the values in sets c, representing all our clusters, and μ, representing all our centroids, that will minimize the average of the distances of every training example to its corresponding cluster centroid.<br>The above cost function is often called the <strong>distortion</strong> of the training examples.<br>In the <strong>cluster assignment step</strong> , our goal is to:<br>Minimize J(…) with $c^{(1)},\dots,c^{(m)}$ (holding $\mu_1,\dots,\mu_K$ fixed)<br>In the <strong>move centroid step</strong>, our goal is to:<br>Minimize J(…) with $\mu_1,\dots,\mu_K$<br>With k-means, <strong>it is not possible for the cost function to sometimes increase</strong> . It should always descend. </p>
<h3 id="04-random-initialization"><a href="#04-random-initialization" class="headerlink" title="04_random-initialization"></a>04_random-initialization</h3><p>In this video, I’d like to talk about <strong>how to initialize K-means and more importantly, this will lead into a discussion of how to make K-means avoid local optima as well.</strong> Here’s the K-means clustering algorithm that we talked about earlier. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/21.png" alt=""></p>
<p><strong>One step</strong> that we never really talked much about was this step of how you randomly initialize the cluster centroids. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/22.png" alt="random_initialization"></p>
<p>There are few different ways that one can imagine using to randomly initialize the cluster centroids. But, it turns out that there is one method that is much more recommended than most of the other options one might think about. So, let me tell you about that option since it’s what often seems to work best. Here’s how I usually initialize my cluster centroids. <strong>When running K-means, you should have the number of cluster centroids, K, set to be less than the number of training examples M.</strong> It would be really weird to run K-means with a number of cluster centroids that’s, you know, equal or greater than the number of examples you have, right? <strong>So the way I usually initialize K-means is, I would randomly pick k training examples. So, and, what I do is then set $\mu_1$ of $\mu_K$ equal to these k examples</strong>.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/23.png" alt=""></p>
<p> Let me show you a concrete example. Lets say that k is equal to 2 and so on this example on the right let’s say I want to find two clusters. So, what I’m going to do in order to initialize my cluster centroids is, I’m going to randomly pick a couple examples. And let’s say, I pick this one and I pick that one. And the way I’m going to initialize my cluster centroids is, I’m just going to initialize my cluster centroids to be right on top of those examples. So that’s my first cluster centroid and that’s my second cluster centroid, and that’s one random initialization of K-means. The one I drew looks like a particularly good one. And sometimes I might get less lucky and maybe I’ll end up picking that as my first random initial example, and that as my second one. And here I’m picking two examples because k equals 2. Some we have randomly picked two training examples and if I chose those two then I’ll end up with, may be this as my first cluster centroid and that as my second initial location of the cluster centroid. So, that’s how you can randomly initialize the cluster centroids. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/24.png" alt=""></p>
<p> And so at initialization, your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on, if you have more clusters and more cluster centroid. And sort of the side common. I should say that in the earlier video where I first illustrated K-means with the animation. In that set of slides. Only for the purpose of illustration. I actually used a different method of initialization for my cluster centroids.But the method described on this slide, this is really the recommended way. And the way that you should probably use, when you implement K-means. So, as they suggested perhaps by these two illustrations on the right. You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized, and so, depending on the random initialization. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/25.png" alt=""></p>
<p><strong>K-means can end up at different solutions. And, in particular, K-means can actually end up at local optima.</strong> If you’re given the data sale like this. Well, it looks like, you know, there are three clusters, and so, if you run K-means and if it ends up at a good local optima this might be really the global optima, you might end up with that cluster ring. But if you had a particularly unlucky, random initialization, K-means can also get stuck at different local optima. So, in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points. And so, this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore, has split the second cluster into two separate sub-clusters like so, and it has also taken the second cluster and split it into two separate sub-clusters like so, and so, both of these examples on the lower right correspond to different local optima of K-means and in fact, in this example here, the cluster, the red cluster has captured only a single optima example. And the term local optima, by the way, refers to local optima of this distortion function J, and what these solutions on the lower left, what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it’s not doing a very good job minimizing this distortion function J. So, if you’re worried about K-means getting stuck in local optima, if you want to increase the odds of K-means finding the best possible clustering, like that shown on top here, what we can do, is try multiple, random initializations. <strong>So, instead of just initializing K-means once and hopping that that works, what we can do is, initialize K-means lots of times and run K-means lots of times, and use that to try to make sure we get as good a solution, as good a local or global optima as possible.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/26.png" alt=""></p>
<p> Concretely, here’s how you could go about doing that. Let’s say, I decide to run K-meanss a hundred times so I’ll execute this loop a hundred times and it’s fairly typical a number of times when came to will be something from 50 up to may be 1000. So, let’s say you decide to say K-means one hundred times. So what that means is that we would randomnly initialize K-means. And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings, and a set of cluster centroids, and then we would then compute the distortion J, that is compute this cause function on the set of cluster assignments and cluster centroids that we got. Finally, having done this whole procedure a hundred times. <strong>You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data, just pick one, that gives us the lowest cost. That gives us the lowest distortion. And it turns out that if you are running K-means with a fairly small number of clusters , so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often, can sometimes make sure that you find a better local optima. Make sure you find the better clustering data. But if K is very large, so, if K is much greater than 10, certainly if K were, you know, if you were trying to find hundreds of clusters, then, having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing, doing multiple random initializations will probably give you a slightly better solution but, but maybe not that much.</strong> But it’s really in the regime of where you have a relatively small number of clusters, especially if you have, maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering. So, that’s K-means with random initialization. If you’re trying to learn a clustering with a relatively small number of clusters, 2, 3, 4, 5, maybe, 6, 7, using multiple random initializations can sometimes, help you find much better clustering of the data. But, even if you are learning a large number of clusters, the initialization, the random initialization method that I describe here. That should give K-means a reasonable starting point to start from for finding a good set of clusters.</p>
<h4 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h4><p>There’s one particular recommended method for randomly initializing your cluster centroids. </p>
<ol>
<li>Have K&lt;m. That is, make sure the number of your clusters is less than the number of your training examples. </li>
<li>Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique). </li>
<li>Set $\mu_1,\dots,\mu_K$ equal to these K examples. </li>
</ol>
<p><strong>K-means can get stuck in local optima</strong>. To decrease the chance of this happening, you can run the algorithm on many different random initializations. In cases where K&lt;10 it is strongly recommended to run a loop of random initializations. </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to <span class="number">100</span>:</span><br><span class="line">   randomly initialize k-means</span><br><span class="line">   <span class="built_in">run</span> k-means to <span class="built_in">get</span> <span class="string">'c'</span> <span class="keyword">and</span> <span class="string">'m'</span></span><br><span class="line">   compute the cost function (distortion) J(c,m)</span><br><span class="line">pick the clustering that gave us the lowest cost</span><br></pre></td></tr></table></figure>
<h3 id="05-choosing-the-number-of-clusters"><a href="#05-choosing-the-number-of-clusters" class="headerlink" title="05_choosing-the-number-of-clusters"></a>05_choosing-the-number-of-clusters</h3><p>In this video I’d like to talk about <strong>one last detail of K-means clustering which is how to choose the number of clusters, or how to choose the value of the parameter capsule K.</strong> </p>
<p>To be honest, there actually isn’t a great way of answering this or doing this automatically and by far the most common way of choosing the number of clusters, is still choosing it manually by looking at visualizations or by looking at the output of the clustering algorithm or something else. But I do get asked this question quite a lot of how do you choose the number of clusters, and so I just want to tell you know what are peoples’ current thinking on it although, the most common thing is actually to choose the number of clusters by hand. A large part of why it might not always be easy to choose the number of clusters is that it is often generally ambiguous how many clusters there are in the data. Looking at this data set some of you may see four clusters and that would suggest using K equals 4. Or some of you may see two clusters and that will suggest K equals 2 and now this may see three clusters. <strong>And so, looking at the data set like this, the true number of clusters, it actually seems genuinely ambiguous to me, and I don’t think there is one right answer. And this is part of our supervised learning. We are aren’t given labels, and so there isn’t always a clear cut answer.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/27.png" alt="what_is_the_right_of_K"></p>
<p> And this is one of the things that makes it more difficult to say, have an automatic algorithm for choosing how many clusters to have. When people talk about ways of choosing the number of clusters, one method that people sometimes talk about is something called the <strong>Elbow Method</strong>. Let me just tell you a little bit about that, and then mention some of its advantages but also shortcomings.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/28.png" alt="elbow_method"></p>
<p> So the Elbow Method, what we’re going to do is vary K, which is the total number of clusters. So, we’re going to run K-means with one cluster, that means really, everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here. And then we’re going to run K means with two clusters, maybe with multiple random initial agents, maybe not. But then, you know, with two clusters we should get, hopefully, a smaller distortion, and so plot that there. And then run K-means with three clusters, hopefully, you get even smaller distortion and plot that there. I’m gonna run K-means with four, five and so on. And so we end up with a curve showing how the distortion, you know, goes down as we increase the number of clusters. And so we get a curve that maybe looks like this. And if you look at this curve, what the Elbow Method does it says “Well, let’s look at this plot. Looks like <strong>there’s a clear elbow there</strong>“. <strong>Right, this is, would be by analogy to the human arm where, you know, if you imagine that you reach out your arm, then, this is your shoulder joint, this is your elbow joint and I guess, your hand is at the end over here. And so this is the Elbow Method.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/29.png" alt="cost_j_with_elbow"></p>
<p>Then you find this sort of pattern where the distortion goes down rapidly from 1 to 2, and 2 to 3, and then you reach an elbow at 3, and then the distortion goes down very slowly after that. And then it looks like, you know what, maybe using three clusters is the right number of clusters, because that’s the elbow of this curve, right? That it goes down, distortion goes down rapidly until K equals 3, really goes down very slowly after that. So let’s pick K equals 3. If you apply the Elbow Method, and if you get a plot that actually looks like this, then, that’s pretty good, and this would be a reasonable way of choosing the number of clusters. </p>
<p>It turns out the Elbow Method isn’t used that often, and one reason is that, if you actually use this on a clustering problem, it turns out that fairly often, you know, you end up with a curve that looks much more ambiguous, maybe something like this.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/30.png" alt="cost_j_without_elbow"></p>
<p> And if you look at this, I don’t know, maybe there’s no clear elbow, but it looks like distortion continuously goes down, maybe 3 is a good number, maybe 4 is a good number, maybe 5 is also not bad. And so, if you actually do this in a practice, you know, if your plot looks like the one on the left and that’s great. It gives you a clear answer, but just as often, you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is. It  makes it harder to choose a number of clusters using this method. So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn’t necessarily, you know, have a very high expectation of it working for any particular problem.</p>
<p> Finally, here’s one other way of how, thinking about how you choose the value of K, very often people are running K-means in order you get clusters for some later purpose, or for some sort of downstream purpose. Maybe you want to use K-means in order to do market segmentation, like in the T-shirt sizing example that we talked about. Maybe you want K-means to organize a computer cluster better, or maybe a learning cluster for some different purpose, and so, if that later, downstream purpose, such as market segmentation. If that gives you an evaluation metric, then often, a better way to determine the number of clusters, is to see how well different numbers of clusters serve that later downstream purpose. Let me step through a specific example. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/31.png" alt="choosing_the_value_of_K_for_downstream_purpose"></p>
<p> Let me go through the T-shirt size example again, and I’m trying to decide, do I want three T-shirt sizes? So, I choose K equals 3, then I might have small, medium and large T-shirts. Or maybe, I want to choose K equals 5, and then I might have, you know, extra small, small, medium, large and extra large T-shirt sizes. So, you can have like 3 T-shirt sizes or four or five T-shirt sizes. We could also have four T-shirt sizes, but I’m just showing three and five here, just to simplify this slide for now. So, if I run K-means with K equals 3, maybe I end up with, that’s my small and that’s my medium and that’s my large. Whereas, if I run K-means with 5 clusters, maybe I end up with, those are my extra small T-shirts, these are my small, these are my medium, these are my large and these are my extra large. And the nice thing about this example is that, this then maybe gives us another way to choose whether we want 3 or 4 or 5 clusters, and in particular, what you can do is, you know, think about this from the perspective of the T-shirt business and ask: “Well if I have five segments, then how well will my T-shirts fit my customers and so, how many T-shirts can I sell? How happy will my customers be?” What really makes sense, from the perspective of the T-shirt business, in terms of whether, I want to have Goer T-shirt sizes so that my T-shirts fit my customers better. Or do I want to have fewer T-shirt sizes so that I make fewer sizes of T-shirts. And I can sell them to the customers more cheaply. And so, the t-shirt selling business, that might give you a way to decide, between three clusters versus five clusters. So, that gives you an example of how a later downstream purpose like the problem of deciding what T-shirts to manufacture, how that can give you an evaluation metric for choosing the number of clusters. </p>
<p> For those of you that are doing the program exercises, if you look at this week’s program exercise associative K-means, that’s an example there of using K-means for image compression. And so if you were trying to choose how many clusters to use for that problem, you could also, again use the evaluation metric of image compression to choose the number of clusters, K? So, how good do you want the image to look versus, how much do you want to compress the file size of the image, and, you know, if you do the programming exercise, what I’ve just said will make more sense at that time. </p>
<p> <strong>So, just summarize, for the most part, the number of customers K is still chosen by hand by human input or human insight. One way to try to do so is to use the Elbow Method, but I wouldn’t always expect that to work well, but I think the better way to think about how to choose the number of clusters is to ask, for what purpose are you running K-means? And then to think, what is the number of clusters K that serves that, you know, whatever later purpose that you actually run the K-means for.</strong></p>
<h4 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h4><p>Choosing K can be quite arbitrary and ambiguous.<br>The <strong>elbow method</strong> : plot the cost J and the number of clusters K. The cost function should reduce as we increase the number of clusters, and then flatten out. Choose K at the point where the cost function starts to flatten out.<br>However, fairly often, the curve is very <strong>gradual</strong> , so there’s no clear elbow.<br><strong>Note</strong>: J will always decrease as K is increased. The one exception is if k-means gets stuck at a bad local optimum.<br>Another way to choose K is to observe how well k-means performs on <strong>a downstream purpose</strong> . In other words, you choose K that proves to be most useful for some goal you’re trying to achieve from using these clusters. </p>
<h2 id="Bonus-Discussion-of-the-drawbacks-of-K-Means"><a href="#Bonus-Discussion-of-the-drawbacks-of-K-Means" class="headerlink" title="Bonus: Discussion of the drawbacks of K-Means"></a>Bonus: Discussion of the drawbacks of K-Means</h2><p>This links to a discussion that shows various situations in which K-means gives totally correct but unexpected results: <a href="http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means" target="_blank" rel="noopener">http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/12/12_support-vector-machines/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/12/12_support-vector-machines/" class="post-title-link" itemprop="url">12_support-vector-machines note12</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-12 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-12T00:00:00+05:30">2018-01-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 14:57:09" itemprop="dateModified" datetime="2020-04-09T14:57:09+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>84k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:16</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This personal note is written after studying the opening course on <a href="https://www.coursera.org" target="_blank" rel="noopener">the coursera website</a>, <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew NG</a> . And images, audios of this note all comes from the opening course. </p>
<h1 id="01-large-margin-classification"><a href="#01-large-margin-classification" class="headerlink" title="01_large-margin-classification"></a>01_large-margin-classification</h1><h2 id="01-optimization-objective"><a href="#01-optimization-objective" class="headerlink" title="01_optimization-objective"></a>01_optimization-objective</h2><p>By now, you’ve seen a range of difference learning algorithms. With supervised learning, the performance<br>of many supervised learning algorithms will be pretty similar, and <strong>what matters less often will be whether you use learning algorithm a or learning algorithm b, but what matters more will often be things like the amount of data you create these algorithms on, as well as your skill in applying these algorithms.</strong> Things like your choice of the features you design to give to the learning algorithms, and how you choose the colorization parameter, and things like that. </p>
<p>But, there’s one more algorithm that is very powerful and is very widely used both within industry and academia, and that’s called the Support Vector Machine. <strong>And compared to both logistic regression and neural networks, the Support Vector Machine, or SVM sometimes gives a cleaner, and sometimes more powerful way of learning complex non-linear functions.</strong> And so let’s take the next videos to talk about that. Later in this course, I will do a quick survey of a range of different supervisory algorithms just as a very briefly describe them. But the support vector machine, given its popularity and how powerful it is, this will be the last of the supervisory algorithms that I’ll spend a significant amount of time on in this course as with our development other learning algorithms, we’re gonna start by talking about the optimization objective. So, let’s get started on this algorithm. </p>
<p><strong>In order to describe the support vector machine, I’m actually going to start with logistic regression, and show how we can modify it a bit, and get what is essentially the support vector machine.</strong> So in logistic regression, we have our familiar form of the hypothesis there and the sigmoid activation function shown on the right. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/1.png" alt="img"></p>
<p>And in order to explain some of the math, I’m going to use z to denote theta transpose axiom. Now let’s think about what we would like logistic regression to do. If we have an example with y equals one and by this I mean an example in either the training set or the test set or the cross-validation set, but when y is equal to one then we’re sort of hoping that h of x will be close to one. Right, we’re hoping to correctly classify that example. And what having x subscript 1, what that means is that theta transpose x must be must larger than 0. So there’s greater than, greater than sign that means much, much greater than 0. And that’s because it is z, the theta of transpose x is when z is much bigger than 0 is far to the right of the sphere. That the outputs of logistic progression becomes close to one. Conversely, if we have an example where y is equal to zero, then what we’re hoping for is that the hypothesis will output a value close to zero. And that corresponds to theta transpose x of z being much less than zero because that corresponds to a hypothesis of putting a value close to zero. </p>
<p>If you look at the cost function of logistic regression, what you’ll find is that each example (x,y) contributes a term like this to the overall cost function, right? So for the overall cost function, we will also have a sum over all the chain examples and the 1 over m term, that this expression here, that’s the term that a single<br>training example contributes to the overall objective function so we can just rush them. Now if I take the definition for the fall of my hypothesis and plug it in over here, then what I get is that each training example contributes this term, ignoring the one over M but it contributes that term to my overall cost function for logistic regression. Now let’s consider two cases of when y is equal to one and when y is equal to zero. In the first case, let’s suppose that y is equal to 1. In that case, only this first term in the objective matters, because this one minus y term would be equal to zero if y is equal to one. So when y is equal to one, when in our example x comma y, when y is equal to 1 what we get is this term.. Minus log one over one, plus E to the negative Z where as similar to the last line I’m using Z to denote data transposed X and of course in a cost I should have this minus line that we just had if Y is equal to one so that’s equal to one I just simplify in a way in the expression that I have written down here. And if we plot this function as a function of z, what you find is that you get this curve shown on the lower left of the slide. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/2.png" alt="img"></p>
<p>And thus, we also see that when z is equal to large, that is, when theta transpose x is large, that corresponds to a value of z that gives us a fairly small value, a very, very small contribution to the consumption. And this kinda explains why, when logistic regression sees a positive example, with y=1, it tries to set theta transport x to be very large because that corresponds to this term, in the cross function, being small. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/3.png" alt="img"></p>
<p>Now, to fill the support vector machine, here’s what we’re going to do. We’re gonna take this cross function, this minus log 1 over 1 plus e to negative z, and modify it a little bit. Let me take this point 1 over here, and let me draw the cross functions you’re going to use. The new pass functions can be flat from here on out, and then we draw something that grows as a straight line, similar to logistic regression. But this is going to be a straight line at this portion. So the curve that I just drew in magenta, and the curve I just drew purple and magenta, so if <strong>it’s pretty close approximation to the cross function used by logistic regression. Except it is now made up of two line segments, there’s this flat portion on the right, and then there’s this straight line portion on the left</strong>. And don’t worry too much about the slope of the straight line portion. It doesn’t matter that much. But that’s the new cost function we’re going to use for when y is equal to one, and you can imagine it should do something pretty similar to logistic regression. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/4.png" alt="img"></p>
<p>But turns out, that this will give the support vector machine computational advantages and give us, later on, an easier optimization problem that would be easier for software to solve. We just talked about the case of y equals one. The other case is if y is equal to zero. In that case, if you look at the cost, then only the second term will apply because the first term goes away, right? If y is equal to zero, then you have a zero here, so you’re left only with the second term of the expression above. And so the cost of an example, or the contribution of the cost function, is going to be given by this term over here. And if you plot that as a function of z, to have pure z on the horizontal axis, you end up with this one. And for the support vector machine, once again, we’re going to replace this blue line with something similar and at the same time we replace it with a new cost, this flat out here, this 0 out here. And that then grows as a straight line, like so. </p>
<p><strong>So let me give these two functions names. This function on the left below I’m going to call cost subscript 1 of z $cost_1(z)$, and this function of the right I’m gonna call cost subscript 0 of z $cost_0(z)$. And the subscript just refers to the cost corresponding to when y is equal to 1, versus when y Is equal to zero.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/5.png" alt="img"></p>
<p>Armed with these definitions, we’re now ready to build a support vector machine. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/6.png" alt="img"></p>
<p>Here’s the cost function, j of theta, that we have for logistic regression. In case this equation looks a bit unfamiliar, it’s because previously we had a minus sign outside, but here what I did was I instead moved the minus signs inside these expressions, so it just makes it look a little different. For the support vector machine what we’re going to do is essentially take this and replace this with cost1 of z, that is cost1 of theta transpose x. And we’re going to take this and replace it with cost0 of z, that is cost0 of theta transpose x. Where the cost one function is what we had on the previous slide that looks like this. And the cost zero function, again what we had on the previous slide, and it looks like this. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/7.png" alt="img"></p>
<p>So what we have for the support vector machine is a minimization problem of one over M, the sum of Y I times cost one, theta transpose X I, plus one minus Y I times cause zero of theta transpose X I, and then plus my usual regularization parameter. Like so. </p>
<p>Now, by convention, for the support of vector machine, we’re actually write things slightly different. We re-parameterize this just very slightly differently. First, we’re going to get rid of the 1 over m terms, and this just this happens to be a slightly different convention that people use for support vector machines compared to or just a progression. But here’s what I mean. You’re one way to do this, we’re just gonna get rid of these one over m terms and this should give you me the same optimal value of beta right? Because one over m is just as constant so whether I solved this minimization problem with one over n in front or not. I should end up with the same optimal value for theta. </p>
<p>Here’s what I mean, to give you an example, suppose I had a minimization problem  $min_u(u-5)^2+1$. Well, the minimum of this happens to be $u=5$. Now if I were to take this objective function and multiply it by 10. So here my minimization problem is min over U, 10 U minus five squared plus 10 ( $min_u10[(u-5)^2+1]$ ). Well the value of U that minimizes this is still U equals five right? So multiply something that you’re minimizing over, by some constant, 10 in this case, it does not change the value of U that gives us, that minimizes this function. So the same way, what I’ve done is by crossing out the M is all I’m doing is multiplying my objective function by some constant M and it doesn’t change the value of theta. That achieves the minimum. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/8.png" alt="img"></p>
<p>The second bit of notational change, which is just, again, the more standard convention when using SVMs instead of logistic regression, is the following. So for logistic regression, we add two terms to the objective function. The first is this term, which is the cost that comes from the training set and the second is this row, which is the regularization term. And what we had was we had a, we control the trade-off between these by saying, what we want is A plus, and then my regularization parameter lambda. And then times some other term B, where I guess I’m using your A to denote this first term, and I’m using B to denote the second term, maybe without the lambda. And instead of prioritizing this as A plus lambda B, and so what we did was by setting different values for this regularization parameter lambda, we could trade off the relative weight between how much we wanted the training set well, that is, minimizing A, versus how much we care about keeping the values of the parameter small, so that will be, the parameter is B for the support vector machine, just by convention, we’re going to use a different parameter. So instead of using lambda here to control the relative waiting between the first and second terms. We’re instead going to use a different parameter which by convention is called C and is set to minimize $C \times a + B$. So for logistic regression, if we set a very large value of lambda, that means you will give B a very high weight. Here is that if we set C to be a very small value, then that responds to giving B a much larger rate than C, than A. So this is just a different way of controlling the trade off, it’s just a different way of prioritizing how much we care about optimizing the first term, versus how much we care about optimizing the second term. And if you want you can think of this as the parameter C playing a role similar to 1 over lambda. And it’s not that it’s two equations or these two expressions will be equal. This equals 1 over lambda, that’s not the case. <strong>It’s rather that if C is equal to 1 over lambda, then these two optimization objectives should give you the same value, the same optimal value for theta.</strong> so we just filling that in I’m gonna cross out lambda here and write in the constant C there. So that gives us our overall optimization objective function for the support vector machine. And if you minimize that function, then what you have is the parameters learned by the SVM. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/9.png" alt="img"></p>
<p>*<em>Finally unlike logistic regression, the support vector machine doesn’t output the probability is that what we have is we have this cost function, that we minimize to get the parameter’s data, and what a support vector machine does is it just makes a prediction of y being equal to one or zero, directly. So the hypothesis will predict one if theta transpose x is greater or equal to zero, and it will predict zero otherwise and so having learned the parameters theta, this is the form of the hypothesis for the support vector machine. So that was a mathematical definition of what a support vector machine does. *</em></p>
<p>In the next few videos, let’s try to get back to intuition about what this optimization objective leads to and whether the source of the hypotheses SVM will learn and we’ll also talk about how to modify this just a little bit to the complex nonlinear functions.</p>
<h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p><strong>Optimization Objective</strong><br>The Support Vector Machine (SVM) is yet another type of supervised machine learning algorithm. It is sometimes cleaner and more powerful.<br>Recall that in logistic regression, we use the following rules:<br>if y=1, then $h_\theta(x) \approx 1$ and $\Theta^Tx \gg 0$<br>if y=0, then $h_\theta(x) \approx 0$ and $\Theta^Tx \ll 0$<br>Recall the cost function for (unregularized) logistic regression: </p>
<p>$$\begin{align<em>}J(\theta) &amp; = \frac{1}{m}\sum_{i=1}^m -y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))\ &amp; = \frac{1}{m}\sum_{i=1}^m -y^{(i)} \log\Big(\dfrac{1}{1 + e^{-\theta^Tx^{(i)}}}\Big) - (1 - y^{(i)})\log\Big(1 - \dfrac{1}{1 + e^{-\theta^Tx^{(i)}}}\Big)\end{align</em>}$$ </p>
<p>To make a support vector machine, we will modify the first term of the cost function $-\log(h_{\theta}(x)) = -\log\Big(\dfrac{1}{1 + e^{-\theta^Tx}}\Big)$ so that when $θ^Tx$ (from now on, we shall refer to this as z) is <strong>greater than</strong> 1, it outputs 0. Furthermore, for values of z <strong>less than</strong> 1, we shall use a straight decreasing line instead of the sigmoid curve.(In the literature, this is called a hinge loss ( <a href="https://en.wikipedia.org/wiki/Hinge_loss" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Hinge_loss</a>) function.) </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/10.png" alt=""></p>
<p>Similarly, we modify the second term of the cost function $$-\log(1 - h_{\theta(x)}) = -\log\Big(1 - \dfrac{1}{1 + e^{-\theta^Tx}}\Big)$$ so that when z is less than -1, it outputs 0. We also modify it so that for values of z greater than -1, we use a straight increasing line instead of the sigmoid curve. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/11.png" alt="logistic_2_of_summary_of_svm_objective"></p>
<p>We shall denote these as $\text{cost}<em>1(z)$ and $\text{cost}_0(z)$ (respectively, note that $\text{cost}_1(z)$ is the cost for classifying when y=1, and $\text{cost}_0(z)$ is the cost for classifying when y=0), and we may define them as follows (where k is an arbitrary constant defining the magnitude of the slope of the line):<br>$$z = \theta^Tx$$<br>$$\text{cost}_0(z) = \max(0, k(1+z))$$<br>$$\text{cost}_1(z) = \max(0, k(1-z))$$<br>Recall the full cost function from (regularized) logistic regression:<br>$$J(\theta) = \frac{1}{m} \sum</em>{i=1}^m y^{(i)}(-\log(h_\theta(x^{(i)}))) + (1 - y^{(i)})(-\log(1 - h_\theta(x^{(i)}))) + \dfrac{\lambda}{2m}\sum_{j=1}^n \Theta^2_j$$<br>Note that the negative sign has been distributed into the sum in the above equation.<br>We may transform this into the cost function for support vector machines by substituting $\text{cost}<em>0(z)$ and $\text{cost}_1(z)$:<br>$$J(\theta) = \frac{1}{m} \sum</em>{i=1}^m y^{(i)} \ \text{cost}<em>1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{\lambda}{2m}\sum</em>{j=1}^n \Theta^2_j$$<br>We can optimize this a bit by multiplying this by m (thus removing the m factor in the denominators). Note that this does not affect our optimization, since we’re simply multiplying our cost function by a positive constant (for example, minimizing $$(u-5)^2 + 1$$ gives us 5; multiplying it by 10 to make it $$10(u-5)^2 + 10$$ still gives us 5 when minimized).<br>$$J(\theta) = \sum_{i=1}^m y^{(i)} \ \text{cost}<em>1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{\lambda}{2}\sum</em>{j=1}^n \Theta^2_j$$<br>Furthermore, convention dictates that we regularize using a factor C, instead of λ, like so:<br>$$J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}<em>1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum</em>{j=1}^n \Theta^2_j$$<br>This is equivalent to multiplying the equation by $C = \dfrac{1}{\lambda}$, and thus results in the same values when optimized. Now, when we wish to regularize more (that is, reduce overfitting), we decrease C, and when we wish to regularize less (that is, reduce underfitting), we increase C.<br>Finally, note that the hypothesis of the Support Vector Machine is not interpreted as the probability of y being 1 or 0 (as it is for the hypothesis of logistic regression). Instead, it outputs either 1 or 0. (In technical terms, it is a discriminant function.)<br>$$h_\theta(x) =\begin{cases} 1 &amp; \text{if} \ \Theta^Tx \geq 0 \ 0 &amp; \text{otherwise}\end{cases}$$ </p>
<h2 id="02-large-margin-intuition"><a href="#02-large-margin-intuition" class="headerlink" title="02_large-margin-intuition"></a>02_large-margin-intuition</h2><p>Sometimes people talk about support vector machines, as large margin classifiers, in this video I’d like to tell you what that means, and this will also give us a useful picture of what an SVM hypothesis may look like. </p>
<p>Here’s my cost function for the support vector machine where here on the left I’ve plotted my cost 1 of z function that I used for positive examples and on the right I’ve  plotted my zero of ‘Z’ function, where I have ‘Z’ here on the horizontal axis. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/12.png" alt="img"></p>
<p>Now, let’s think about what it takes to make these cost functions small. If you have a positive example, so if y is equal to 1, then cost 1 of Z is zero only when Z is greater than or equal to 1. So in other words, if you have a positive example, we really want theta transpose x to be greater than or equal to 1 and conversely if y is equal to zero, look this cost zero of z function, then it’s only in this region where z is less than equal to 1 we have the cost is zero as z is equals to zero, and this is an interesting property of the support vector machine right, which is that, if you have a positive example so if y is equal to one, then all we really need is that theta transpose x is greater than equal to zero.And that would mean that we classify correctly because if theta transpose x is greater than zero our hypothesis will predict zero. And similarly, if you have a negative example, then really all you want is that theta transpose x is less than zero and that will make sure we got the example right. <strong>But the support vector machine wants a bit more than that. It says, you know, don’t just barely get the example right. So then don’t just have it just a little bit bigger than zero. What i really want is for this to be quite a lot bigger than zero say maybe bit greater or equal to one and I want this to be much less than zero. Maybe I want it less than or equal to -1. And so this builds in an extra safety factor or safety margin factor into the support vector machine.</strong> </p>
<p>Logistic regression does something similar too of course, but let’s see what happens or let’s see what the consequences of this are, in the context of the support vector machine. Concretely, what I’d like to do next is consider a case case where we set this constant C to be a very large value, so let’s imagine we set C to a very large value, may be a hundred thousand, some huge number. Let’s see what the support vector machine will do. <strong>If C is very, very large, then when minimizing this optimization objective, we’re going to be highly motivated to choose a value, so that this first term is equal to zero.</strong> So let’s try to understand the optimization problem in the context of, what would it take to make this first term in the objective equal to zero, because you know, maybe we’ll set C to some huge constant, and this will hope, this should give us additional intuition about what sort of hypotheses a support vector machine learns. So we saw already that <strong>whenever you have a training example with a label of y=1 if you want to make that first term zero, what you need is is to find a value of theta so that theta transpose $x^{(i)}$ is greater than or equal to 1. And similarly, whenever we have an example, with label zero, in order to make sure that the cost, cost zero of Z,  in order to make sure that cost is zero we need that theta transpose $x^{(i)}$ is less than or equal to -1.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/13.png" alt="img"></p>
<p>So, if we think of our optimization problem as now, really choosing parameters and show that this first term is equal to zero, what we’re left with is the following optimization problem. We’re going to minimize that first term zero, so C times zero, because we’re going to choose parameters so that’s equal to zero, plus one half and then you know that second term and this first term is ‘C’ times zero, so let’s just cross that out because I know that’s going to be zero. And this will be subject to the constraint that theta transpose $x^{(i)}$ is greater than or equal to one, if $y^{(i)}$ Is equal to one and theta transpose $x^{(i)}$ is less than or equal to minus one whenever you have a negative example and it turns out that when you solve this optimization problem, when you minimize this as a function of the parameters theta you get a very interesting decision boundary.<br>$$<br>\min_\limits{\theta}C\sum_\limits{i=1}^{m}\left[y^{(i)}{\cos}t_{1}\left(\theta^{T}x^{(i)}\right)+\left(1-y^{(i)}\right){\cos}t\left(\theta^{T}x^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{i=1}^{n}\theta^{2}_{j}<br>$$<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/14.png" alt="img"></p>
<p>Concretely, if you look at a data set like this with positive and negative examples,  this data is linearly separable and by that, I mean that there exists, you know, a straight line, although there is many a different straight lines, they can separate the positive and negative examples perfectly. For example, here is one green decision boundary that separates the positive and negative examples, but somehow that doesn’t look like a very natural one, right? </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/41.png" alt="img"></p>
<p>Or by drawing an even worse one, you know here’s another magenta decision boundary that separates the positive and negative examples but just barely. But neither of those seem like particularly good choices. The Support Vector Machines will instead choose this decision boundary, which I’m drawing in black. And that seems like a much better decision boundary than either of the ones that I drew in magenta or in green. The black line seems like a more robust separator, it does a better job of separating the positive and negative examples. And mathematically, what that does is, this black decision boundary has a larger distance. That distance is called the margin, when I draw up this two extra blue lines, we see that the black decision boundary has some larger minimum distance from any of my training examples, whereas the magenta and the green lines they come awfully close to the training examples. and then that seems to do a less a good job separating the positive and negative classes than my black line. And so this distance is called the margin of the support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large margin as possible. So the support vector machine is sometimes also called a large margin classifier and this is actually a consequence of the optimization problem we wrote down on the previous slide. </p>
<p>I know that you might be wondering how is it that the optimization problem I wrote down in the previous slide, how does that lead to this large margin classifier. I know I haven’t explained that yet. And <strong>in the next video I’m going to sketch a little bit of the intuition about why that optimization problem gives us this large margin classifier. But this is a useful feature to keep in mind if you are trying to understand what are the sorts of hypothesis that an SVM will choose. That is, trying to separate the positive and negative examples with as big a margin as possible.</strong><br>$$<br>\min_\limits{\theta}C\sum_\limits{i=1}^{m}\left[y^{(i)}{\cos}t_{1}\left(\theta^{T}x^{(i)}\right)+\left(1-y^{(i)}\right){\cos}t\left(\theta^{T}x^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{i=1}^{n}\theta^{2}<em>{j} \<br>\min_\limits{\theta} \frac{1}{2}\sum\limits</em>{j=1}^{n} s.t. \cases{\theta^Tx \ge 1 &amp; if y=1, then left term is 0 \\theta^Tx \le 0 &amp; if y=-1, then left term is 0}<br>$$</p>
<h3 id="outliers"><a href="#outliers" class="headerlink" title="outliers"></a>outliers</h3><p>I want to say one last thing about large margin classifiers in this intuition, so we wrote out this large margin classification setting in the case of when C, that regularization concept, was very large, I think I set that to a hundred thousand or something. So given a dataset like this, maybe we’ll choose that decision boundary that separate the positive and negative examples on large margin.  (personal note : In order to minimize the cost function as possible as the classifier, SVM , can, it need to choose a proper $\theta$ to make the left term equal to 0 when C is a pretty large constant)</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/15.png" alt="img"></p>
<p>Now, the SVM is actually slightly more sophisticated than this large margin view might suggest. And in particular, if all you’re doing is use a large margin classifier then your learning algorithms can be sensitive to outliers, so lets just add an extra positive example like that shown on the screen. If he had one example then it seems as if to separate data with a large margin, maybe I’ll end up learning a decision boundary like that, right? that is the magenta line and it’s really not clear that based on the single outlier based on a single example and it’s really not clear that it’s actually a good idea to change my decision boundary from the black one over to the magenta one. <strong>So, if C, if the regularization parameter C were very large, then this is actually what SVM will do, it will change the decision boundary from the black to the magenta one but if C were reasonably small if you were to use the C, not too large then you still end up with this black decision boundary.</strong>  </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/16.png" alt="Large_margin_classifier_in_presence_of_outlier"></p>
<p>And of course if the data were not linearly separable so if you had some positive examples in here, or if you had some negative examples in here then the SVM will also do the right thing. And so this picture of a large margin classifier that’s really, that’s really the picture that gives better intuition only for the case of when the regulations parameter C is very large, and just to remind you this corresponds C plays a role similar to one over Lambda, where Lambda is the regularization parameter we had previously. And so it’s only of one over Lambda is very large or equivalently if Lambda is very small that you end up with things like this Magenta decision boundary, but in practice when applying support vector machines, <strong>when C is not very very large like that, it can do a better job ignoring the few outliers like here. And also do fine and  do reasonable things even if your data is not linearly separable</strong>. But when we talk about bias and variance in the context of support vector machines which will do a little bit later, <strong>hopefully all of of this trade-offs involving the regularization parameter will become clearer at that time. So I hope that gives some intuition about how this support vector machine functions as a large margin classifier that tries to separate the data with a large margin, technically this picture of this view is true only when the parameter C is very large, which is a useful way to think about support vector machines.</strong> There was one missing step in this video which is, why is it that the optimization problem we wrote down on these slides, how does that actually lead to the large margin classifier, I didn’t do that in this video, in the next video I will sketch a little bit more of the math behind that to explain that separate reasoning of how the optimization problem we wrote out results in a large margin classifier.</p>
<h3 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h3><p>A useful way to think about Support Vector Machines is to think of them as Large Margin Classifiers .<br>If y=1, we want $\Theta^Tx \geq 1$ (not just ≥0)<br>If y=0, we want $\Theta^Tx \leq -1$ (not just &lt;0)<br>Now when we set our constant C to a <strong>very large</strong> value (e.g. 100,000), our optimizing function will constrain Θ such that the equation A (the summation of the cost of each example) equals 0. We impose the following constraints on Θ:<br>$$\Theta^Tx \geq 1 \text{ if y=1 and } \Theta^Tx \leq -1 \text{ if y=0 .}$$<br>If C is very large, we must choose Θ parameters such that:<br>$$\sum_{i=1}^m y^{(i)}\text{cost}<em>1(\Theta^Tx) + (1 - y^{(i)})\text{cost}_0(\Theta^Tx) = 0$$<br>This reduces our cost function to:<br>$$ \begin{align*} J(\theta) = C \cdot 0 + \dfrac{1}{2}\sum</em>{j=1}^n \Theta^2_j \newline = \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j \end{align<em>}$$<br>Recall the decision boundary from logistic regression (the line separating the positive and negative examples). In SVMs, the decision boundary has the special property that it is *</em>as far away as possible** from both the positive and the negative examples.<br>The distance of the decision boundary to the nearest example is called the <strong>margin</strong> . Since SVMs maximize this margin, it is often called a <strong>Large Margin Classifier</strong>.<br>The SVM will separate the negative and positive examples by a large margin .<br>This large margin is only achieved when <strong>C is very large</strong> .<br>Data is <strong>linearly separable</strong> when a <strong>straight line</strong> can separate the positive and negative examples.<br>If we have <strong>outlier</strong> examples that we don’t want to affect the decision boundary, then we can <strong>reduce</strong> C.<br>Increasing and decreasing C is similar to respectively decreasing and increasing $λ$, and can simplify our decision boundary. </p>
<h2 id="03-mathematics-behind-large-margin-classification"><a href="#03-mathematics-behind-large-margin-classification" class="headerlink" title="03_mathematics-behind-large-margin-classification"></a>03_mathematics-behind-large-margin-classification</h2><p>In this video, I’d like to tell you a bit about the math behind large margin classification. This video is optional, so please feel free to skip it. It may also give you better intuition about how the optimization problem of the support vex machine, how that leads to large margin classifiers. In order to get started, let me first remind you of a couple of properties of what vector inner products look like. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/17.png" alt=""></p>
<p>Let’s say I have two vectors U and V, that look like this. So both two dimensional vectors. Then let’s see what U transpose V looks like. And U transpose V is also called the inner products between the vectors U and V. Use a two dimensional vector, so I can on plot it on this figure. So let’s say that’s the vector U. And what I mean by that is if on the horizontal axis that value takes whatever value U1 is and on the vertical axis the height of that is whatever U2 is the second component of the vector U. Now, one quantity that will be nice to have is the norm of the vector U. So, these are, you know, double bars on the left and right that denotes the norm or length of U. So this just means; really the euclidean length of the vector U. And this is Pythagoras theorem is just equal to U1 squared plus U2 squared square root, right? And this is the length of the vector U. That’s a real number. Just say you know, what is the length of this, what is the length of this vector down here. What is the length of this arrow that I just drew, is the normal view? Now let’s go back and look at the vector V because we want to compute the inner product. So V will be some other vector with, you know, some value V1, V2. And so, the vector V will look like that, towards V like so. Now let’s go back and look at how to compute the inner product between U and V. Here’s how you can do it. Let me take the vector V and project it down onto the vector U. So I’m going to take a orthogonal projection or a 90 degree projection, and project it down onto U like so. And what I’m going to do measure length of this red line that I just drew here. So, I’m going to call the length of that red line P. So, P is the length or is the magnitude of the projection of the vector V onto the vector U. Let me just write that down. So, P is the length of the projection of the vector V onto the vector U. And it is possible to show that unit product U transpose V, that this is going to be equal to P  times the norm or the length of the vector U. So, this is one way to compute the inner product. And if you actually do the geometry figure out what P is and figure out what the norm of U is. This should give you the same way, the same answer as the other way of computing unit product. Right. Which is if you take U transpose V then U transposes this U1 U2, its a one by two matrix, 1 times V. And so this should actually give you U1, V1 plus U2, V2. And so the theorem of linear algebra that these two formulas give you the same answer. And by the way, U transpose V is also equal to V transpose U. So if you were to do the same process in reverse, instead of projecting V onto U, you could project U onto V. Then, you know, do the same process, but with the rows of U and V reversed. And you would actually, you should actually get the same number whatever that number is. And just to clarify what’s going on in this equation the norm of U is a real number and P is also a real number. And so U transpose V is the regular multiplication as two real numbers of the length of P times the normal view.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/18.png" alt=""><br>Just one last detail, which is if you look at the norm of P, P is actually signed so to the right. And it can either be positive or negative. So let me say what I mean by that, if U is a vector that looks like this and V is a vector that looks like this. So if the angle between U and V is greater than ninety degrees. Then if I project V onto U, what I get is a projection it looks like this and so that length P. And in this case, I will still have that U transpose V is equal to P times the norm of U. Except in this example P will be negative. So, you know, in inner products if the angle between U and V is less than ninety degrees, then P is the positive length for that red line whereas if the angle of this angle of here is greater than 90 degrees then P here will be negative of the length of the super line of that little line segment right over there. So the inner product between two vectors can also be negative if the angle between them is greater than 90 degrees. So that’s how vector inner products work. We’re going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/19.png" alt=""><br>Here is the optimization objective for the support vector machine that we worked out earlier. Just for the purpose of this slide I am going to make one simplification or once just to make the objective easy to analyze and what I’m going to do is ignore the indeceptrums.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/20.png" alt=""><br>So, we’ll just ignore theta 0 and set that to be equal to 0. To make things easier to plot, I’m also going to set N the number of features to be equal to 2. So, we have only 2 features, X1 and X2. Now, let’s look at the objective function. The optimization objective of the SVM. What we have only two features. When N is equal to 2. This can be written, one half of theta one squared plus theta two squared. Because we only have two parameters, theta one and theta two. What I’m going to do is rewrite this a bit. I’m going to write this as one half of theta one squared plus theta two squared and the square root squared. And the reason I can do that, is because for any number, you know, W, right, the square roots of W and then squared, that’s just equal to W. So square roots and squared should give you the same thing. What you may notice is that this term inside is that’s equal to the norm or the length of the vector theta and what I mean by that is that if we write out the vector theta like this, as you know theta one, theta two. Then this term that I’ve just underlined in red, that’s exactly the length, or the norm, of the vector theta. We are calling the definition of the norm of the vector that we have on the previous line. And in fact this is actually equal to the length of the vector theta, whether you write it as theta zero, theta 1, theta 2. That is, if theta zero is equal to zero, as I assume here. Or just the length of theta 1, theta 2; but for this line I am going to ignore theta 0. So let me just, you know, treat theta as this, let me just write theta, the normal theta as this theta 1, theta 2 only, but the math works out either way, whether we include theta zero here or not. So it’s not going to matter for the rest of our derivation. And so finally this means that my optimization objective is equal to one half of the norm of theta squared. <strong>Support vector machine is doing in the optimization objective is it’s minimizing the squared norm of the square length of the parameter vector theta.</strong> </p>
<p>Now what I’d like to do is look at these terms, theta transpose X and understand better what they’re doing. So given the parameter vector theta and given and example x, what is this is equal to? And on the previous slide, we figured out what U transpose V looks like, with different vectors U and V. And so we’re going to take those definitions, you know, with theta and X(i) playing the roles of U and V. And let’s see what that picture looks like. So, let’s say I plot. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/21.png" alt="img"></p>
<p>Let’s say I look at just a single training example. Let’s say I have a positive example the drawing was across there and let’s say that is my example X(i), what that really means is plotted on the horizontal axis some value X(i) 1 and on the vertical axis X(i) 2. That’s how I plot my training examples. And although we haven’t been really thinking of this as a vector, what this really is, this is a vector from the origin from 0, 0 out to the location of this training example. And now let’s say we have a parameter vector and I’m going to plot that as vector, as well. What I mean by that is if I plot theta 1 here and theta 2 there so what is the inner product theta transpose X(i). While using our earlier method, the way we compute that is we take my example and project it onto my parameter vector theta. And then I’m going to look at the length of this segment that I’m coloring in, in red. And I’m going to call that P superscript I to denote that this is a projection of the i-th training example onto the parameter vector theta. And so what we have is that theta transpose X(i) is equal to following what we have on the previous slide, this is going to be equal to P times the length of the norm of the vector theta. And this is of course also equal to theta 1 x1 plus theta 2 x2. So each of these is, you know, an equally valid way of computing the inner product between theta and X(i). Okay. So where does this leave us? *<em>What this means is that, this constrains that theta transpose X(i) be greater than or equal to one or less than minus one. What this means is that it can replace the use of constraints that P(i) times X be greater than or equal to one. Because theta transpose X(i) is equal to P(i) times the norm of theta. *</em></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/22.png" alt="img"></p>
<p>So writing that into our optimization objective. This is what we get where I have, instead of theta transpose X(i), I now have this P(i) times the norm of theta. And just to remind you we worked out earlier too that this optimization objective can be written as one half times the norm of theta squared. So, now let’s consider the training example that we have at the bottom and for now, continuing to use the simplification that theta 0 is equal to 0. Let’s see what decision boundary the support vector machine will choose. Here’s one option, let’s say the support vector machine were to choose this decision boundary. This is not a very good choice because it has very small margins. This decision boundary comes very close to the training examples. Let’s see why the support vector machine will not do this. For this choice of parameters it’s possible to show that the parameter vector theta is actually at 90 degrees to the decision boundary. And so, that green decision boundary corresponds to a parameter vector theta that points in that direction. And by the way, the simplification that theta 0 equals 0 that just means that the decision boundary must pass through the origin, (0,0) over there. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/23.png" alt="img"></p>
<p>So now, let’s look at what this implies for the optimization objective. Let’s say that this example here. Let’s say that’s my first example, you know, X1. If we look at the projection of this example onto my parameters theta. That’s the projection. And so that little red line segment. That is equal to P1. And that is going to be pretty small, right. And similarly, if this example here, if this happens to be X2, that’s my second example. Then, if I look at the projection of this this example onto theta. You know. Then, let me draw this one in magenta. This little magenta line segment, that’s going to be P2. That’s the projection of the second example onto my, onto the direction of my parameter vector theta which goes like this. And so, this little projection line segment is getting pretty small. P2 will actually be a negative number, right so P2 is in the opposite direction. This vector has greater than 90 degree angle with my parameter vector theta, it’s going to be less than 0. <strong>And so what we’re finding is that these terms P(i) are going to be pretty small numbers. So if we look at the optimization objective and see, well, for positive examples we need P(i) times the norm of theta to be bigger than either one. But if P(i) over here, if P1 over here is pretty small, that means that we need the norm of theta to be pretty large, right? If P1 of theta is small and we want P1 you know times in all of theta to be bigger than either one, well the only way for that to be true for the profit that these two numbers to be large if P1 is small, as we said we want the norm of theta to be large. And similarly for our negative example, we need P2 times the norm of theta to be less than or equal to minus one. And we saw in this example already that P2 is going pretty small negative number, and so the only way for that to happen as well is for the norm of theta to be large, but what we are doing in the optimization objective is we are trying to find a setting of parameters where the norm of theta is small, and so you know, so this doesn’t seem like such a good direction for the parameter vector and theta.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/24.png" alt="img"></p>
<p>In contrast, just look at a different decision boundary. Here, let’s say, this SVM chooses that decision boundary. Now the is going to be very different. If that is the decision boundary, here is the corresponding direction for theta. So, with the direction boundary you know, that vertical line that corresponds to it is possible to show using linear algebra that the way to get that green decision boundary is have the vector of theta be at 90 degrees to it, and now if you look at the projection of your data onto the vector x, lets say its before this example is my example of x1. So when I project this on to x, or onto theta, what I find is that this is P1. That length there is P1. The other example, that example is and I do the same projection and what I find is that this length here is a P2 really that is going to be less than 0. And you notice that now P1 and P2, these lengths of the projections are going to be much bigger, and <strong>so if we still need to enforce these constraints that P1 of the norm of theta is phase number one because P1 is so much bigger now. The normal can be smaller. And so, what this means is that by choosing the decision boundary shown on the right instead of on the left, the SVM can make the norm of the parameters theta much smaller. So, if we can make the norm of theta smaller and therefore make the squared norm of theta smaller, which is why the SVM would choose this hypothesis on the right instead. And this is how the SVM gives rise to this large margin certification effect.</strong> </p>
<p>Mainly, if you look at this green line, if you look at this green hypothesis we want the projections of my positive and negative examples onto theta to be large, and the only way for that to hold true this is if surrounding the green line. There’s this large margin, there’s this large gap that separates positive and negative examples is really the magnitude of this gap. The magnitude of this margin is exactly the values of P1, P2, P3 and so on. And so by making the margin large, by these tyros P1, P2, P3 and so on. </p>
<p><strong>that’s the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective. And this is why this machine ends up with enlarge margin classifiers because it’s trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/25.png" alt="img"></p>
<p>Finally, we did this whole derivation using this simplification that the parameter theta 0 must be equal to 0. The effect of that as I mentioned briefly, is that if theta 0 is equal to 0 what that means is that we are entertaining decision boundaries that pass through the origins of decision boundaries pass through the origin like that, if you allow theta zero to be non 0 then what that means is that you entertain the decision boundaries that did not cross through the origin, like that one I just drew. And I’m not going to do the full derivation that. It turns out that this same large margin proof works in pretty much in exactly the same way. And there’s a generalization of this argument that we just went through them long ago through that shows that even when theta 0 is non 0, what the SVM is trying to do when you have this optimization objective. Which again corresponds to the case of when C is very large. But it is possible to show that, you know, when theta is not equal to 0 this support vector machine is still finding is really trying to find the large margin separator that between the positive and negative examples. So that explains how this support vector machine is a large margin classifier. In the next video we will start to talk about how to take some of these SVM ideas and start to apply them to build a complex nonlinear classifiers.</p>
<h3 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h3><p>Vector Inner Product<br>Say we have two vectors, u and v:<br>$$\begin{align<em>} u = \begin{bmatrix} u_1 \newline u_2 \end{bmatrix} &amp; v = \begin{bmatrix} v_1 \newline v_2 \end{bmatrix} \end{align</em>}$$<br><strong>The length of vector</strong> v is denoted $||v||$, and it describes the line on a graph from origin (0,0) to $(v_1,v_2)$.<br>The length of vector v can be calculated with $\sqrt{v_1^2 + v_2^2}$ by the Pythagorean theorem.<br><strong>The projection of vector</strong> v onto vector u is found by taking a right angle from u to the end of v, creating a right triangle.<br>p= length of projection of v onto the vector u.<br>$$u^Tv= p \cdot ||u||$$<br>Note that $u^Tv = ||u|| \cdot ||v|| \cos \theta$ where θ is the angle between u and v. Also, $p = ||v|| \cos \theta$. If you substitute p for $||v|| \cos \theta$, you get $u^Tv= p \cdot ||u||$.<br>So the product $u^Tv$ is equal to the length of the projection times the length of vector u.<br>In our example, since u and v are vectors of the same length, $u^Tv = v^Tu$.<br>$$u^Tv = v^Tu = p \cdot ||u|| = u_1v_1 + u_2v_2$$<br>If the <strong>angle</strong> between the lines for v and u is <strong>greater than 90 degrees</strong> , then the projection p will be <strong>negative</strong> .<br>$$\begin{align<em>}&amp;\min_\Theta \dfrac{1}{2}\sum_{j=1}^n \Theta_j^2 \newline&amp;= \dfrac{1}{2}(\Theta_1^2 + \Theta_2^2 + \dots + \Theta_n^2) \newline&amp;= \dfrac{1}{2}(\sqrt{\Theta_1^2 + \Theta_2^2 + \dots + \Theta_n^2})^2 \newline&amp;= \dfrac{1}{2}||\Theta ||^2 \newline\end{align</em>}$$<br>We can use the same rules to rewrite $\Theta^Tx^{(i)}$:<br>$$\Theta^Tx^{(i)} = p^{(i)} \cdot ||\Theta || = \Theta_1x_1^{(i)} + \Theta_2x_2^{(i)} + \dots + \Theta_n x_n^{(i)}$$<br>So we now have a new optimization objective by substituting $p^{(i)} \cdot ||\Theta ||$ in for $\Theta^Tx^{(i)}$:<br>If y=1, we want $p^{(i)} \cdot ||\Theta || \geq 1$<br>If y=0, we want $p^{(i)} \cdot ||\Theta || \leq -1$<br>The reason this causes a “large margin” is because: the vector for Θ is perpendicular to the decision boundary. In order for our <strong>optimization objective</strong> (above) to hold true, we need the absolute value of our projections $p^{(i)}$ to be as large as possible.<br>If $\Theta_0 =0$, then all our decision boundaries will intersect (0,0). If $\Theta_0 \neq 0$, the support vector machine will still find a large margin for the decision boundary. </p>
<h1 id="02-kernels"><a href="#02-kernels" class="headerlink" title="02_kernels"></a>02_kernels</h1><h2 id="01-kernels-i"><a href="#01-kernels-i" class="headerlink" title="01_kernels-i"></a>01_kernels-i</h2><p>In this video, I’d like to start adapting support vector machines in order to develop complex nonlinear classifiers. The main technique for doing that is something called kernels. Let’s see what this kernels are and how to use them.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/26.png" alt="img"></p>
<p>If you have a training set that looks like this, and you want to find a nonlinear decision boundary to distinguish the positive and negative examples, maybe a decision boundary that looks like that. One way to do so is to come up with a set of complex polynomial features, right? So, set of features that looks like this, so that you end up with a hypothesis X that predicts 1 if you know that theta 0 and plus theta 1 X1 plus dot dot dot all those polynomial features is greater than 0, and predict 0, otherwise.<br>$$<br> h(\theta)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots<br>$$<br> And another way of writing this, to introduce a level of new notation that I’ll use later, is that we can think of a hypothesis as computing a decision boundary using this. So, theta 0 plus theta 1 f1 plus theta 2, f2 plus theta 3, f3 plus and so on.<br>$$<br>  h(\theta)=\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\theta_4f_4+\cdots<br>$$<br> Where I’m going to use this new denotation f1, f2, f3 and so on to denote these new sort of features that I’m computing, so f1 is just X1, f2 is equal to X2, f3 is equal to this one here.<br>$$<br> f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,f_5=x_2^2,\cdots<br>$$<br>we seen previously that coming up with these high order polynomials is one way to come up with lots more features, <strong>the question is, is there a different choice of features or is there better sort of features than this high order polynomials because you know it’s not clear that this high order polynomial is what we want, and what we talked about computer vision talk about when the input is an image with lots of pixels. We also saw how using high order polynomials becomes very computationally expensive because there are a lot of these higher order polynomial terms. So, is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/27.png" alt="img"></p>
<p>So, here is one idea for how to define new features f1, f2, f3. On this line I am going to define only three new features, but for real problems we can get to define a much larger number. But here’s what I’m going to do in this phase of features X1, X2, and I’m going to leave X0 out of this, the interceptor X0, but in this phase X1 X2, I’m going to just, you know, manually pick a few points, and then call these points l1, we are going to pick a different point, let’s call that l2 and let’s pick the third one and call this one l3, and for now let’s just say that I’m going to choose these three points manually. I’m going to call these three points <strong>landmarks</strong>, so line landmark one, two, three. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/28.png" alt="img"></p>
<p><strong>What I’m going to do is define my new features as follows, given an example X, let me define my first feature f1 to be some measure of the similarity between my training example X and my first landmark and this specific formula that I’m going to use to measure similarity is going to  be this $f_i = similarity(x^{(1)}, l^{(1)}) = e^{(-\frac{||x^{(1)}-l^{(1)}||^2}{2δ^2})}$.</strong> So, depending on whether or not you watched the previous optional video, this notation, you know, this is the length of the vector W (=$x^{(1)}-l^{(1)}$). And so, this thing here, this $||x-l^{(1)}||^2$, this is actually just the <strong>euclidean distance squared</strong>, is the euclidean distance between the point x and the landmark l1. We will see more about this later. But that’s my first feature, and my second feature f2 is going to be, you know, similarity function that measures how similar X is to l2 and the game is going to be defined as the following function. This is E to the minus of the square of the euclidean distance between X and the second landmark, that is what the enumerator is and then divided by 2 sigma squared and similarly f3 is, you know, similarity between X and l3, which is equal to, again, similar formula. And what this similarity function is, the mathematical term for this, is that this is going to be a kernel function. And the specific kernel I’m using here, this is actually called a Gaussian kernel. And so this formula, this particular choice of similarity function is called a <strong>Gaussian kernel</strong>. </p>
<p>But the way the terminology goes is that, you know, in the abstract these different similarity functions are called <strong>kernels</strong> and we can have different similarity functions and the specific example I’m giving here is called the Gaussian kernel. We’ll see other examples of other kernels. But for now just think of these as similarity functions. And so, instead of writing similarity between X and l, sometimes we also write this a kernel denoted you know, lower case k between x and one of my landmarks all right. <strong>So let’s see what this kernel actually do and why these sorts of similarity functions, why these expressions might make sense.</strong><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/29.png" alt=""></p>
<p>So let’s take my first landmark. My landmark l1, which is one of those points I chose on my figure just now. So the similarity of the kernel between x and l1 is given by this expression. Just to make sure, you know, we are on the same page about what the numerator term is, the numerator can also be written as a sum from j equals 1 through N on sort of the distance. So this is the component wise distance between the vector X and the vector l. And again for the purpose of these slides I’m ignoring X0. So just ignoring the intercept term X0, which is always equal to 1. So, you know, this is how you compute the kernel with similarity between X and a landmark. So let’s see what this function does. Suppose X is close to one of the landmarks. Then this euclidean distance formula and the numerator will be close to 0, right. So, that is this term here, the distance was great, the distance using X and 0 will be close to zero, and so f1, this is a simple feature, will be approximately E to the minus 0 and then the numerator squared over 2 is equal to squared so that E to the 0, E to minus 0, E to 0 is going to be close to one. And I’ll put the approximation symbol here because the distance may not be exactly 0, but if X is closer to landmark this term will be close to 0 and so f1 would be close 1. Conversely, if X is far from 01 then this first feature f1 will be E to the minus of some large number squared, divided divided by two sigma squared and E to the minus of a large number is going to be close to 0. <strong>So what these features do is they measure how similar X is from one of your landmarks and the feature f is going to be close to one when X is close to your landmark and is going to be 0 or close to zero when X is far from your landmark.</strong> </p>
<p>Each of these landmarks. On the previous line, I drew three landmarks, l1, l2, l3. <strong>Each of these landmarks, defines a new feature f1, f2 and f3. That is, given the the training example X, we can now compute three new features: f1, f2, and f3, given, you know, the three landmarks that I wrote just now.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/30.png" alt="img"></p>
<p>But first, let’s look at this exponentiation function, let’s look at this similarity function and plot in some figures and just, you know, understand better what this really looks like. For this example, let’s say I have two features X1 and X2. And let’s say my first landmark, l1 is at a location, $\begin{bmatrix}3\5\end{bmatrix}$. So and let’s say I set sigma squared equals one for now. If I plot what this feature looks like, what I get is this figure. So the vertical axis, the height of the surface is the value of f1 and down here on the horizontal axis are, if I have some training example, and there is x1 and there is x2. Given a certain training example, the training example here which shows the value of x1 and x2 at a height above the surface, shows the corresponding value of f1 and down below this is the same figure I had showed, using a quantifiable plot, with x1 on horizontal axis, x2 on horizontal axis and so, this figure on the bottom is just a contour plot of the 3D surface. You notice that when X is equal to 3 5 exactly, then we the f1 takes on the value 1, because that’s at the maximum and X moves away as X goes further away then this feature takes on values that are close to 0. And so, this is really a feature, f1 measures, you know, how close X is to the first landmark and if varies between 0 and one depending on how close X is to the first landmark l1. Now the other was due on this slide is show the effects of varying this parameter sigma squared. So, sigma squared is the parameter of the Gaussian kernel and as you vary it, you get slightly different effects. Let’s set sigma squared to be equal to 0.5 and see what we get. We set sigma square to 0.5, what you find is that the kernel looks similar, except for the width of the bump becomes narrower. The contours shrink a bit too. So if sigma squared equals to 0.5 then as you start from X equals $\begin{bmatrix}3\5\end{bmatrix}$ and as you move away, then the feature f1 falls to zero much more rapidly and conversely, if you has increase since where three in that case and as I move away from, you know l. So this point here is really l, right, that’s l1 is at location $\begin{bmatrix}3\5\end{bmatrix}$,  right. So it’s shown up here. And if sigma squared is large, then as you move away from l1, the value of the feature falls away much more slowly. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/31.png" alt="img"></p>
<p>So, given this definition of the features, let’s see what source of hypothesis we can learn. Given the training example X, we are going to compute these features f1, f2, f3 and a hypothesis is going to predict one when theta 0 plus theta 1 f1 plus theta 2 f2, and so on is greater than or equal to 0. For this particular example, let’s say that I’ve already found a learning algorithm and let’s say that, you know, somehow I ended up with these values of the parameter. So if theta 0 equals minus 0.5, theta 1 equals 1, theta 2 equals 1, and theta 3 equals 0 And what I want to do is consider what happens if we have a training example that takes has location at <strong><em>this magenta dot</em></strong>, right where I just drew this dot over here. So let’s say I have a training example X, what would my hypothesis predict? Well, If I look at this formula. Because my training example X is close to l1, we have that f1 is going to be close to 1 the because my training example X is far from l2 and l3 I have that, you know, f2 would be close to 0 and f3 will be close to 0. So, if I look at that formula, I have theta 0 plus theta 1 times 1 plus theta 2 times some value. Not exactly 0, but let’s say close to 0. Then plus theta 3 times something close to 0. And this is going to be equal to plugging in these values now. So, that gives minus 0.5 plus 1 times 1 which is 1, and so on. Which is equal to 0.5 which is greater than or equal to 0. So, at this point, we’re going to predict Y equals 1, because that’s greater than or equal to zero.<br>$$<br>h_θ(x) = θ_0+θ_1f_1+θ_2f_2+θ_3f_3=-0.5+1<em>1+1</em>0+0<em>0=0.5≥0<br>$$<br>Now let’s take a different point. Now lets’ say I take a different point, I’m going to draw this one in a different color, **</em>in cyan*** say, for a point out there, if that were my training example X, then if you make a similar computation, you find that f1, f2, f3 are all going to be close to 0. And so, we have theta 0 plus theta1, f1, plus so on and this will be about equal to minus 0.5, because theta 0 is minus 0.5 and f1, f2, f3 are all zero. So this will be minus 0.5, this is less than zero. And so, at this point out there, we’re going to predict Y equals zero.<br>$$<br>h_θ(x) = θ_0+θ_1f_1+θ_2f_2+θ_3f_3=-0.5+1<em>0+1</em>0+0<em>0=-0.5&lt;0<br>$$<br>And if you do this yourself for a range of different points, be sure to convince yourself that if you have a training example that’s close to L2, say, then at this point we’ll also predict Y equals one. And in fact, what you end up doing is, you know, if you look around this boundary, this space, what we’ll find is that for points near l1 and l2 we end up predicting positive. And for points far away from l1 and l2, that’s for points far away from these two landmarks, we end up predicting that the class is equal to 0. As so, what we end up doing,is that the decision boundary of this hypothesis would end up looking something like this where inside this red decision boundary would predict Y equals 1 and outside we predict Y equals 0. And so this is how with this definition of the landmarks and of the kernel function. We can learn pretty complex non-linear decision boundary, like what I just drew where we predict positive when we’re close to either one of the two landmarks. And we predict negative when we’re very far away from any of the landmarks. And so this is part of the idea of kernels of and how we use them with the support vector machine, *</em>which is that we define these extra features using landmarks and similarity functions to learn more complex nonlinear classifiers. So hopefully that gives you a sense of the idea of kernels and how we could use it to define new features for the Support Vector Machine. **</p>
<p><strong><em>But there are a couple of questions that we haven’t answered yet. One is, how do we get these landmarks? How do we choose these landmarks? And another is, what other similarity functions, if any, can we use other than the one we talked about, which is called the Gaussian kernel</em></strong>. In the next video we give answers to these questions and put everything together to show how support vector machines with kernels can be a powerful way to learn complex nonlinear functions.</p>
<h3 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h3><p>Kernels allow us to make complex, non-linear classifiers using Support Vector Machines.<br>Given x, compute new feature depending on proximity to landmarks $l^{(1)},\ l^{(2)},\ l^{(3)}$.<br>To do this, we find the “similarity” of x and some landmark $l^{(i)}$:<br>$$f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{||x - l^{(i)}||^2}{2\sigma^2})$$<br>This “similarity” function is called a <strong>Gaussian Kernel</strong> . It is a specific example of a kernel.<br>The similarity function can also be written as follows:<br>$$f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\sigma^2})$$<br>There are a couple properties of the similarity function:<br>If $x \approx l^{(i)}$, then $f_i = \exp(-\dfrac{\approx 0^2}{2\sigma^2}) \approx 1$<br>If x is far from $l^{(i)}$, then $f_i = \exp(-\dfrac{(large\ number)^2}{2\sigma^2}) \approx 0$<br>In other words, if x and the landmark are close, then the similarity will be close to 1, and if x and the landmark are far away from each other, the similarity will be close to 0.<br>Each landmark gives us the features in our hypothesis:<br>$$\begin{align<em>}l^{(1)} \rightarrow f_1 \newline l^{(2)} \rightarrow f_2 \newline l^{(3)} \rightarrow f_3 \newline\dots \newline h_\Theta(x) = \Theta_1f_1 + \Theta_2f_2 + \Theta_3f_3 + \dots\end{align</em>}$$<br>$\sigma^2$ is a parameter of the Gaussian Kernel, and it can be modified to increase or decrease the <strong>drop-off</strong> of our feature $f_i$. Combined with looking at the values inside Θ, we can choose these landmarks to get the general shape of the decision boundary. </p>
<h2 id="02-kernels-ii"><a href="#02-kernels-ii" class="headerlink" title="02_kernels-ii"></a>02_kernels-ii</h2><p>In the last video, <strong>we started to talk about the kernels idea and how it can be used to define new features for the support vector machine. In this video, I’d like to throw in some of the missing details and, also, say a few words about how to use these ideas in practice. Such as, how they pertain to, for example, the bias variance trade-off in support vector machines.</strong> </p>
<p>In the last video, I talked about the process of picking a few landmarks. You know, l1, l2, l3 and that allowed us to define the similarity function also called the kernel or in this example if you have this similarity function this is a Gaussian kernel. And that allowed us to build this form of a hypothesis function. </p>
<h3 id="where-do-we-get-these-landmarks-from"><a href="#where-do-we-get-these-landmarks-from" class="headerlink" title="where do we get these landmarks from"></a>where do we get these landmarks from</h3><p>Where do we get l1, l2, l3 from? And it seems, also, that for complex learning problems, maybe we want a lot more landmarks than just three of them that we might choose by hand. So in practice this is how the landmarks are chosen which is that given the machine learning problem. We have some data set of some some positive and negative examples. So, this is the idea here which is that we’re gonna take the examples and for every training example that we have, we are just going to call it. We’re just going to put landmarks as exactly the same locations as the training examples. So if I have one training example if that is x1, well then I’m going to choose this is my first landmark to be at exactly the same location as my first training example. And if I have a different training example x2. Well we’re going to set the second landmark to be the location of my second training example. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/32.png" alt="choosing_the_landmarks"></p>
<p>On the figure on the right, I used red and blue dots just as illustration, the color of this figure, the color of the dots on the figure on the right is not significant. But what I’m going to end up with using this method is I’m going to end up with m landmarks of l1, l2 down to l(m) if I have m training examples with one landmark per location of my per location of each of my training examples. And this is nice because it is saying that my features are basically going to measure how close an example is to one of the things I saw in my training set. So, just to write this outline a little more concretely, given m training examples, I’m going to choose the the location of my landmarks to be exactly near the locations of my m training examples. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/33.png" alt="img"></p>
<p>When you are given example x, and in this example x can be something in the training set, it can be something in the cross validation set, or it can be something in the test set. Given an example x we are going to compute, you know, these features as so f1, f2, and so on. Where l1 is actually equal to x1 and so on. And these then give me a feature vector. So let me write f as the feature vector. I’m going to take these f1, f2 and so on, and just group them into feature vector. Take those down to fm. And, you know, just by convention. If we want, we can add an extra feature f0, which is always equal to 1. So this plays a role similar to what we had previously. For x0, which was our interceptor. So, for example, if we have a training example x(i), y(i), the features we would compute for this training example will be as follows: given x(i), we will then map it to, you know, f1(i). Which is the similarity. I’m going to abbreviate as SIM instead of writing out the whole word similarity, right? And f2(i) equals the similarity between x(i) and l2, and so on, down to fm(i) equals the similarity between x(i) and l(m). And somewhere in the middle. Somewhere in this list, you know, at the i-th component, I will actually have one feature component which is f subscript i(i), which is going to be the similarity between x and l(i). <strong>Where l(i) is equal to x(i), and so you know f(i) is just going to be the similarity between x and itself.</strong> And if you’re using the Gaussian kernel this is actually e to the minus 0 over 2 sigma squared and so, this will be equal to 1 and that’s okay. So one of my features for this training example is going to be equal to 1. And then similar to what I have above. I can take all of these m features and group them into a feature vector. So instead of representing my example, using, you know, x(i) which is this what R(n) plus one dimensional vector. Depending on whether you can set terms, is either R(n) or R(n) plus 1. We can now instead represent my training example using this feature vector f. I am going to write this f superscript i.  Which is going to be taking all of these things and stacking them into a vector. So, $f_1^{(i)}$ down to $f_m^{(i)}$ and if you want and well, usually we’ll also add this $f_0^{(i)}$ , where $f_0^{(i)}$ is equal to 1.</p>
<p>And so this vector here gives me my new feature vector with which to represent my training example. So given these kernels and similarity functions, here’s how we use a simple vector machine. <strong>If you already have a learning set of parameters theta, then if you given a value of x and you want to make a prediction. What we do is we compute the features f, which is now an $R^{m}$ plus 1 dimensional feature vector. And we have m here because we have m training examples and thus m landmarks and what we do is we predict 1 if theta transpose f is greater than or equal to 0. Right.</strong> </p>
<p>So, if theta transpose f, of course, that’s just equal to theta 0, f0 plus theta 1, f1 plus dot dot dot, plus theta m f(m). And so my parameter vector theta is also now going to be an m plus 1 dimensional vector. <strong>And we have m here because where the number of landmarks is equal to the training set size. So m was the training set size and now, the parameter vector theta is going to be m plus one dimensional. So that’s how you make a prediction if you already have a setting for the parameter’s theta.</strong> </p>
<p><strong>How do you get the parameter’s theta?</strong> Well you do that using the SVM learning algorithm, and specifically what you do is you would solve this minimization problem. You’ve minimized the parameter’s theta of C times this cost function which we had before. Only now, instead of looking there instead of making predictions using theta transpose x(i) using our original features, x(i). <strong>Instead we’ve taken the features x(i) and replace them with a new features so we are using theta transpose f(i) to make a prediction on the i’th training examples and we see that, you know, in both places here and it’s by solving this minimization problem that you get the parameters for your Support Vector Machine.</strong></p>
<p>$$<br>minJ(θ)=min C[\sum_{i=1}^{m} y^{(i)}Cost_1(θ^Tf^{(i)}) + (1-y^{(i)})Cost_0(θ^Tf^{(i)})  ] + \frac{1}{2}\sum_{j=1}^{n}θ_j^2<br>$$<br>And one last detail is because this optimization problem we really have n equals m features. That is here. The number of features we have. <strong>Really, the effective number of features we have is dimension of f.</strong> So that n is actually going to be equal to m. So, if you want to, you can think of this as a sum, this really is a sum from j equals 1 through m. And then one way to think about this, is you can think of it as n being equal to m, because if f isn’t a new feature, then we have m plus 1 features, with the plus 1 coming from the interceptor. And here, we still do sum from j equal 1 through n, because similar to our earlier videos on regularization, we still do not regularize the parameter theta zero, which is why this is a sum for j equals 1 through m instead of j equals zero though m.  So that’s the support vector machine learning algorithm. That’s one sort of, mathematical detail aside that I should mention, which is that in the way the support vector machine is implemented, this last term is actually done a little bit differently. So you don’t really need to know about this last detail in order to use support vector machines, and in fact the equations that are written down here should give you all the intuitions that should need. But in the way the support vector machine is implemented, you know, that term, the sum of j of theta j squared right? Another way to write this is this can be written as theta transpose theta if we ignore the parameter theta 0. So theta 1 down to theta m.  Ignoring theta 0. Then this sum of j of theta j squared that this can also be written theta transpose theta. </p>
<h3 id="SVM-thetas"><a href="#SVM-thetas" class="headerlink" title="SVM thetas"></a>SVM thetas</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/34.png" alt="SVM_with_Kernels"></p>
<p>And what most support vector machine implementations do is actually replace this theta transpose theta, will instead, theta transpose times some matrix inside, that depends on the kernel you use, times theta. And so this gives us a slightly different distance metric. We’ll use a slightly different measure instead of minimizing exactly the norm of theta squared means that minimize something slightly similar to it. That’s like a rescale version of the parameter vector theta that depends on the kernel. But this is kind of a mathematical detail. That allows the support vector machine software to run much more efficiently. And the reason the support vector machine does this is with this modification. It allows it to scale to much bigger training sets. </p>
<p>Because for example, if you have a training set with 10,000 training examples. Then, you know, the way we define landmarks, we end up with 10,000 landmarks. And so theta becomes 10,000 dimensional. And maybe that works, <strong>but when m becomes really, really big then solving for all of these parameters, you know, if m were 50,000 or a 100,000 then solving for all of these parameters can become expensive for the support vector machine optimization software</strong>, thus solving the minimization problem that I drew here. So kind of as mathematical detail, which again you really don’t need to know about. It actually modifies that last term a little bit to optimize something slightly different than just minimizing the norm squared of theta squared, of theta. But if you want, you can feel free to think of this as an kind of an implementational detail that does change the objective a bit, but is done primarily for reasons of computational efficiency, so usually you don’t really have to worry about this. </p>
<p>*<em>And by the way, in case your wondering why we don’t apply the kernel’s idea to other algorithms as well like logistic regression, it turns out that if you want, you can actually apply the kernel’s idea and define the source of features using landmarks and so on for logistic regression. But the computational tricks that apply for support vector machines don’t generalize well to other algorithms like logistic regression. And so, using kernels with logistic regression is going too very slow, whereas, because of computational tricks, like that embodied and how it modifies this and the details of how the support vector machine software is implemented, support vector machines and kernels tend go particularly well together. Whereas, logistic regression and kernels, you know, you can do it, but this would run very slowly. And it won’t be able to take advantage of advanced optimization techniques that people have figured out for the particular case of running a support vector machine with a kernel.But all this pertains only to how you actually implement software to minimize the cost function. *</em></p>
<p>I will say more about that in the next video, but you really don’t need to know about how to write software to minimize this  cost function because you can find very good off the shelf software for doing so. And just as, you know, I wouldn’t recommend writing code to invert a matrix or to compute a square root, I actually do not recommend writing software to minimize this cost function yourself, but instead to use off the shelf software packages that people have developed and so those software packages already embody these numerical optimization tricks, so you don’t really have to worry about them. </p>
<h3 id="SVM-regularization-parameters"><a href="#SVM-regularization-parameters" class="headerlink" title="SVM regularization parameters"></a>SVM regularization parameters</h3><p><strong>But one other thing that is worth knowing about is when you’re applying a support vector machine, how do you choose the parameters of the support vector machine?</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/35.png" alt="SVM_regularization_parameters"></p>
<p>And the last thing I want to do in this video is say a little word about the bias and variance trade offs when using a support vector machine. When using an SVM, one of the things you need to choose is the parameter C which was in the optimization objective, and you recall that C played a role similar to 1 over lambda, where lambda was the regularization parameter we had for logistic regression. So, if you have a large value of C, this corresponds to what we have back in logistic regression, of a small value of lambda meaning of not using much regularization. And if you do that, you tend to have a hypothesis with lower bias and higher variance. Whereas if you use a smaller value of C then this corresponds to when we are using logistic regression with a large value of lambda and that corresponds to a hypothesis with higher bias and lower variance. And so, hypothesis with large C has a higher variance, and is more prone to overfitting, whereas hypothesis with small C has higher bias and is thus more prone to underfitting. So this parameter C is one of the parameters we need to choose. The other one is the parameter sigma squared, which appeared in the Gaussian kernel. So if the Gaussian kernel sigma squared is large, then in the similarity function, which was this you know E to the minus x minus landmark varies squared over 2 sigma squared. In this one of the example; If I have only one feature, x1, if I have a landmark there at that location, if sigma squared is large, then, you know, the Gaussian kernel would tend to fall off relatively slowly and so this would be my feature f(i), and so this would be smoother function that varies more smoothly, and so this will give you a hypothesis with higher bias and lower variance, because the Gaussian kernel that falls off smoothly, you tend to get a hypothesis that varies slowly, or varies smoothly as you change the input x. Whereas in contrast, if sigma squared was small and if that’s my landmark given my 1 feature x1, you know, my Gaussian kernel, my similarity function, will vary more abruptly. And in both cases I’d pick out 1, and so if sigma squared is small, then my features vary less smoothly. So if it’s just higher slopes or higher derivatives here. And using this, you end up fitting hypotheses of lower bias and you can have higher variance. And if you look at this week’s points exercise, you actually get to play around with some of these ideas yourself and see these effects yourself. So, that was the support vector machine with kernels algorithm. And hopefully this discussion of bias and variance will give you some sense of how you can expect this algorithm to behave as well.</p>
<h3 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h3><p>One way to get the landmarks is to put them in the <strong>exact same</strong> locations as all the training examples. This gives us m landmarks, with one landmark per training example.<br>Given example x:<br>$f_1 = similarity(x,l^{(1)}), f_2 = similarity(x,l^{(2)}), f_3 = similarity(x,l^{(3)})$, and so on.<br>This gives us a “feature vector,” $f_{(i)}$ of all our features for example $x_{(i)}$. We may also set $f_0 = 1$ to correspond with $Θ<em>0$. Thus given training example $x</em>{(i)}$:<br>$$x^{(i)} \rightarrow \begin{bmatrix}f_1^{(i)} = similarity(x^{(i)}, l^{(1)}) \ f_2^{(i)} = similarity(x^{(i)}, l^{(2)}) \ \vdots \ f_m^{(i)} = similarity(x^{(i)}, l^{(m)}) \ \end{bmatrix}$$<br>Now to get the parameters Θ we can use the SVM minimization algorithm but with $f^{(i)}$ substituted in for $x^{(i)}$:<br>$$\min_{\Theta} C \sum_{i=1}^m y^{(i)}\text{cost}<em>1(\Theta^Tf^{(i)}) + (1 - y^{(i)})\text{cost}_0(\theta^Tf^{(i)}) + \dfrac{1}{2}\sum</em>{j=1}^n \Theta^2_j$$<br>Using kernels to generate f(i) is not exclusive to SVMs and may also be applied to logistic regression. However, because of computational optimizations on SVMs, kernels combined with SVMs is much faster than with other algorithms, so kernels are almost always found combined only with SVMs.<br><strong>Choosing SVM Parameters</strong><br>Choosing C (recall that $C = \dfrac{1}{\lambda}$<br>If C is large, then we get higher variance/lower bias<br>If C is small, then we get lower variance/higher bias<br>The other parameter we must choose is $σ^2$ from the Gaussian Kernel function:<br>With a large $σ^2$, the features fi vary more smoothly, causing higher bias and lower variance.<br>With a small $σ^2$, the features fi vary less smoothly, causing lower bias and higher variance.<br><strong>Using An SVM</strong><br>There are lots of good SVM libraries already written. A. Ng often uses ‘liblinear’ and ‘libsvm’. In practical application, you should use one of these libraries rather than rewrite the functions.<br>In practical application, the choices you do need to make are:<br>Choice of parameter C<br>Choice of kernel (similarity function)<br>No kernel (“linear” kernel) – gives standard linear classifier<br>Choose when n is large and when m is small<br>Gaussian Kernel (above) – need to choose $σ^2$<br>Choose when n is small and m is large<br>The library may ask you to provide the kernel function.<br><strong>Note</strong>: do perform feature scaling before using the Gaussian Kernel.<br><strong>Note</strong>: not all similarity functions are valid kernels. They must satisfy “Mercer’s Theorem” which guarantees that the SVM package’s optimizations run correctly and do not diverge.<br>You want to train C and the parameters for the kernel function using the training and cross-validation datasets.<br><strong>Multi-class Classification</strong><br>Many SVM libraries have multi-class classification built-in.<br>You can use the one-vs-all method just like we did for logistic regression, where $$y \in {1,2,3,\dots,K}$$ with $$\Theta^{(1)}, \Theta^{(2)}, \dots,\Theta{(K)}$$. We pick class i with the largest $$(\Theta^{(i)})^Tx$$.<br><strong>Logistic Regression vs. SVMs</strong><br>If n is large (relative to m), then use logistic regression, or SVM without a kernel (the “linear kernel”)<br>If n is small and m is intermediate, then use SVM with a Gaussian Kernel<br>If n is small and m is large, then manually create/add more features, then use logistic regression or SVM without a kernel.<br>In the first case, we don’t have enough examples to need a complicated polynomial hypothesis. In the second example, we have enough examples that we may need a complex non-linear hypothesis. In the last case, we want to increase our features so that logistic regression becomes applicable.<br>Note : a neural network is likely to work well for any of these situations, but may be slower to train.<br><strong>Additional references</strong><br>“An Idiot’s Guide to Support Vector Machines”: <a href="http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf" target="_blank" rel="noopener">http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf</a> </p>
<h1 id="03-svms-in-practice"><a href="#03-svms-in-practice" class="headerlink" title="03_svms-in-practice"></a>03_svms-in-practice</h1><p>So far we’ve been talking about SVMs in a fairly abstract level. In this video I’d like to talk about what you actually need to do in order to run or to use an SVM. </p>
<p>The support vector machine algorithm poses a particular optimization problem. But as I briefly mentioned in an earlier video, I really do not recommend writing your own software to solve for the parameter’s theta yourself. So just as today, very few of us, or maybe almost essentially none of us would think of writing code ourselves to invert a matrix or take a square root of a number, and so on. We just, you know, call some library function to do that. In the same way, the software for solving the SVM optimization problem is very complex, and there have been researchers that have been doing essentially numerical optimization research for many years. So you come up with good software libraries and good software packages to do this. And then strongly recommend just using one of the highly optimized software libraries rather than trying to implement something yourself. And there are lots of good software libraries out there. The two that I happen to use the most often are the linear SVM but there are really lots of good software libraries for doing this that you know, you can link to many of the major programming languages that you may be using to code up learning algorithm. </p>
<p>Even though you shouldn’t be writing your own SVM optimization software, there are a few things you need to do, though. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/36.png" alt="Use_SVM_software_package"></p>
<h2 id="linear-kernel"><a href="#linear-kernel" class="headerlink" title="linear kernel"></a>linear kernel</h2><p><strong>First is to come up with with some choice of the parameter’s C</strong>. We talked a little bit of the bias/variance properties of this in the earlier video. <strong>Second, you also need to choose the kernel or the similarity function that you want to use.</strong> So one choice might be if we decide not to use any kernel. And the idea of no kernel is also called <strong><em>a linear kernel</em></strong>. So if someone says, I use an SVM with a linear kernel, what that means is you know, they use an SVM without using without using a kernel and it was a version of the SVM that just uses theta transpose X, right, that predicts 1 theta 0 plus theta 1 X1 plus so on plus theta N, X N is greater than equals 0. This term linear kernel, you can think of this as you know this is the version of the SVM that just gives you a standard linear classifier. So that would be one reasonable choice for some problems, and you know, there would be many software libraries, like linear, was one example, out of many, one example of a software library that can train an SVM without using a kernel, also called a linear kernel. <strong>So, why would you want to do this? If you have a large number of features, if N is large, and M the number of training examples is small, then you know you have a huge number of features that if X, this is an X is an Rn, Rn +1. So if you have a huge number of features already, with a small training set, you know, maybe you want to just fit a linear decision boundary and not try to fit a very complicated nonlinear function, because might not have enough data. And you might risk overfitting, if you’re trying to fit a very complicated function in a very high dimensional feature space, but if your training set sample is small. So this would be one reasonable setting where you might decide to just not use a kernel, or equivalents to use what’s called a linear kernel.</strong> </p>
<h2 id="Gaussian-kernel"><a href="#Gaussian-kernel" class="headerlink" title="Gaussian kernel"></a>Gaussian kernel</h2><p>A second choice for the kernel that you might make, is this <strong><em>Gaussian kernel</em></strong>, and this is what we had previously. And if you do this, then the other choice you need to make is to choose this parameter sigma squared when we also talk a little bit about the bias variance tradeoffs of how, if sigma squared is large, then you tend to have a higher bias, lower variance classifier, but if sigma squared is small, then you have a higher variance, lower bias classifier. <strong>So when would you choose a Gaussian kernel? Well, if your omission of features X, I mean Rn, and if N is small, and, ideally, you know, if n is large, right, so that’s if, you know, we have say, a two-dimensional training set, like the example I drew earlier. So n is equal to 2, but we have a pretty large training set. So, you know, I’ve drawn in a fairly large number of training examples, then maybe you want to use a kernel to fit a more complex nonlinear decision boundary, and the Gaussian kernel would be a fine way to do this.</strong> </p>
<p>I’ll say more towards the end of the video, a little bit more about when you might choose a linear kernel, a Gaussian kernel and so on. <strong>But if concretely, if you decide to use a Gaussian kernel, then here’s what you need to do. Depending on what support vector machine software package you use, it may ask you to implement a kernel function, or to implement the similarity function.</strong> </p>
<p>So if you’re using an octave or MATLAB implementation of an SVM, it may ask you to provide a function to compute a particular feature of the kernel. So this is really computing f subscript i for one particular value of i, where f here is just a single real number, so maybe I should move this better written f(i), but what you need to do is to write a kernel function that takes this input, you know, a training example or a test example whatever it takes in some vector X and takes as input one of the landmarks and but only I’ve come down X1 and X2 here, because the landmarks are really training examples as well. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/37.png" alt="gaussian_kernel"></p>
<p>But what you need to do is write software that takes this input, you know, X1, X2 and computes this sort of similarity function between them and return a real number. And so what some support vector machine packages do is expect you to provide this kernel function that take this input you know, X1, X2 and returns a real number. And then it will take it from there and it will automatically generate all the features, and so automatically take X and map it to f1, f2, down to f(m) using this function that you write, and generate all the features and train the support vector machine from there. But sometimes you do need to provide this function yourself. Other if you are using the Gaussian kernel, some SVM implementations will also include the Gaussian kernel and a few other kernels as well, since the Gaussian kernel is probably the most common kernel. Gaussian and linear kernels are really the two most popular kernels by far. Just one implementational note. <strong>If you have features of very different scales, it is important to perform feature scaling before using the Gaussian kernel</strong>. And here’s why. If you imagine the computing the norm between X and l, right, so this term here, and the numerator term over there. What this is doing, the norm between X and l, that’s really saying, you know, let’s compute the vector V, which is equal to X minus l. And then let’s compute the norm does vector V, which is the difference between X. So the norm of V is really equal to V1 squared plus V2 squared plus dot dot dot, plus Vn squared. Because here X is in Rn, or Rn plus 1, but I’m going to ignore, you know, X0. So, let’s pretend X is an Rn, square on the left side is what makes this correct. So this is equal to that, right? And so written differently, this is going to be X1 minus l1 squared, plus x2 minus l2 squared, plus dot dot dot plus Xn minus ln squared. And now if your features take on very different ranges of value. So take a housing prediction, for example, if your data is some data about houses. And if X is in the range of thousands of square feet, for the first feature, X1. But if your second feature, X2 is the number of bedrooms. So if this is in the range of one to five bedrooms, then X1 minus l1 is going to be huge. This could be like a thousand squared, whereas X2 minus l2 is going to be much smaller and if that’s the case, then in this term, those distances will be almost essentially dominated by the sizes of the houses and the number of bathrooms would be largely ignored. As so as, to avoid this in order to make a machine work well, do perform future scaling. And that will sure that the SVM gives, you know, comparable amount of attention to all of your different features, and not just to in this example to size of houses were big movement here the features. </p>
<p>When you try a support vector machines chances are by far the two most common kernels you use will be the linear kernel, meaning no kernel, or the Gaussian kernel that we talked about. <strong>And just one note of warning which is that not all similarity functions you might come up with are valid kernels. And the Gaussian kernel and the linear kernel and other kernels that you sometimes others will use, all of them need to satisfy a technical condition. It’s called Mercer’s Theorem and the reason you need to this is because support vector machine algorithms or implementations of the SVM have lots of clever numerical optimization tricks.</strong> </p>
<p><strong>In order to solve for the parameter’s theta efficiently and in the original design envisaged, those are decision made to restrict our attention only to kernels that satisfy this technical condition called Mercer’s Theorem. And what that does is, that makes sure that all of these SVM packages, all of these SVM software packages can use the large class of optimizations and get the parameter theta very quickly. So, what most people end up doing is using either the linear or Gaussian kernel, but there are a few other kernels that also satisfy Mercer’s theorem</strong> and that you may run across other people using, although I personally end up using other kernels you know, very, very rarely, if at all. </p>
<h2 id="Other-choices-of-kernel"><a href="#Other-choices-of-kernel" class="headerlink" title="Other choices of kernel"></a>Other choices of kernel</h2><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/38.png" alt="other_choices_of_kernel"></p>
<p>Just to mention some of the other kernels that you may run across. One is the <strong><em>polynomial kernel.</em></strong> And for that the similarity between X and l is defined as, there are a lot of options, you can take X transpose l squared. So, here’s one measure of how similar X and l are. If X and l are very close with each other, then the inner product will tend to be large. And so, you know, this is a slightly unusual kernel. That is not used that often, but you may run across some people using it. This is one version of a polynomial kernel. Another is X transpose l cubed. These are all examples of the polynomial kernel. X transpose l plus 1 cubed. X transpose l plus maybe a number different then one 5 and, you know, to the power of 4 and so the polynomial kernel actually has two parameters. One is, what number do you add over here? It could be 0. This is really plus 0 over there, as well as what’s the degree of the polynomial over there. So the degree power and these numbers. And the more general form of the polynomial kernel is X transpose l, plus some constant and then to some degree in the X1 and so both of these are parameters for the polynomial kernel. So the polynomial kernel almost always or usually performs worse. And the Gaussian kernel does not use that much, but this is just something that you may run across. Usually it is used only for data where X and l are all strictly non negative, and so that ensures that these inner products are never negative. And this captures the intuition that X and l are very similar to each other, then maybe the inter product between them will be large. They have some other properties as well but people tend not to use it much. </p>
<p>And then, depending on what you’re doing, there are other, sort of more esoteric kernels as well, that you may come across. You know, there’s a <strong><em>string kernel</em></strong>, this is sometimes used if your input data is text strings or other types of strings. There are things like the <strong><em>chi-square kernel</em></strong>, the <strong><em>histogram intersection kernel</em></strong>, and so on. There are sort of more esoteric kernels that you can use to measure similarity between different objects. So for example, if you’re trying to do some sort of text classification problem, where the input x is a string then maybe we want to find the similarity between two strings using the string kernel, but I personally you know end up very rarely, if at all, using these more esoteric kernels.</p>
<p>I think I might have use the chi-square kernel, may be once in my life and the histogram kernel, may be once or twice in my life. I’ve actually never used the string kernel myself. But in case you’ve run across this in other applications. You know, if you do a quick web search we do a quick Google search or quick Bing search you should have found definitions that these are the kernels as well.</p>
<h2 id="Two-last-details"><a href="#Two-last-details" class="headerlink" title="Two last details"></a>Two last details</h2><h3 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/39.png" alt="Multi-class_classification"></p>
<p>So just two last details I want to talk about in this video. One in multiclass classification. So, you have four classes or more generally 3 classes output some appropriate decision boundary between your multiple classes. Most SVM, many SVM packages already have built-in multiclass classification functionality. So if your using a pattern like that, you just use the both that functionality and that should work fine. Otherwise, one way to do this is to use the one versus all method that we talked about when we are developing logistic regression. So what you do is you trade kSVM’s if you have k classes, one to distinguish each of the classes from the rest. And this would give you k parameter vectors, so this will give you, upi lmpw. theta 1, which is trying to distinguish class y equals one from all of the other classes, then you get the second parameter, vector theta 2, which is what you get when you, you know, have y equals 2 as the positive class and all the others as negative class and so on up to a parameter vector theta k, which is the parameter vector for distinguishing the final class key from anything else, and then lastly, this is exactly the same as the one versus all method we have for logistic regression. Where we you just predict the class i with the largest theta transpose X.  So let’s multiclass classification designate. For the more common cases that there is a good chance that whatever software package you use, you know, there will be a reasonable chance that are already have built in multiclass classification functionality, and so you don’t need to worry about this result. Finally, we developed support vector machines starting off with logistic regression and then modifying the cost function a little bit. ****</p>
<h3 id="logistic-regression-vs-SVMs"><a href="#logistic-regression-vs-SVMs" class="headerlink" title="logistic regression vs. SVMs"></a>logistic regression vs. SVMs</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/12/40.png" alt="logistic_regression_VS_svms"></p>
<p>The last thing we want to do in this video is, just say a little bit about. when you will use one of these two algorithms, so let’s say n is the number of features and m is the number of training examples. So, when should we use one algorithm versus the other? Well, if n is larger relative to your training set size, so for example, if you take a business with a number of features this is much larger than m and this might be, for example, if you have a text classification problem, where you know, the dimension of the feature vector is I don’t know, maybe, 10 thousand. And if your training set size is maybe 10 you know, maybe, up to 1000. So, imagine a spam classification problem, where email spam, where you have 10,000 features corresponding to 10,000 words but you have, you know, maybe 10 training examples or maybe up to 1,000 examples. So if n is large relative to m, then what I would usually do is use logistic regression or use it as the m without a kernel or use it with a linear kernel. Because, if you have so many features with smaller training sets, you know, a linear function will probably do fine, and you don’t have really enough data to fit a very complicated nonlinear function. Now if is n is small and m is intermediate what I mean by this is n is maybe anywhere from 1 - 1000, 1 would be very small. But maybe up to 1000 features and if the number of training examples is maybe anywhere from 10, you know, 10 to maybe up to 10,000 examples. Maybe up to 50,000 examples. If m is pretty big like maybe 10,000 but not a million. Right? So if m is an intermediate size then often an SVM with a linear kernel will work well. We talked about this early as well, with the one concrete example, this would be if you have a two dimensional training set. So, if n is equal to 2 where you have, you know, drawing in a pretty large number of training examples. So Gaussian kernel will do a pretty good job separating positive and negative classes. One third setting that’s of interest is if n is small but m is large. So if n is you know, again maybe 1 to 1000, could be larger. But if m was, maybe 50,000 and greater to millions. So, 50,000, a 100,000, million, trillion. You have very very large training set sizes, right. So if this is the case, then a SVM of the Gaussian Kernel will be somewhat slow to run. Today’s SVM packages, if you’re using a Gaussian Kernel, tend to struggle a bit. If you have, you know, maybe 50 thousands okay, but if you have a million training examples, maybe or even a 100,000 with a massive value of m. Today’s SVM packages are very good, but they can still struggle a little bit <strong>when you have a massive, massive trainings that size when using a Gaussian Kernel. So in that case, what I would usually do is try to just manually create have more features and then use logistic regression or an SVM without the Kernel.</strong> And in case you look at this slide and you see logistic regression or SVM without a kernel. <strong>In both of these places, I kind of paired them together. There’s a reason for that, is that logistic regression and SVM without the kernel, those are really pretty similar algorithms and, you know, either logistic regression or SVM without a kernel will usually do pretty similar things and give pretty similar performance, but depending on your implementational details, one may be more efficient than the other.</strong> But, where one of these algorithms applies, logistic regression where SVM without a kernel, the other one is to likely to work pretty well as well. But along with the power of the SVM is when you use different kernels to learn complex nonlinear functions. And this regime, you know, when you have maybe up to 10,000 examples, maybe up to 50,000. And your number of features, this is reasonably large. That’s a very common regime and maybe that’s a regime where a support vector machine with a kernel kernel will shine. You can do things that are much harder to do that will need logistic regression. </p>
<p><strong>And finally, where do neural networks fit in? Well for all of these problems, for all of these different regimes, a well designed neural network is likely to work well as well. The one disadvantage, or the one reason that might not sometimes use the neural network is that, for some of these problems, the neural network might be slow to train. But if you have a very good SVM implementation package, that could run faster, quite a bit faster than your neural network. And, although we didn’t show this earlier, it turns out that the optimization problem that the SVM has is a convex optimization problem and so the good SVM optimization software packages will always find the global minimum or something close to it. And so for the SVM you don’t need to worry about local optima. In practice local optima aren’t a huge problem for neural networks but they all solve, so this is one less thing to worry about if you’re using an SVM. And depending on your problem, the neural network may be slower, especially in this sort of regime than the SVM.</strong></p>
<p>In case the guidelines they gave here, seem a little bit vague and if you’re looking at some problems, you know, the guidelines are a bit vague, I’m still not entirely sure, should I use this algorithm or that algorithm, that’s actually okay. <strong>When I face a machine learning problem, you know, sometimes its actually just not clear whether that’s the best algorithm to use, but as you saw in the earlier videos, really, you know, the algorithm does matter, but what often matters even more is things like, how much data do you have. And how skilled are you, how good are you at doing error analysis and debugging learning algorithms, figuring out how to design new features and figuring out what other features to give you learning algorithms and so on. And often those things will matter more than what you are using logistic regression or an SVM.</strong> But having said that, the SVM is still widely perceived as one of the most powerful learning algorithms, and there is this regime of when there’s a very effective way to learn complex non linear functions. And so I actually, together with logistic regressions, neural networks, SVM’s, using those to speed learning algorithms you’re I think very well positioned to build state of the art you know, machine learning systems for a wide region for applications and this is another very powerful tool to have in your arsenal. One that is used all over the place in Silicon Valley, or in industry and in the Academia, to build many high performance machine learning system.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/11/11_machine-learning-system-design/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/11/11_machine-learning-system-design/" class="post-title-link" itemprop="url">11_machine-learning-system-design note11</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-11 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-11T00:00:00+05:30">2018-01-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 14:57:09" itemprop="dateModified" datetime="2020-04-09T14:57:09+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>37k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>34 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In the next few videos I’d like to talk about <strong>machine learning system design</strong>. These videos will touch on <strong>the main issues that you may face when designing a complex machine learning system, and will actually try to give advice on how to strategize putting together a complex machine learning system.</strong> </p>
<p>In case this next set of videos seems a little disjointed that’s because these videos will touch on a range of the different issues that you may come across when designing complex learning systems. And <strong><em>even though the next set of videos may seem somewhat less mathematical, I think that this material may turn out to be very useful, and potentially huge time savers when you’re building big machine learning systems.</em></strong> </p>
<h1 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h1><p>This personal note is written after studying the opening course on <a href="https://www.coursera.org" target="_blank" rel="noopener">the coursera website</a>, <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew NG</a> . And images, audios of this note all comes from the opening course. </p>
<h1 id="01-building-a-spam-classifier"><a href="#01-building-a-spam-classifier" class="headerlink" title="01_building-a-spam-classifier"></a>01_building-a-spam-classifier</h1><h2 id="Prioritizing-What-to-Work-On"><a href="#Prioritizing-What-to-Work-On" class="headerlink" title="Prioritizing What to Work On"></a>Prioritizing What to Work On</h2><p>Concretely, I’d like to begin with the issue of prioritizing how to spend your time on what to work on, and I’ll begin with an example on spam classification. </p>
<p>Let’s say you want to build <strong>a spam classifier</strong>. Here are a couple of examples of obvious spam and non-spam emails. if the one on the left tried to sell things. And notice how spammers will deliberately misspell words, like Vincent with a 1 there, and mortgages. And on the right as maybe an obvious example of non-stamp email, actually email from my younger brother. Let’s say we have a labeled training set of some number of spam emails and some non-spam emails denoted with labels y equals 1 or 0, how do we build a classifier using supervised learning to distinguish between spam and non-spam? </p>
<p><strong>In order to apply supervised learning, the first decision we must make is how do we want to represent x, that is the features of the email.</strong> Given the features x and the labels y in our training set, we can then train a classifier, for example using logistic regression. Here’s one way to choose a set of features for our emails. We could come up with, say, a list of maybe a hundred words that we think are indicative of whether e-mail is spam or non-spam, for example, if a piece of e-mail contains the word ‘deal’ maybe it’s more likely to be spam if it contains the word  ‘buy’ maybe more likely to be spam, a word like ‘discount’ is more likely to be spam, whereas if a piece of email contains my name, Andrew, maybe that means the person actually knows who I am and that might mean it’s less likely to be spam. And maybe for some reason I think the word “now” may be indicative of non-spam because I get a lot of urgent emails, and so on, and maybe we choose a hundred words or so. <strong>Given a piece of email, we can then take this piece of email and encode it into a feature vector as follows.</strong> I’m going to take my list of a hundred words and sort them in alphabetical order say. It doesn’t have to be sorted. But, you know, here’s a, here’s my list of words, just count and so on, until eventually I’ll get down to now, and so on and given a piece of e-mail like that shown on the right, I’m going to check and see whether or not each of these words appears in the e-mail and then I’m going to define a feature vector x where in this piece of an email on the right, my name doesn’t appear so I’m gonna put a zero there. The word “by” does appear, so I’m gonna put a one there and I’m just gonna put one’s or zeroes. I’m gonna put a one even though the word “by” occurs twice. I’m not gonna recount how many times the word occurs. The word “due” appears, I put a one there. The word “discount” doesn’t appear, at least not in this this little short email, and so on. The word “now” does appear and so on. <strong>So I put ones and zeroes in this feature vector depending on whether or not a particular word appears.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/1.png" alt="example_of_spam-email_and_non-spam-email"></p>
<p><strong>And in this example my feature vector would have to mention one hundred, if I have a hundred, if if I chose a hundred words to use for this representation and each of my features $X_j$ will basically be $1$ if you have a particular word that, we’ll call this word j, appears in the email and $X_j$ would be $0$ otherwise.</strong></p>
<p>Okay. So that gives me a feature representation of a piece of email. By the way, even though I’ve described this process as manually picking a hundred words, in practice what’s most commonly done is to look through a training set, and in the training set depict the most frequently occurring n words where n is usually between ten thousand and fifty thousand, and use those as your features. So rather than manually picking a hundred words, here you look through the training examples and pick the most frequently occurring words like ten thousand to fifty thousand words, and those form the features that you are going to use to represent your email for spam classification. </p>
<p><strong>Now, if you’re building a spam classifier, one question that you may face is, what’s the best use of your time in order to make your spam classifier have higher accuracy, you have lower error.</strong> One natural inclination is going to collect lots of data. Right? And in fact there’s this tendency to think that, well the more data we have the better the algorithm will do. And in fact, in the email spam domain, there are actually pretty serious projects called Honey Pot Projects, which create fake email addresses and try to get these fake email addresses into the hands of spammers and use that to try to collect tons of spam email, and therefore you know, get a lot of spam data to train learning algorithms. But we’ve already seen in the previous sets of videos that getting lots of data will often help, but not all the time. But for most machine learning problems, there are a lot of other things you could usually imagine doing to improve performance. For spam, one thing you might think of is to develop more sophisticated features on the email, maybe based on the email routing information. And this would be information contained in the email header. So, when spammers send email, very often they will try to obscure the origins of the email, and maybe use fake email headers. Or send email through very unusual sets of computer service. Through very unusual routes, in order to get the spam to you. And some of this information will be reflected in the email header. And so one can imagine, looking at the email headers and trying to develop more sophisticated features to capture this sort of email routing information to identify if something is spam. Something else you might consider doing is to look at the email message body, that is the email text, and try to develop more sophisticated features. For example, should the word ‘discount’ and the word ‘discounts’ be treated as the same words or should we have treat the words ‘deal’ and ‘dealer’ as the same word? Maybe even though one is lower case and one in capitalized in this example. Or do we want more complex features about punctuation because maybe spam is using exclamation marks a lot more. I don’t know. And along the same lines, maybe we also want to develop more sophisticated algorithms to detect and maybe to correct to deliberate misspellings, like mortgage, medicine, watches. Because spammers actually do this, because if you have watches with a 4 in there then well, with the simple technique that we talked about just now, the spam classifier might not equate this as the same thing as the word “watches,” and so it may have a harder time realizing that something is spam with these deliberate misspellings. And this is why spammers do it. While working on a machine learning problem, very often you can brainstorm lists of different things to try, like these. By the way, I’ve actually worked on the spam problem myself for a while. And I actually spent quite some time on it. And even though I kind of understand the spam problem, I actually know a bit about it, I would actually have a very hard time telling you of these four options which is the best use of your time so what happens, frankly what happens far too often is that a research group or product group will randomly fixate on one of these options. And sometimes that turns out not to be the most fruitful way to spend your time depending, you know, on which of these options someone ends up randomly fixating on. By the way, in fact, if you even get to the stage where you brainstorm a list of different options to try, you’re probably already ahead of the curve. Sadly, what most people do is instead of trying to list out the options of things you might try, what far too many people do is wake up one morning and, for some reason, just, you know, have a weird gut feeling that, “Oh let’s have a huge honeypot project to go and collect tons more data” and for whatever strange reason just sort of wake up one morning and randomly fixate on one thing and just work on that for six months. But I think we can do better. And in particular what I’d like to do in the next video is tell you about the concept of error analysis and talk about the way where you can try to have a more systematic way to choose amongst the options of the many different things you might work, and therefore be more likely to select what is actually a good way to spend your time, you know for the next few weeks, or next few days or the next few months.</p>
<p><strong>System Design Example:</strong>   </p>
<p>Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set.  If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/2.png" alt="Building_a_spam_classifier"></p>
<p>So how could you spend your time to improve the accuracy of this classifier? </p>
<ul>
<li><p>Collect lots of data (for example “honeypot” project but doesn’t always work)   </p>
</li>
<li><p>Develop sophisticated features (for example: using email header data in spam emails)   </p>
</li>
<li><p>Develop sophisticated features for message body (for example: should“discount” and “discounts” be treated as the same word? How about “deal” and “Dealer”? Features about punctuation)? </p>
</li>
<li><p>Develop algorithms to process your input in different ways (recognizing misspellings in spam, for example, med1cine, m0rtgage, w4tches).   </p>
<p>It is difficult to tell which of the options will be most helpful.</p>
</li>
</ul>
<h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p>The recommended approach to solving machine learning problems is to:  </p>
<ul>
<li><strong><em>Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.</em></strong>   </li>
<li><strong><em>Plot learning curves to decide if more data, more features, etc. are likely to help.</em></strong>   </li>
<li><strong><em>Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.</em></strong>   </li>
</ul>
<p>For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/3.png" alt="error_analysis"></p>
<p>We could also see how classifying each word according to its root changes our error rate:</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/4.png" alt="The_importance_of_numerical_evaluation"></p>
<p>It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature.  Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not.</p>
<h1 id="02-handling-skewed-data"><a href="#02-handling-skewed-data" class="headerlink" title="02_handling-skewed-data"></a>02_handling-skewed-data</h1><h2 id="01-error-metrics-for-skewed-classes"><a href="#01-error-metrics-for-skewed-classes" class="headerlink" title="01_error-metrics-for-skewed-classes"></a>01_error-metrics-for-skewed-classes</h2><p>In the previous video, I talked about error analysis and the importance of having error metrics, that is of having a single real number evaluation metric for your learning algorithm to tell how well it’s doing. </p>
<p>In the context of evaluation and of error metrics, there is one important case, where it’s particularly tricky to come up with an appropriate error metric, or evaluation metric, for your learning algorithm. That case is the case of what’s called <strong>skewed classes</strong>. Let me tell you what that means. </p>
<p>Consider the problem of cancer classification, where we have features of medical patients and we want to decide whether or not they have cancer. So this is like the malignant versus benign tumor classification example that we had earlier. So let’s say y equals 1 if the patient has cancer and y equals 0 if they do not. We have trained the progression classifier and let’s say we test our classifier on a test set and find that we get 1 percent error. So, we’re making 99% correct diagnosis. Seems like a really impressive result, right. We’re correct 99% percent of the time. </p>
<p>But now, let’s say we find out that only 0.5 percent of patients in our training test sets actually have cancer. So only half a percent of the patients that come through our screening process have cancer. In this case, the 1% error no longer looks so impressive.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/5.png" alt="cancer_classification_example_of_skewed-class"></p>
<p>And in particular, here’s a piece of code, here’s actually a piece of non learning code that takes this input of features x and it ignores it. <strong><em>It just sets y equals 0 and always predicts, you know, nobody has cancer and this algorithm would actually get 0.5 percent error. So this is even better than the 1% error that we were getting just now and this is a non learning algorithm that you know, it is just predicting y equals 0 all the time. So this setting of when the ratio of positive to negative examples is very close to one of two extremes, where, in this case, the number of positive examples is much, much smaller than the number of negative examples because y equals one so rarely, this is what we call the case of skewed classes.</em></strong> </p>
<p>We just have a lot more of examples from one class than from the other class. And by just predicting y equals 0 all the time, or maybe our predicting y equals 1 all the time, an algorithm can do pretty well. So the problem with using classification error or classification accuracy as our evaluation metric is the following. </p>
<p>Let’s say you have one joining algorithm that’s getting 99.2% accuracy. So, that’s a 0.8% error. Let’s say you make a change to your algorithm and you now are getting 99.5% accuracy. That is 0.5% error. So, is this an improvement to the algorithm or not? One of the nice things about having a single real number evaluation metric is this helps us to quickly decide if we just need a good change to the algorithm or not. <strong>By going from 99.2% accuracy to 99.5% accuracy. You know, did we just do something useful or did we just replace our code with something that just predicts y equals zero more often?  So, if you have very skewed classes it becomes much harder to use just classification accuracy, because you can get very high classification accuracies or very low errors, and it’s not always clear if doing so is really improving the quality of your classifier because predicting y equals 0 all the time doesn’t seem like a particularly good classifier.</strong> But just predicting y equals 0 more often can bring your error down to, you know, maybe as low as 0.5%. </p>
<p>When we’re faced with such a skewed classes therefore we would want to come up with a different error metric or a different evaluation metric. One such evaluation metric are what’s called <strong>precision and recall</strong>. Let me explain what that is. Let’s say we are evaluating a classifier on the test set. For the examples in the test set the actual class of that example in the test set is going to be either one or zero, right, if there is a binary classification problem. And what our learning algorithm will do is it will, you know, predict some value for the class and our learning algorithm will predict the value for each example in my test set and the predicted value will also be either one or zero. So let me draw a two by two table as follows, depending on a full of these entries depending on what was the actual class and what was the predicted class. If we have an example where the actual class is one and the predicted class is one then that’s called an example that’s a <strong><em>true positive</em></strong>, meaning our algorithm predicted that it’s positive and in reality the example is positive. If our learning algorithm predicted that something is negative, class zero, and the actual class is also class zero then that’s what’s called a <strong><em>true negative</em></strong>. We predicted zero and it actually is zero. To find the other two boxes, if our learning algorithm predicts that the class is one but the actual class is zero, then that’s called a <strong><em>false positive</em></strong>. So that means our algorithm for the patient is cancelled out in reality if the patient does not. And finally, the last box is a zero, one. That’s called a <strong><em>false negative</em></strong> because our algorithm predicted zero, but the actual class was one. And so, we have this little sort of two by two table based on what was the actual class and what was the predicted class. So here’s a different way of evaluating the performance of our algorithm. We’re going to compute two numbers. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/6.png" alt="definitions_of_precision_and_recall"></p>
<p>The first is called <strong><em>precision</em></strong> - and what that says is, of all the patients where we’ve predicted that they have cancer, what fraction of them actually have cancer? So let me write this down, the precision of a classifier is the number of true positives divided by the number that we predicted as positive, right? So of all the patients that we went to those patients and we told them, “We think you have cancer.” Of all those patients, what fraction of them actually have cancer? So that’s called precision. <strong>And another way to write this would be true positives and then in the denominator is the number of predicted positives, and so that would be the sum of the, you know, entries in this first row of the table. So it would be true positives divided by positives. I’m going to abbreviate positive as POS and then plus false positives, again abbreviating positive using POS.</strong> So that’s called precision, and as you can tell high precision would be good. That means that all the patients that we went to and we said, “You know, we’re very sorry. We think you have cancer,” <strong><em>high precision</em></strong> means that of that group of patients most of them we had actually made accurate predictions on them and they do have cancer. </p>
<p>The second number we’re going to compute is called <strong><em>recall</em></strong>, and what recall say is, if all the patients in, let’s say, in the test set or the cross-validation set, but if all the patients in the data set that actually have cancer, what fraction of them that we correctly detect as having cancer. So if all the patients have cancer, how many of them did we actually go to them and you know, correctly told them that we think they need treatment. <strong>So, writing this down, recall is defined as the number of positives, the number of true positives, meaning the number of people that have cancer and that we correctly predicted have cancer and we take that and divide that by, divide that by the number of actual positives, so this is the right number of actual positives of all the people that do have cancer.</strong> What fraction do we directly flag and you know, send the treatment. So, to rewrite this in a different form, the denominator would be the number of actual positives as you know, is the sum of the entries in this first column over here. And so writing things out differently, this is therefore, the number of true positives, divided by the number of true positives plus the number of false negatives. And so once again, having a high recall would be a good thing. </p>
<p>So by computing precision and recall this will usually give us a better sense of how well our classifier is doing. And in particular if we have a learning algorithm that predicts y equals zero all the time, if it predicts no one has cancer, then this classifier will have a recall equal to zero, because there won’t be any true positives and so that’s a quick way for us to recognize that, you know, a classifier that predicts y equals 0 all the time, just isn’t a very good classifier. And more generally, even for settings where we have very skewed classes, it’s not possible for an algorithm to sort of “cheat” and somehow get a very high precision and a very high recall by doing some simple thing like predicting y equals 0 all the time or predicting y equals 1 all the time. <strong>And so we’re much more sure that a classifier of a high precision or high recall actually is a good classifier, and this gives us a more useful evaluation metric that is a more direct way to actually understand whether, you know, our algorithm may be doing well.</strong> So one final note in the definition of precision and recall, that we would define precision and recall, usually we use the convention that y is equal to 1, in the presence of the more rare class. So if we are trying to detect. rare conditions such as cancer, hopefully that’s a rare condition, precision and recall are defined setting y equals 1, rather than y equals 0, to be sort of that the presence of that rare class that we’re trying to detect. And by using precision and recall, we find, what happens is that even if we have very skewed classes, it’s not possible for an algorithm to you know, “cheat” and predict y equals 1 all the time, or predict y equals 0 all the time, and get high precision and recall. And in particular, if a classifier is getting high precision and high recall, then we are actually confident that the algorithm has to be doing well, even if we have very skewed classes. <strong><em>So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms, than looking at classification error or classification accuracy, when the classes are very skewed.</em></strong></p>
<h2 id="02-trading-off-precision-and-recall"><a href="#02-trading-off-precision-and-recall" class="headerlink" title="02_trading-off-precision-and-recall"></a>02_trading-off-precision-and-recall</h2><p>In the last video,we talked about precision and recall as an evaluation metric for classification problems with skewed constants. <strong>For many applications, we’ll want to somehow control the trade-off between precision and recall.</strong> Let me tell you how to do that and also show you some even more effective ways to use precision and recall as an evaluation metric for learning algorithms.</p>
<p>As a reminder,here are the definitions of precision and recall from the previous video. Let’s continue our cancer classification example, where y equals 1 if the patient has cancer and y equals 0 otherwise. And let’s say we’re trained in logistic regression classifier which outputs probability between 0 and 1. So, as usual, we’re going to predict 1, y equals 1, if h(x) is greater or equal to 0.5. And predict 0 if the hypothesis outputs a value less than 0.5. And this classifier may give us some value for precision and some value for recall. But now, suppose we want to predict that the patient has cancer only if we’re very confident that they really do. Because if you go to a patient and you tell them that they have cancer, it’s going to give them a huge shock. What we give is a seriously bad news, and they may end up going through a pretty painful treatment process and so on. And so maybe we want to tell someone that we think they have cancer only if they are very confident. One way to do this would be to modify the algorithm, so that instead of setting this threshold at 0.5, we might instead say that we will predict that y is equal to 1 only if h(x) is greater or equal to 0.7. So this is like saying, we’ll tell someone they have cancer only if we think there’s a greater than or equal to, 70% chance that they have cancer. And, if you do this, then you’re predicting someone has cancer<br>only when you’re more confident and so you end up with a classifier that has higher precision. Because all of the patients that you’re going to and saying, we think you have cancer, although those patients are now ones that you’re pretty confident actually have cancer. And <strong>so a higher fraction of the patients that you predict have cancer will actually turn out to have cancer because making those predictions only if we’re pretty confident. But in contrast this classifier will have lower recall because now we’re going to make predictions, we’re going to predict y = 1 on a smaller number of patients.</strong> Now, can even take this further. Instead of setting the threshold at 0.7, we can set this at 0.9. Now we’ll predict y=1 only if we are more than 90% certain that the patient has cancer. And so, a large fraction of those patients will turn out to have cancer. And so this would be a higher precision classifier will have lower recall because we want to correctly detect that those patients have cancer. <strong>Now consider a different example. Suppose we want to avoid missing too many actual cases of cancer, so we want to avoid false negatives. In particular, if a patient actually has cancer, but we fail to tell them that they have cancer then that can be really bad. Because if we tell a patient that they don’t have cancer, then they’re not going to go for treatment. And if it turns out that they have cancer, but we fail to tell them they have cancer, well, they may not get treated at all. And so that would be a really bad outcome because they die because we told them that they don’t have cancer. They fail to get treated, but it turns out they actually have cancer.</strong> So, suppose that, when in doubt, we want to predict that y=1. So, when in doubt, we want to predict that they have cancer so that at least they look further into it, and these can get treated in case they do turn out to have cancer. <strong>In this case, rather than setting higher probability threshold, we might instead take this value and instead set it to a lower value.</strong> So maybe 0.3 like so, right? And by doing so, we’re saying that,you know what, if we think there’s more than a 30% chance that they have cancer we better be more conservative and tell them that they may have cancer so that they can seek treatment if necessary. <strong>And in this case what we would have is going to be a higher recall classifier, because we’re going to be correctly flagging a higher fraction of all of the patients that actually do have cancer.</strong> But we’re going to end up with lower precision because a higher fraction of the patients that we said have cancer, a high fraction of them will turnout not to have cancer after all. And by the way, just as a sider, when I talk about this to other students, I’ve been told before, it’s pretty amazing, some of my students say, is how I can tell the story both ways. <strong>Why we might want to have higher precision or higher recall and the story actually seems to work both ways. But I hope the details of the algorithm is true and the more general principle is depending on where you want, whether you want higher precision- lower recall, or higher recall- lower precision.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/7.png" alt="trading-off_precision_and_recall"></p>
<p>You can end up predicting y=1 when h(x) is greater than some threshold. And so in general, for most classifiers there is going to be a trade off between precision and recall, and as you vary the value of this threshold that we join here, you can actually plot out some curve that trades off precision and recall. Where a value up here, this would correspond to a very high value of the threshold, maybe threshold equals 0.99. So that’s saying, predict y=1 only if we’re more than 99% confident, at least 99% probability this one. So that would be a high precision, relatively low recall. Where as the point down here, will correspond to a value of the threshold that’s much lower, maybe equal 0.01, meaning, when in doubt at all, predict y=1, and if you do that, you end up with a much lower precision, higher recall classifier. And as you vary the threshold, if you want you can actually trace of a curve for your classifier to see the range of different values you can get for precision recall. And by the way, the precision-recall curve can look like many different shapes. Sometimes it will look like this, sometimes it will look like that. Now there are many different possible shapes for the precision-recall curve, depending on the details of the classifier.</p>
<p><strong>So, this raises another interesting question which is, is there a way to choose this threshold automatically? Or more generally, if we have a few different algorithms or a few different ideas for algorithms, how do we compare different precision recall numbers? Concretely, suppose we have three different learning algorithms. So actually, maybe these are three different learning algorithms, maybe these are the same algorithm but just with different values for the threshold. How do we decide which of these algorithms is best?</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/8.png" alt="F1_score"></p>
<h1 id="03-using-large-data-sets"><a href="#03-using-large-data-sets" class="headerlink" title="03_using-large-data-sets"></a>03_using-large-data-sets</h1><p>In the previous video, we talked about evaluation metrics. In this video, I’d like to switch tracks a bit and touch on another important aspect of machine learning system design, which will often come up, which is <strong>the issue of how much data to train on</strong>.  </p>
<p>Now, in some earlier videos, <strong><em>I had cautioned against blindly going out and just spending lots of time collecting lots of data, because it’s only sometimes that that would actually help.</em></strong> But it turns out that under certain conditions, and I will say in this video what those conditions are, <strong><em>getting a lot of data and training on a certain type of learning algorithm, can be a very effective way to get a learning algorithm to do very good performance. And this arises often enough that if those conditions hold true for your problem and if you’re able to get a lot of data, this could be a very good way to get a very high performance learning algorithm.</em></strong> So in this video, let’s talk more about that. Let me start with a story. </p>
<h2 id="Michelle-Banko-and-Eric-Broule"><a href="#Michelle-Banko-and-Eric-Broule" class="headerlink" title="Michelle Banko and Eric Broule"></a>Michelle Banko and Eric Broule</h2><p>Many, many years ago, two researchers that I know, Michelle Banko and Eric Broule ran the following fascinating study. They were interested in studying the effect of using different learning algorithms versus trying them out on different training set sciences, they were considering the problem of classifying between confusable words, so for example, in the sentence: for breakfast I ate, should it be to, two or too? Well, for this example, for breakfast I ate two, 2 eggs. So, this is one example of a set of confusable words and that’s a different set. So they took machine learning problems like these, sort of supervised learning problems to try to categorize what is the appropriate word to go into a certain position in an English sentence. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/9.png" alt="img"> </p>
<p>They took a few different learning algorithms which were, you know, sort of considered state of the art back in the day, when they ran the study in 2001, so they took a variance, roughly a variance on logistic regression called the Perceptron. They also took some of their algorithms that were fairly out back then but somewhat less used now so when the algorithm also very similar to which is a regression but different in some ways, much used somewhat less, used not too much right now took what’s called a memory based learning algorithm again used somewhat less now. But I’ll talk a little bit about that later. And they used a naive based algorithm, which is something they’ll actually talk about in this course. The exact algorithms of these details aren’t important. Think of this as, you know, just picking four different classification algorithms and really the exact algorithms aren’t important. <strong>But what they did was they varied the training set size and tried out these learning algorithms on the range of training set sizes and that’s the result they got.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/10.png" alt="img"> </p>
<p>And the trends are very clear, right? </p>
<ol>
<li>first, most of these algorithms give remarkably similar performance. </li>
<li>And second, as the training set size increases, on the horizontal axis is the training set size in millions go from, you know, a hundred thousand up to a thousand million that is a billion training examples. The performance of the algorithms all pretty much monotonically increase and the fact that if you pick any algorithm, may be pick a “inferior algorithm”, but if you give that “inferior algorithm” more data, then from these examples, it looks like it will most likely beat even a “superior algorithm”. </li>
</ol>
<p>So since this original study which is very influential, <strong>there’s been a range of many different studies showing similar results that show that many different learning algorithms you know tend to, can sometimes, depending on details, can give pretty similar ranges of performance, but what can really drive performance is you can give the algorithm a ton of training data.</strong> And this is, results like these has led to a saying in machine learning that often in machine learning <strong>it’s not who has the best algorithm that wins, it’s who has the most data So when is this true and when is this not true?</strong> Because we have a learning algorithm for which this is true then getting a lot of data is often maybe the best way to ensure that we have an algorithm with very high performance rather than you know, debating worrying about exactly which of these items to use.</p>
<h2 id="the-features-x-have-sufficient-information"><a href="#the-features-x-have-sufficient-information" class="headerlink" title="the features x have sufficient information"></a>the features x have sufficient information</h2><p>Let’s try to lay out a set of assumptions under which having a massive training set we think will be able to help. Let’s assume that in our machine learning problem, the features x have sufficient information, with which we can use to predict y accurately. </p>
<p>For example, if we take the confusable words all of them that we had on the previous slide. Let’s say that it features x capture what are the surrounding words around the blank that we’re trying to fill in. So the features capture then we want to have, sometimes for breakfast I have black eggs. Then yeah that is pretty much information to tell me that the word I want in the middle is TWO and that is not word TO and its not the word TOO. <strong>So the features capture, you know, one of these surrounding words then that gives me enough information to pretty unambiguously decide what is the label y or in other words what is the word that I should be using to fill in that blank out of this set of three confusable words.</strong> So that’s an example what the futures x has sufficient information for specific y. </p>
<p>For a counter example. <strong>Consider a problem of predicting the price of a house from only the size of the house and from no other features.</strong> So if you imagine I tell you that a house is, you know, 500 square feet but I don’t give you any other features. I don’t tell you that the house is in an expensive part of the city. Or if I don’t tell you that the house, the number of rooms in the house, or how nicely furnished the house is, or whether the house is new or old. If I don’t tell you anything other than that this is a 500 square foot house, well there’s so many other factors that would affect the price of a house other than just the size of a house that if all you know is the size, it’s actually very difficult to predict the price accurately. So that would be a counter example to this assumption that the features have sufficient information to predict the price to the desired level of accuracy. </p>
<h2 id="domain-knowledge"><a href="#domain-knowledge" class="headerlink" title="domain knowledge"></a>domain knowledge</h2><p>The way I think about testing this assumption, one way I often think about it is, how often I ask myself. Given the input features x, given the features, given the same information available as well as learning algorithm. If we were to go to <strong>human expert in this domain</strong>. Can a human experts actually or can human expert confidently predict the value of y. <strong><em>For this first example if we go to, you know an expert human English speaker.</em></strong> You go to someone that speaks English well, right, then a human expert in English just read most people like you and me will probably we would probably be able to predict what word should go in here, to a good English speaker can predict this well, and so this gives me confidence that x allows us to predict y accurately, but in contrast if we go to <strong><em>an expert in house prices.</em></strong> Like maybe an expert realtor, right, someone who sells houses for a living. If I just tell them the size of a house and I tell them what the price is well even an expert in pricing or selling houses wouldn’t be able to tell me and so this is fine that for the housing price example knowing only the size doesn’t give me enough information to predict the price of the house. So, let’s say, this assumption holds. Let’s see then, when having a lot of data could help. Suppose the features have enough information to predict the value of y. </p>
<p>And let’s suppose we use a learning algorithm with a large number of parameters so maybe logistic regression or linear regression with a large number of features. Or one thing that I sometimes do, one thing that I often do actually is using neural network with many hidden units. That would be another learning algorithm with a lot of parameters. So these are all powerful learning algorithms with a lot of parameters that can fit very complex functions. So, I’m going to call these, I’m going to think of these as low-bias algorithms because you know we can fit very complex functions and because we have a very powerful learning algorithm, they can fit very complex functions. Chances are, if we run these algorithms on the data sets, it will be able to fit the training set well, and so hopefully the training error will be slow. </p>
<h2 id="Large-data-rationale"><a href="#Large-data-rationale" class="headerlink" title="Large data rationale"></a>Large data rationale</h2><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/11/11.png" alt="img"> </p>
<p><strong>Now let’s say, we use a massive, massive training set, in that case, if we have a huge training set, then hopefully even though we have a lot of parameters but if the training set is sort of even much larger than the number of parameters then hopefully these albums will be unlikely to overfit.</strong> Right, <strong>because we have such a massive training set and by unlikely to overfit what that means is that the training error will hopefully be close to the test error.</strong> Finally putting these two together that the train set error is small and the test set error is close to the training error what this two together imply is that hopefully the test set error will also be small. </p>
<p><strong>Another way to think about this is that in order to have a high performance learning algorithm we want it not to have high bias and not to have high variance. So the bias problem we’re going to address by making sure we have a learning algorithm with many parameters and so that gives us a low bias algorithm and by using a very large training set, this ensures that we don’t have a variance problem here.</strong> So hopefully our algorithm will have no variance and so is by pulling these two together, that we end up with a low bias and a low variance learning algorithm and this allows us to do well on the test set. And fundamentally it’s a key ingredients of assuming that the features have enough information and we have a rich class of functions that’s why it guarantees low bias, and then it having a massive training set that that’s what guarantees more variance. So this gives us a set of conditions rather hopefully some understanding of what’s the sort of problem where if you have a lot of data and you train a learning algorithm with lot of parameters, that might be a good way to give a high performance learning algorithm </p>
<h2 id="The-Key-Test"><a href="#The-Key-Test" class="headerlink" title="The Key Test"></a>The Key Test</h2><p>and really, I think <strong>the key test</strong> that I often ask myself are <strong>first</strong>, can a human experts look at the features x and confidently predict the value of y. Because that’s sort of a certification that y can be predicted accurately from the features x and <strong>second</strong>, can we actually get a large training set, and train the learning algorithm with a lot of parameters in the training set and if you can’t do both then that’s more often give you a very kind performance learning algorithm.</p>
<h1 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h1><h2 id="Prioritizing-What-to-Work-On-1"><a href="#Prioritizing-What-to-Work-On-1" class="headerlink" title="Prioritizing What to Work On"></a>Prioritizing What to Work On</h2><p>Different ways we can approach a machine learning problem: </p>
<ul>
<li>Collect lots of data (for example “honeypot” project but doesn’t always work) </li>
<li>Develop sophisticated features (for example: using email header data in spam emails) </li>
<li>Develop algorithms to process your input in different ways (recognizing misspellings in spam). </li>
</ul>
<p>It is difficult to tell which of the options will be helpful. </p>
<h2 id="Error-Analysis-1"><a href="#Error-Analysis-1" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p>The recommended approach to solving machine learning problems is: </p>
<ul>
<li>Start with a simple algorithm, implement it quickly, and test it early. </li>
<li>Plot learning curves to decide if more data, more features, etc. will help </li>
<li>Error analysis: manually examine the errors on examples in the cross validation set and try to spot a trend. </li>
</ul>
<p>It’s important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance.<br>You may need to process your input before it is useful. For example, if your input is a set of words, you may want to treat the same word with different forms (fail/failing/failed) as one word, so must use “stemming software” to recognize them all as one.</p>
<h2 id="Error-Metrics-for-Skewed-Classes"><a href="#Error-Metrics-for-Skewed-Classes" class="headerlink" title="Error Metrics for Skewed Classes"></a>Error Metrics for Skewed Classes</h2><p>It is sometimes difficult to tell whether a reduction in error is actually an improvement of the algorithm. </p>
<p>For example: In predicting a cancer diagnoses where 0.5% of the examples have cancer, we find our learning algorithm has a 1% error. However, if we were to simply classify every single example as a 0, then our error would reduce to 0.5% even though we did not improve the algorithm. </p>
<p>This usually happens with <strong>skewed classes</strong> ; that is, when our class is very rare in the entire data set.<br>Or to say it another way, when we have lot more examples from one class than from the other class.<br>For this we can use <strong>Precision/Recall</strong> . </p>
<ul>
<li>Predicted: 1, Actual: 1 — True positive </li>
<li>Predicted: 0, Actual: 0 — True negative </li>
<li>Predicted: 0, Actual, 1 — False negative </li>
<li>Predicted: 1, Actual: 0 — False positive </li>
</ul>
<p><strong>Precision</strong> : of all patients we predicted where y=1, what fraction actually has cancer?<br>$$\dfrac{\text{True Positives}}{\text{Total number of predicted positives}} = \dfrac{\text{True Positives}}{\text{True Positives}+\text{False positives}}$$<br><strong>Recall</strong> : Of all the patients that actually have cancer, what fraction did we correctly detect as having cancer?<br>$$\dfrac{\text{True Positives}}{\text{Total number of actual positives}}= \dfrac{\text{True Positives}}{\text{True Positives}+\text{False negatives}}$$<br>These two metrics give us a better sense of how our classifier is doing. We want both precision and recall to be high.<br>In the example at the beginning of the section, if we classify all patients as 0, then our <strong>recall</strong> will be $\dfrac{0}{0 + f} = 0$, so despite having a lower error percentage, we can quickly see it has worse recall.<br>Accuracy = $\frac {true\ positive + true\ negative} {total\ population}$<br>Note 1: if an algorithm predicts only negatives like it does in one of exercises, the precision is not defined, it is impossible to divide by 0. F1 score will not be defined too. </p>
<h2 id="Trading-Off-Precision-and-Recall"><a href="#Trading-Off-Precision-and-Recall" class="headerlink" title="Trading Off Precision and Recall"></a>Trading Off Precision and Recall</h2><p>We might want a <strong>confident prediction</strong> of two classes using logistic regression. One way is to increase our threshold:<br>Predict 1 if: $h_\theta(x) \geq 0.7$<br>Predict 0 if: $h_\theta(x) &lt; 0.7$<br>This way, we only predict cancer if the patient has a 70% chance.<br>Doing this, we will have <strong>higher precision</strong> but <strong>but lower recall</strong>(refer to the definitions in the previous section).<br>In the opposite example, we can lower our threshold:<br>Predict 1 if: $h_\theta(x) \geq 0.3$<br>Predict 0 if: $h_\theta(x) &lt; 0.3$<br>That way, we get a very <strong>safe prediction</strong>. This will cause higher recall but lower precision .<br>The greater the threshold, the greater the precision and the lower the recall.<br>The lower the threshold, the greater the recall and the lower the precision.<br>In order to turn these two metrics into one single number, we can take the <strong>F value</strong> .<br>One way is to take the <strong>average</strong> :<br>$$\dfrac{P+R}{2}$$<br>This does not work well. If we predict all y=0 then that will bring the average up despite having 0 recall. If we predict all examples as y=1, then the very high recall will bring up the average despite having 0 precision.<br>A better way is to compute the F Score (or <strong>F1 score</strong>):<br>$$\text{F Score} = 2\dfrac{PR}{P + R}$$<br>In order for the F Score to be large, both precision and recall must be large.<br>We want to train precision and recall on <strong>the cross validation</strong> set so as not to bias our test set. </p>
<h2 id="Data-for-Machine-Learning"><a href="#Data-for-Machine-Learning" class="headerlink" title="Data for Machine Learning"></a>Data for Machine Learning</h2><p>How much data should we train on?<br>In certain cases, an “inferior algorithm,” if given enough data, can outperform a superior algorithm with less data.<br>We must choose our features to have <strong>enough</strong> information. A useful test is: Given input x, would a human expert be able to confidently predict y?<br><strong>Rationale for large data</strong> : if we have a low bias algorithm (many features or hidden units making a very complex function), then the larger the training set we use, the less we will have overfitting (and the more accurate the algorithm will be on the test set). </p>
<h2 id="Quiz-instructions"><a href="#Quiz-instructions" class="headerlink" title="Quiz instructions"></a>Quiz instructions</h2><p>When the quiz instructions tell you to enter a value to “two decimal digits”, what it really means is “two significant digits”. So, just for example, the value 0.0123 should be entered as “0.012”, not “0.01”.<br>References:<br><a href="https://class.coursera.org/ml/lecture/index" target="_blank" rel="noopener">https://class.coursera.org/ml/lecture/index</a><br><a href="http://www.cedar.buffalo.edu/~srihari/CSE555/Chap9.Part2.pdf" target="_blank" rel="noopener">http://www.cedar.buffalo.edu/~srihari/CSE555/Chap9.Part2.pdf</a><br><a href="http://blog.stephenpurpura.com/post/13052575854/managing-bias-variance-tradeoff-in-machine-learning" target="_blank" rel="noopener">http://blog.stephenpurpura.com/post/13052575854/managing-bias-variance-tradeoff-in-machine-learning</a><br><a href="http://www.cedar.buffalo.edu/~srihari/CSE574/Chap3/Bias-Variance.pdf" target="_blank" rel="noopener">http://www.cedar.buffalo.edu/~srihari/CSE574/Chap3/Bias-Variance.pdf</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Karan"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Karan</p>
  <div class="site-description" itemprop="description">Refuse to Fall</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">87</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:snakecoding.py@gmail.com" title="Get In Touch → mailto:snakecoding.py@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>Get In Touch</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Karan</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">2.4m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">35:43</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


    </div>
</body>
</html>
