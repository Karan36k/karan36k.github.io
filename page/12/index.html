<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">

<script>
    (function(){
        if(''){
                         If (prompt('Please enter the article password') !== ''){
                                 Alert('Password error!');
                history.back();
            }
        }
    })();
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"snakecoding.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":"flat","style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":false},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Refuse to Fall">
<meta property="og:type" content="website">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://snakecoding.com/page/12/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="Refuse to Fall">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Karan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://snakecoding.com/page/12/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Machine Learning</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Machine Learning</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Data Science</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">17</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">91</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/yourname" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/16/16_recommender-systems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/16/16_recommender-systems/" class="post-title-link" itemprop="url">16_recommender-systems note16</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-16 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-16T00:00:00+05:30">2018-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>49k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>44 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="01-predicting-movie-ratings"><a href="#01-predicting-movie-ratings" class="headerlink" title="01_predicting-movie-ratings"></a>01_predicting-movie-ratings</h2><h3 id="01-predicting-movie-ratings-1"><a href="#01-predicting-movie-ratings-1" class="headerlink" title="01_predicting-movie-ratings"></a>01_predicting-movie-ratings</h3><p>In this next set of videos, I would like to tell you about <strong>recommender systems</strong>. There are two reasons, I had two motivations for why I wanted to talk about recommender systems. The first is just that it is an important application of machine learning. Over the last few years, occasionally I visit different, you know, technology companies here in Silicon Valley and I often talk to people working on machine learning applications there and so I’ve asked people what are the most important applications of machine learning or what are the machine learning applications that you would most like to get an improvement in the performance of. And one of the most frequent answers I heard was that there are many groups out in Silicon Valley now, trying to build better recommender systems. So, if you think about what the websites are like Amazon, or what Netflix or what eBay, or what iTunes Genius, made by Apple does, there are many websites or systems that try to recommend new products to use. So, Amazon recommends new books to you, Netflix try to recommend new movies to you, and so on. And these sorts of recommender systems, that look at what books you may have purchased in the past, or what movies you have rated in the past, but these are the systems that are responsible for today, a substantial fraction of Amazon’s revenue and for a company like Netflix, the recommendations that they make to the users is also responsible for a substantial fraction of the movies watched by their users. And so an improvement in performance of a recommender system can have a substantial and immediate impact on the bottom line of many of these companies. Recommender systems is kind of a funny problem, within academic machine learning so that we could go to an academic machine learning conference, the problem of recommender systems, actually receives relatively little attention, or at least it’s sort of a smaller fraction of what goes on within Academia. But if you look at what’s happening, many technology companies, the ability to build these systems seems to be a high priority for many companies. And that’s one of the reasons why I want to talk about them in this class. </p>
<p>The second reason that I want to talk about recommender systems is that as we approach the last few sets of videos of this class I wanted to talk about a few of the big ideas in machine learning and share with you, you know, some of the big ideas in machine learning. And <strong>we’ve already seen in this class that features are important for machine learning, the features you choose will have a big effect on the performance of your learning algorithm.</strong> </p>
<p><strong>So there’s this big idea in machine learning, which is that for some problems, maybe not all problems, but some problems, there are algorithms that can try to automatically learn a good set of features for you. So rather than trying to hand design, or hand code the features, which is mostly what we’ve been doing so far, there are a few settings where you might be able to have an algorithm, just to learn what feature to use, and the recommender systems is just one example of that sort of setting.</strong> </p>
<p>There are many others, but engraved through recommender systems, will be able to go a little bit into this idea of learning the features and you’ll be able to see at least one example of this, I think, big idea in machine learning as well. </p>
<p>So, without further ado, let’s get started, and talk about the recommender system problem formulation. As my running example, I’m going to use the modern problem of predicting movie ratings. So, here’s a problem. Imagine that you’re a website or a company that sells or rents out movies, or what have you. And so, you know, Amazon, and Netflix, and I think iTunes are all examples of companies that do this, and let’s say you let your users rate different movies, using a 1 to 5 star rating. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/1.png" alt="Example: Predicting movie ratings"></p>
<p>So, users may, you know, something one, two, three, four or five stars. In order to make this example just a little bit nicer, I’m going to allow 0 to 5 stars as well, because that just makes some of the math come out just nicer. Although most of these websites use the 1 to 5 star scale. So here, I have 5 movies. You know, Love That Lasts, Romance Forever, Cute Puppies of Love, Nonstop Car Chases, and Swords vs. Karate. And we have 4 users, which, calling, you know, Alice, Bob, Carol, and Dave, with initials A, B, C, and D, we’ll call them users 1, 2, 3, and 4. So, let’s say Alice really likes Love That Lasts and rates that 5 stars, likes Romance Forever, rates it 5 stars. She did not watch Cute Puppies of Love, and did rate it, so we don’t have a rating for that, and Alice really did not like Nonstop Car Chases or Swords vs. Karate. And a different user Bob, user two, maybe rated a different set of movies, maybe she likes to Love at Last, did not to watch Romance Forever, just have a rating of 4, a 0, a 0, and maybe our 3rd user, rates this 0, did not watch that one, 0, 5, 5, and, you know, let’s just fill in some of the numbers. And so just to introduce a bit of notation, this notation that we’ll be using throughout, I’m going to use NU to denote the number of users. So in this example, NU will be equal to 4. So the u-subscript stands for users and Nm, going to use to denote the number of movies, so here I have five movies so Nm equals equals 5. And you know for this example, I have for this example, I have loosely 3 maybe romantic or romantic comedy movies and 2 action movies and you know, if you look at this small example, it looks like Alice and Bob are giving high ratings to these romantic comedies or movies about love, and giving very low ratings about the action movies, and for Carol and Dave, it’s the opposite, right? Carol and Dave, users three and four, really like the action movies and give them high ratings, but don’t like the romance and love- type movies as much. Specifically, in the recommender system problem, we are given the following data. Our data comprises the following: we have these values r(i, j), and r(i, j) is 1 if user J has rated movie I. So our users rate only some of the movies, and so, you know, we don’t have ratings for those movies. And whenever r(i, j) is equal to 1, whenever user j has rated movie i, we also get this number y(i, j), which is the rating given by user j to movie i. And so, y(i, j) would be a number from zero to five, depending on the star rating, zero to five stars that user gave that particular movie. So, the recommender system problem is given this data that has give these r(i, j)’s and the y(i, j)’s to look through the data and look at all the movie ratings that are missing and to try to predict what these values of the question marks should be. In the particular example, I have a very small number of movies and a very small number of users and so most users have rated most movies but in the realistic settings your users each of your users may have rated only a minuscule fraction of your movies but looking at this data, you know, if Alice and Bob both like the romantic movies maybe we think that Alice would have given this a five. Maybe we think Bob would have given this a 4.5 or some high value, as we think maybe Carol and Dave were doing these very low ratings. And Dave, well, if Dave really likes action movies, maybe he would have given Swords and Karate a 4 rating or maybe a 5 rating, okay? And so, our job in developing a recommender system is to come up with a learning algorithm that can automatically go fill in these missing values for us so that we can look at, say, the movies that the user has not yet watched, and recommend new movies to that user to watch. You try to predict what else might be interesting to a user. So that’s the formalism of the recommender system problem. </p>
<p>In the next video we’ll start to develop a learning algorithm to address this problem.</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p><strong>Problem Formulation</strong></p>
<p>Recommendation is currently a very popular application of machine learning.<br>Say we are trying to recommend movies to customers. We can use the following definitions </p>
<ul>
<li>$n_u =$ number of users </li>
<li>$n_m =$ number of movies </li>
<li>$r(i,j) = 1$ if user j has rated movie i </li>
<li>$y(i,j) =$ rating given by user j to movie i (defined only if r(i,j)=1) </li>
</ul>
<h3 id="02-content-based-recommendations"><a href="#02-content-based-recommendations" class="headerlink" title="02_content-based-recommendations"></a>02_content-based-recommendations</h3><p>In the last video, we talked about the recommender systems problem where for example you might have a set of movies and you may have a set of users, each who have rated some subset of the movies. They’ve rated the movies one to five stars or zero to five stars. And what we would like to do is look at these users and predict how they would have rated other movies that they have not yet rated. In this video I’d like to talk about our first approach to building a recommender system. This approach is called <strong>content based recommendations</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/2.png" alt="content based recommendations"></p>
<p>Here’s our data set from before and just to remind you of a bit of notation, I was using nu to denote the number of users and so that’s equal to 4, and nm to denote the number of movies, I have 5 movies. So, how do I predict what these missing values would be? Let’s suppose that for each of these movies I have a set of features for them. In particular, let’s say that for each of the movies have two features which I’m going to denote x1 and x2. Where x1 measures the degree to which a movie is a romantic movie and x2 measures the degree to which a movie is an action movie. So, if you take a movie, Love at last, you know it’s 0.9 rating on the romance scale. This is a highly romantic movie, but zero on the action scale. So, almost no action in that movie. Romance forever is a 1.0, lot of romance and 0.01 action. I don’t know, maybe there’s a minor car crash in that movie or something. So there’s a little bit of action. Skipping one, let’s do Swords vs karate, maybe that has a 0 romance rating and no romance at all in that but plenty of action. And Nonstop car chases, maybe again there’s a tiny bit of romance in that movie but mainly action. And Cute puppies of love mainly a romance movie with no action at all. So if we have features like these, then each movie can be represented with a feature vector. Let’s take movie one. So let’s call these movies 1, 2, 3, 4, and 5. But my first movie, Love at last, I have my two features, 0.9 and 0. And so these are features x1 and x2. And let’s add an extra feature as usual, which is my interceptor feature x0 = 1. And so putting these together I would then have a feature x1. The superscript 1 denotes it’s the feature vector for my first movie, and this feature vector is equal to 1. The first 1 there is this interceptor. And then my two feature is 0.90 like so. So for Love at last I would have a feature vector x1, for the movie Romance forever I may have a software feature of vector x2, and so on, and for Swords vs karate I would have a different feature vector x superscript 5. Also, consistence with our earlier node notation that we were using, we’re going to set n to be the number of features not counting this x0 interceptor. So n is equal to 2 because it’s we have two features x1 and x2 capturing the degree of romance and the degree of action in each movie. <strong>Now in order to make predictions here’s one thing that we do which is that we could treat predicting the ratings of each user as a separate linear regression problem. So specifically, let’s say that for each user j, we’re going to learn the parameter vector theta j, which would be an R3 in this case.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/3.png" alt="problem formulation"></p>
<p>More generally, theta (j) would be an R (n+1), where n is the number of features not counting the set term. And we’re going to predict user j as rating movie i with just the inner product between parameters vectors theta and the features xi. So let’s take a specific example. Let’s take user 1, so that would be Alice. And associated with Alice would be some parameter vector theta 1. And our second user, Bob, will be associated a different parameter vector theta 2. Carol will be associated with a different parameter vector theta 3 and Dave a different parameter vector theta 4. So let’s say you want to make a prediction for what Alice will think of the movie Cute puppies of love. Well that movie is going to have some parameter vector x3 where we have that x3 is going to be equal to 1, which is my intercept term and then 0.99 and then 0. And let’s say, for this example, let’s say that we’ve somehow already gotten a parameter vector theta 1 for Alice. We’ll say it later exactly how we come up with this parameter vector. But let’s just say for now that some unspecified learning algorithm has learned the parameter vector theta 1 and is equal to this 0,5,0. So our prediction for this entry is going to be equal to theta 1, that is Alice’s parameter vector, transpose x3, that is the feature vector for the Cute puppies of love movie, number 3. And so the inner product between these two vectors is gonna be 5 times 0.99, which is equal to 4.95. And so my prediction for this value over here is going to be 4.95. And maybe that seems like a reasonable value if indeed this is my parameter vector theta 1. So, all we’re doing here is we’re applying a different copy of this linear regression for each user, and we’re saying that what Alice does is Alice has some parameter vector theta 1 that she uses, that we use to predict her ratings as a function of how romantic and how action packed a movie is. And Bob and Carol and Dave, each of them have a different linear function of the romanticness and actionness, or degree of romance and degree of action in a movie and that that’s how we’re gonna predict that their star ratings. More formally, here’s how we can write down the problem. Our notation is that r(i,j) is equal to 1 if user j has rated movie i and y(i,j) is the rating of that movie, if that rating exists. That is, if that user has actually rated that movie. And, on the previous slide we also defined these, theta j, which is a parameter for the user xi, which is a feature vector for a specific movie. And for each user and each movie, we predict that rating as follows. So let me introduce just temporarily introduce one extra bit of notation mj. We’re gonna use mj to denote the number of users rated by movie j. We don’t need this notation only for this line. Now in order to learn the parameter vector for theta j, well how do we do so. This is basically a linear regression problem. So what we can do is just choose a parameter vector theta j so that the predicted values here are as close as possible to the values that we observed in our training sets and the values we observed in our data. So let’s write that down. In order to learn the parameter vector theta j, let’s minimize over the parameter vector theta j of sum, and I want to sum over all movies that user j has rated. So we write it as sum over all values of i. That’s a :r(i,j) equals 1. So the way to read this summation syntax is this is summation over all the values of i, so the r(i.j) is equal to 1. So you’ll be summing over all the movies that user j has rated. And then I’m going to compute theta j, transpose x i. So that’s the prediction of using j’s rating on movie i,- y (i,j). So that’s the actual observed rating squared. And then, let me just divide by the number of movies that user j has actually rated. So let’s just divide by 1 over 2m j. And so this is just like the least squares regressions. It’s just like linear regression, where we want to choose the parameter vector theta j to minimize this type of squared error term. And if you want, you can also add in irregularization terms so plus lambda over 2m and this is really 2mj because we have mj examples. User j has rated that many movies, it’s not like we have that many data points with which to fit the parameters of theta j. And then let me add in my usual regularization term here of theta j k squared. As usual, this sum is from k equals 1 through n, so here, theta j is going to be an n plus 1 dimensional vector, where in our early example n was equal to 2. But more broadly, more generally n is the number of features we have per movie. And so as usual we don’t regularize over theta 0. We don’t regularize over the bias terms. The sum is from k equals 1 through n. So if you minimize this as a function of theta j you get a good solution, you get a pretty good estimate of a parameter vector theta j with which to make predictions for user j’s movie ratings. For recommender systems, I’m gonna change this notation a little bit. <strong>So to simplify the subsequent math, I with to get rid of this term mj. So that’s just a constant, right? So I can delete it without changing the value of theta j that I get out of this optimization. So if you imagine taking this whole equation, taking this whole expression and multiplying it by mj, get rid of that constant.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/4.png" alt="optimization objective"><br>And when I minimize this, I should still get the same value of theta j as before. So just to repeat what we wrote on the previous slide, here’s our optimization objective. In order to learn theta j which is the parameter for user j, we’re going to minimize over theta j of this optimization objectives. So this is our usual squared error term and then this is our regularizations term. Now of course in building a recommender system, we don’t just want to learn parameters for a single user. We want to learn parameters for all of our users. I have n subscript u users, so I want to learn all of these parameters. And so, what I’m going to do is take this optimization objective and just add the mixture summation there. So this expression here with the one half on top of this is exactly the same as what we had on top. Except that now instead of just doing this for a specific user theta j, I’m going to sum my objective over all of my users and then minimize this overall optimization objective, minimize this overall cost on. And when I minimize this as a function of theta 1, theta 2, up to theta nu, I will get a separate parameter vector for each user. And I can then use that to make predictions for all of my users, for all of my n subscript users. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/5.png" alt="optimization algorithm"><br>So putting everything together, this was our optimization objective on top. And to give this thing a name, I’ll just call this J(theta1, …, theta nu). So j as usual is my optimization objective, which I’m trying to minimize. Next, in order to actually do the minimization, if you were to derive the gradient descent update, these are the equations that you would get. So you take theta j, k, and subtract from an alpha, which is the learning rate, times these terms over here on the right. So there’s slightly different cases when k equals 0 and when k does not equal 0. Because our regularization term here regularizes only the values of theta jk for k not equal to 0, so we don’t regularize theta 0, so with slightly different updates when k equals 0 and k is not equal to 0. And this term over here, for example, is just the partial derivative with respect to your parameter, that of your optimization objective. Right and so this is just gradient descent and I’ve already computed the derivatives and plugged them into here. And if this gradient descent update look a lot like what we have here for linear regression. That’s because these are essentially the same as linear regression. The only minor difference is that for linear regression we have these 1 over m terms, this really would’ve been 1 over mj. But because earlier when we are deriving the optimization objective, we got rid of this, that’s why we don’t have this 1 over m term. But otherwise, it’s really some of my training examples of the ever times xk plus that regularization term, plus that term of regularization contributes to the derivative. And so if you’re using gradient descent here’s how you can minimize the cost function j to learn all the parameters. And using these formulas for the derivative if you want, you can also plug them into a more advanced optimization algorithm, like conjugate gradient or LBFGS or what have you. And use that to try to minimize the cost function j as well. </p>
<p>So hopefully you now know how you can apply essentially a deviation on linear regression in order to predict different movie ratings by different users. This particular algorithm is called a content based recommendations, or a content based approach, <strong>because we assume that we have available to us features for the different movies. And so where features that capture what is the content of these movies, of how romantic is this movie, how much action is in this movie. And we’re really using features of a content of the movies to make our predictions. But for many movies, we don’t actually have such features. Or maybe very difficult to get such features</strong> for all of our movies, for all of whatever items we’re trying to sell. And so, in the next video, we’ll start to talk about an approach to recommender systems that isn’t content based and does not assume that we have someone else giving us all of these features for all of the movies in our data set. </p>
<h4 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h4><p><strong>Content Based Recommendations</strong><br>We can introduce two features, $x_1$ and $x_2$ which represents how much romance or how much action a movie may have (on a scale of 0−1).<br>One approach is that we could do linear regression for every single user. For each user j, learn a parameter $\theta^{(j)} \in \mathbb{R}^3$. Predict user j as rating movie i with $(\theta^{(j)})^Tx^{(i)}$ stars.<br>$\theta^{(j)} =$ parameter vector for user j<br>$x^{(i)} =$ feature vector for movie i<br>For user j, movie i, predicted rating: $(\theta^{(j)})^T(x^{(i)})$<br>$m^{(j)} =$ number of movies rated by user j<br>To learn $\theta^{(j)}$, we do the following<br>$$min_{\theta^{(j)}} = \dfrac{1}{2}\displaystyle \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{k=1}^n(\theta_k^{(j)})^2$$<br>This is our familiar linear regression. The base of the first summation is choosing all i such that $r(i,j) = 1$.<br>To get the parameters for all our users, we do the following:<br>$$min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2$$<br>We can apply our linear regression gradient descent update using the above cost function.<br>The only real difference is that we eliminate the constant $\dfrac{1}{m}$. </p>
<h2 id="02-collaborative-filtering"><a href="#02-collaborative-filtering" class="headerlink" title="02_collaborative-filtering"></a>02_collaborative-filtering</h2><h3 id="01-collaborative-filtering"><a href="#01-collaborative-filtering" class="headerlink" title="01_collaborative-filtering"></a>01_collaborative-filtering</h3><p>In this video we’ll talk about an approach to building a recommender system that’s called <strong>collaborative filtering</strong>. The algorithm that we’re talking about has a very interesting property that it does what is called <strong>feature learning</strong> and by that I mean that this will be an algorithm that can start to learn for itself what features to use.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/6.png" alt="problem motivation 1"></p>
<p>Here was the data set that we had and we had assumed that for each movie, someone had come and told us how romantic that movie was and how much action there was in that movie. But as you can imagine <strong>it can be very difficult and time consuming and expensive to actually try to get someone to, you know, watch each movie and tell you how romantic each movie and how action packed is each movie,</strong> and often you’ll want even more features than just these two. So where do you get these features from? So let’s change the problem a bit and suppose that we have a data set where we do not know the values of these features. So we’re given the data set of movies and of how the users rated them, but we have no idea how romantic each movie is and we have no idea how action packed each movie is so I’ve replaced all of these things with question marks. But now let’s make a slightly different assumption. Let’s say we’ve gone to each of our users, and each of our users has told has told us how much they like the romantic movies and how much they like action packed movies. So Alice has associated a current of theta 1. Bob theta 2. Carol theta 3. Dave theta 4. And let’s say we also use this and that Alice tells us that she really likes romantic movies and so there’s a five there which is the multiplier associated with X1 and lets say that Alice tells us she really doesn’t like action movies and so there’s a 0 there. And Bob tells us something similar so we have theta 2 over here. Whereas Carol tells us that she really likes action movies which is why there’s a 5 there, that’s the multiplier associated with X2, and remember there’s also X0 equals 1 and let’s say that Carol tells us she doesn’t like romantic movies and so on, similarly for Dave. So let’s assume that somehow we can go to users and each user J just tells us what is the value of theta J for them. And so basically specifies to us of how much they like different types of movies. If we can get these parameters theta from our users then it turns out that it becomes possible to try to infer what are the values of x1 and x2 for each movie. Let’s look at an example. Let’s look at movie 1. So that movie 1 has associated with it a feature vector x1. And you know this movie is called Love at last but let’s <strong>ignore</strong> that. Let’s pretend we don’t know what this movie is about, so let’s ignore the title of this movie. All we know is that Alice loved this move. Bob loved this movie. Carol and Dave hated this movie. So what can we infer? Well, we know from the feature vectors that Alice and Bob love romantic movies because they told us that there’s a 5 here. Whereas Carol and Dave, we know that they hate romantic movies and that they love action movies. So because those are the parameter vectors that you know, uses 3 and 4, Carol and Dave, gave us. And so based on the fact that movie 1 is loved by Alice and Bob and hated by Carol and Dave, we might reasonably conclude that this is probably a romantic movie, it is probably not much of an action movie. this example is a little bit mathematically simplified but what we’re really asking is what feature vector should X1 be so that theta 1 transpose x1 is approximately equal to 5, that’s Alice’s rating, and theta 2 transpose x1 is also approximately equal to 5, and theta 3 transpose x1 is approximately equal to 0, so this would be Carol’s rating, and theta 4 transpose X1 is approximately equal to 0. And from this it looks like, you know, X1 equals one that’s the intercept term, and then 1.0, 0.0, that makes sense given what we know of Alice, Bob, Carol, and Dave’s preferences for movies and the way they rated this movie. And so more generally, we can go down this list and try to figure out what might be reasonable features for these other movies as well. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/7.png" alt="problem motivation 2"></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/8.png" alt="Optimization Algorithm"></p>
<p>Let’s formalize this problem of learning the features XI. Let’s say that our users have given us their preferences. So let’s say that our users have come and, you know, told us these values for theta 1 through theta of NU and we want to learn the feature vector XI for movie number I. What we can do is therefore pose the following optimization problem. So we want to sum over all the indices J for which we have a rating for movie I because we’re trying to learn the features for movie I that is this feature vector XI. So and then what we want to do is minimize this squared error, so we want to choose features XI, so that, you know, the predictive value of how user J rates movie I will be similar, will be not too far in the squared error sense of the actual value YIJ that we actually observe in the rating of user j on movie I. So, just to summarize what this term does is it tries to choose features XI so that for all the users J that have rated that movie, the algorithm also predicts a value for how that user would have rated that movie that is not too far, in the squared error sense, from the actual value that the user had rated that movie. So that’s the squared error term. As usual, we can also add this sort of regularization term to prevent the features from becoming too big. So this is how we would learn the features for one specific movie but what we want to do is learn all the features for all the movies and so what I’m going to do is add this extra summation here so I’m going to sum over all Nm movies, N subscript m movies, and minimize this objective on top that sums of all movies. And if you do that, you end up with the following optimization problem. And if you minimize this, you have hopefully a reasonable set of features for all of your movies. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/9.png" alt="Collaborative filtering"></p>
<p>So putting everything together, what we, the algorithm we talked about in the previous video and the algorithm that we just talked about in this video. In the previous video, what we showed was that you know, if you have a set of movie ratings, so if you have the data the rij’s and then you have the yij’s that will be the movie ratings. Then given features for your different movies we can learn these parameters theta. So if you knew the features, you can learn the parameters theta for your different users. And what we showed earlier in this video is that if your users are willing to give you parameters, then you can estimate features for the different movies. So this is kind of a chicken and egg problem. Which comes first? You know, do we want if we can get the thetas, we can know the Xs. If we have the Xs, we can learn the thetas. And what you can do is, and then this actually works, what you can do is in fact randomly guess some value of the thetas. Now based on your initial random guess for the thetas, you can then go ahead and use the procedure that we just talked about in order to learn features for your different movies. Now given some initial set of features for your movies you can then use this first method that we talked about in the previous video to try to get an even better estimate for your parameters theta. Now that you have a better setting of the parameters theta for your users, we can use that to maybe even get a better set of features and so on. We can sort of keep iterating, going back and forth and optimizing theta, x theta, x theta, nd this actually works and if you do this, this will actually cause your album to converge to a reasonable set of features for you movies and a reasonable set of parameters for your different users. So this is a basic collaborative filtering algorithm. This isn’t actually the final algorithm that we’re going to use. In the next video we are going to be able to improve on this algorithm and make it quite a bit more computationally efficient. But, hopefully this gives you a sense of how you can formulate a problem where you can simultaneously learn the parameters and simultaneously learn the features from the different movies. And for this problem, for the recommender system problem, this is possible only because each user rates multiple movies and hopefully each movie is rated by multiple users. And so you can do this back and forth process to estimate theta and x.  </p>
<p>So to summarize, in this video we’ve seen an initial collaborative filtering algorithm. The term collaborative filtering refers to the observation that when you run this algorithm with a large set of users, what all of these users are effectively doing are sort of collaboratively–or collaborating to get better movie ratings for everyone because with every user rating some subset with the movies, every user is helping the algorithm a little bit to learn better features, and then by helping– by rating a few movies myself, I will be helping the system learn better features and then these features can be used by the system to make better movie predictions for everyone else. And so there is a sense of collaboration where every user is helping the system learn better features for the common good. This is this collaborative filtering. </p>
<p>And, in the next video what we going to do is take the ideas that have worked out, and try to develop a better an even better algorithm, a slightly better technique for collaborative filtering. </p>
<h4 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h4><p>It can be very difficult to find features such as “amount of romance” or “amount of action” in a movie. To figure this out, we can use feature finders .<br>We can let the users tell us how much they like the different genres, providing their parameter vector immediately for us.<br>To infer the features from given parameters, we use the squared error function with regularization over all the users:<br>$$min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2$$<br>You can also randomly guess the values for theta to guess the features repeatedly. You will actually converge to a good set of features. </p>
<h3 id="02-collaborative-filtering-algorithm"><a href="#02-collaborative-filtering-algorithm" class="headerlink" title="02_collaborative-filtering-algorithm"></a>02_collaborative-filtering-algorithm</h3><p>In the last couple videos, we talked about the ideas of how, <strong>first, if you’re given features for movies, you can use that to learn parameters data for users. And second, if you’re given parameters for the users, you can use that to learn features for the movies.</strong> </p>
<p>In this video we’re going to take those ideas and put them together to come up with a <strong>collaborative filtering algorithm</strong>. So one of the things we worked out earlier is that if you have features for the movies then you can solve this minimization problem to find the parameters theta for your users. And then we also worked that out, if you are given the parameters theta, you can also use that to estimate the features x, and you can do that by solving this minimization problem. So one thing you could do is actually go back and forth. Maybe randomly initialize the parameters and then solve for theta, solve for x, solve for theta, solve for x. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/10.png" alt="Collaborative filtering optimization objective:"></p>
<p>But, it turns out that there is a more efficient algorithm that doesn’t need to go back and forth between the x’s and the thetas, but that can solve for theta and x simultaneously. And here it is. What we are going to do, is basically take both of these optimization objectives, and put them into the same objective. So I’m going to define the new optimization objective j, which is a cost function, that is a function of my features x and a function of my parameters theta. And, it’s basically the two optimization objectives I had on top, but I put together. So, in order to explain this, first, I want to point out that this term over here, this squared error term, is the same as this squared error term and the summations look a little bit different, but let’s see what the summations are really doing. The first summation is sum over all users J and then sum over all movies rated by that user. So, this is really summing over all pairs IJ, that correspond to a movie that was rated by a user. Sum over J says, for every user, the sum of all the movies rated by that user. This summation down here, just does things in the opposite order. This says for every movie I, sum over all the users J that have rated that movie and so, you know these summations, both of these are just summations over all pairs ij for which r of i J is equal to 1. It’s just something over all the user movie pairs for which you have a rating. and so those two terms up there is just exactly this first term, and I’ve just written the summation here explicitly, where I’m just saying the sum of all pairs IJ, such that RIJ is equal to 1. So what we’re going to do is define a <strong>combined optimization objective</strong> that we want to minimize in order to solve simultaneously for x and theta. And then the other terms in the optimization objective are this, which is a regularization in terms of theta. So that came down here and the final piece is this term which is my optimization objective for the x’s and that became this. <strong>And this optimization objective j actually has an interesting property that if you were to hold the x’s constant and just minimize with respect to the thetas then you’d be solving exactly this problem, whereas if you were to do the opposite, if you were to hold the thetas constant, and minimize j only with respect to the x’s, then it becomes equivalent to this. Because either this term or this term is constant if you’re minimizing only the respective x’s or only respective thetas.</strong> </p>
<p>So here’s an optimization objective that puts together my cost functions in terms of x and in terms of theta. And in order to come up with just one optimization problem, what we’re going to do, is treat this cost function, as a function of my features x and of my user pro user parameters data and just minimize this whole thing, as a function of both the Xs and a function of the thetas. And really the only difference between this and the older algorithm is that, instead of going back and forth, previously we talked about minimizing with respect to theta then minimizing with respect to x, whereas minimizing with respect to theta, minimizing with respect to x and so on. In this new version instead of sequentially going between the 2 sets of parameters x and theta, what we are going to do is just minimize with respect to both sets of parameters simultaneously. </p>
<p>Finally one last detail is that when we’re learning the features this way. <strong>Previously we have been using this convention that we have a feature x0 equals one that corresponds to an interceptor. When we are using this sort of formalism where we’re are actually learning the features, we are actually going to do away with this convention.</strong> And so the features we are going to learn x, will be in Rn. Whereas previously we had features x and Rn + 1 including <strong>the intercept term</strong>. By getting rid of x0 we now just have x in Rn. And so similarly, because the parameters theta is in the same dimension, we now also have theta in RN because if there’s no x0, then there’s no need parameter theta 0 as well. And the reason we do away with this convention is because we’re now learning all the features, right? <strong>So there is no need to hard code the feature that is always equal to one. Because if the algorithm really wants a feature that is always equal to 1, it can choose to learn one for itself. So if the algorithm chooses, it can set the feature X1 equals 1. So there’s no need to hard code the feature of 001, the algorithm now has the flexibility to just learn it by itself.</strong></p>
<p>$$J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2$$ </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/11.png" alt="Collaborative filtering algorithm"></p>
<p>So, putting everything together, here is our collaborative filtering algorithm. <strong>First</strong> we are going to initialize x and theta to small random values. And this is a little bit like neural network training, where there we were also initializing all the parameters of a neural network to small random values. <strong>Next</strong> we’re then going to minimize the cost function using great descent or one of the advance optimization algorithms. So, if you take derivatives you find that the great descent like these and so this term here is the partial derivative of the cost function, I’m not going to write that out, with respect to the feature value Xik and similarly this term here is also a partial derivative value of the cost function with respect to the parameter theta that we’re minimizing. And just as a reminder, in this formula that we no longer have this X0 equals 1 and so we have that x is in Rn and theta is a Rn. In this new formalism, we’re regularizing every one of our perimeters theta, you know, every one of our parameters Xn. There’s no longer the special case theta zero, which was regularized differently, or which was not regularized compared to the parameters theta 1 down to theta. So there is now no longer a theta 0, which is why in these updates, I did not break out a special case for k equals 0. So we then use gradient descent to minimize the cost function j with respect to the features x and with respect to the parameters theta. And <strong>finally</strong>, given a user, if a user has some parameters, theta, and if there’s a movie with some sort of learned features x, we would then predict that that movie would be given a star rating by that user of theta transpose j. Or just to fill those in, then we’re saying that if user J has not yet rated movie I, then what we do is predict that user J is going to rate movie I according to theta J transpose Xi. </p>
<p>So that’s the collaborative filtering algorithm and if you implement this algorithm you actually get a pretty decent algorithm that will simultaneously learn good features for hopefully all the movies as well as learn parameters for all the users and hopefully give pretty good predictions for how different users will rate different movies that they have not yet rated.</p>
<h4 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h4><p>To speed things up, we can simultaneously minimize our features and our parameters:<br>$$J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2$$<br>It looks very complicated, but we’ve <strong>only combined the cost function for theta and the cost function for x</strong>.<br>Because the algorithm can learn them itself, <strong>the bias units where $x_0=1$ have been removed,</strong> therefore $x∈ℝ^n$ and $θ∈ℝ^n$.<br>These are the steps in the algorithm: </p>
<ol>
<li>Initialize $x^{(i)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)}$ to small <strong>random</strong> values. This serves to break symmetry and ensures that the algorithm learns features $x^{(i)},…,x^{(n_m)}$ that are different from each other. </li>
<li>Minimize $J(x^{(i)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})$ using gradient descent (or an advanced optimization algorithm).E.g. for every $j=1,…,n_u,i=1,…n_m$:<br>$$x_k^{(i)} := x_k^{(i)} - \alpha\left (\displaystyle \sum_{j:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \theta_k^{(j)}} + \lambda x_k^{(i)} \right)$$<br>$$\theta_k^{(j)} := \theta_k^{(j)} - \alpha\left (\displaystyle \sum_{i:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}} + \lambda \theta_k^{(j)} \right)$$ </li>
<li>For a user with parameters θ and a movie with (learned) features x, predict a star rating of $\theta^Tx$. </li>
</ol>
<h2 id="03-low-rank-matrix-factorization"><a href="#03-low-rank-matrix-factorization" class="headerlink" title="03_low-rank-matrix-factorization"></a>03_low-rank-matrix-factorization</h2><h3 id="01-vectorization-low-rank-matrix-factorization"><a href="#01-vectorization-low-rank-matrix-factorization" class="headerlink" title="01_vectorization-low-rank-matrix-factorization"></a>01_vectorization-low-rank-matrix-factorization</h3><p>In the last few videos, we talked about a collaborative filtering algorithm. In this video I’m going to say a little bit about <strong>the vectorization implementation of this algorithm</strong>. And also talk a little bit about other things you can do with this algorithm. </p>
<p>For example, one of the things you can do is, given one product can you find other products that are related to this so that for example, a user has recently been looking at one product. Are there other related products that you could recommend to this user? So let’s see what we could do about that. What I’d like to do is work out an alternative way of writing out the predictions of the collaborative filtering algorithm. To start, here is our data set with our five movies and what I’m going to do is take all the ratings by all the users and group them into a matrix. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/12.png" alt="data set of five movies"></p>
<p>So, here we have five movies and four users, and so this matrix y is going to be a 5 by 4 matrix. It’s just you know, taking all of the elements, all of this data. Including question marks, and grouping them into this matrix. And of course the elements of this matrix of the (i, j) element of this matrix is really what we were previously writing as y superscript i, j. It’s the rating given to movie i by user j.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/13.png" alt="the vectorization implementation of prediction algorithm"></p>
<p>Given this matrix y of all the ratings that we have, there’s an alternative way of writing out all the predictive ratings of the algorithm. And, in particular if you look at what a certain user predicts on a certain movie, what user j predicts on movie i is given by this formula. And so, if you have a matrix of the predicted ratings, what you would have is the following matrix where the i, j entry. So this corresponds to the rating that we predict using j will give to movie i is exactly equal to that theta j transpose XI, and so, you know, this is a matrix where this first element the one-one element is a predictive rating of user one or movie one and this element, this is the one-two element is the predicted rating of user two on movie one, and so on, and this is the predicted rating of user one on the last movie and if you want, you know, this rating is what we would have predicted for this value and this rating is what we would have predicted for that value, and so on. Now, given this matrix of predictive ratings there is then a simpler or vectorized way of writing these out. In particular if I define the matrix x, and this is going to be just like the matrix we had earlier for linear regression to be sort of x1 transpose x2 transpose down to x of nm transpose. So I’m take all the features for my movies and stack them in rows. So if you think of each movie as one example and stack all of the features of the different movies and rows. And if we also to find a matrix capital theta, and what I’m going to do is take each of the per user parameter vectors, and stack them in rows, like so. So that’s theta 1, which is the parameter vector for the first user. And, you know, theta 2, and so, you must stack them in rows like this to define a matrix capital theta and so I have nu parameter vectors all stacked in rows like this. Now given this definition for the matrix x and this definition for the matrix theta in order to have a vectorized way of computing the matrix of all the predictions you can just compute x times the matrix theta transpose, and that gives you a vectorized way of computing this matrix over here. To give the collaborative filtering algorithm that you’ve been using another name. The algorithm that we’re using is also called low rank matrix factorization. And so if you hear people talk about low rank matrix factorization that’s essentially exactly the algorithm that we have been talking about. And this term comes from the property that this matrix x times theta transpose has a mathematical property in linear algebra called that this is a low rank matrix and so that’s what gives rise to this name low rank matrix factorization for these algorithms, because of this low rank property of this matrix x theta transpose. In case you don’t know what low rank means or in case you don’t know what a low rank matrix is, don’t worry about it. You really don’t need to know that in order to use this algorithm. But if you’re an expert in linear algebra, that’s what gives this algorithm, this other name of low rank matrix factorization. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/14.png" alt="find related products"></p>
<p>Finally, having run the collaborative filtering algorithm here’s something else that you can do which is use the learned features in order to find related movies. Specifically for each product i really for each movie i, we’ve learned a feature vector xi. So, you know, when you learn a certain features without really know that can the advance what the different features are going to be, but if you run the algorithm and perfectly the features will tend to capture what are the important aspects of these different movies or different products or what have you. What are the important aspects that cause some users to like certain movies and cause some users to like different sets of movies. So maybe you end up learning a feature, you know, where x1 equals romance, x2 equals action similar to an earlier video and maybe you learned a different feature x3 which is a degree to which this is a comedy. Then some feature x4 which is, you know, some other thing. And you have N features all together and after you have learned features it’s actually often pretty difficult to go in to the learned features and come up with a human understandable interpretation of what these features really are. But in practice, you know, the features even though these features can be hard to visualize. It can be hard to figure out just what these features are. Usually, it will learn features that are very meaningful for capturing whatever are the most important or the most salient properties of a movie that causes you to like or dislike it. And so now let’s say we want to address the following problem. Say you have some specific movie i and you want to find other movies j that are related to that movie. And so well, why would you want to do this? Right, maybe you have a user that’s browsing movies, and they’re currently watching movie j, than what’s a reasonable movie to recommend to them to watch after they’re done with movie j? Or if someone’s recently purchased movie j, well, what’s a different movie that would be reasonable to recommend to them for them to consider purchasing. So, now that you have learned these feature vectors, this gives us a very convenient way to measure how similar two movies are. In particular, movie i has a feature vector xi. and so if you can find a different movie, j, so that the distance between xi and xj is small, then this is a pretty strong indication that, you know, movies j and i are somehow similar. At least in the sense that some of them likes movie i, maybe more likely to like movie j as well. So, just to recap, if your user is looking at some movie i and if you want to find the 5 most similar movies to that movie in order to recommend 5 new movies to them, what you do is find the five movies j, with the smallest distance between the features between these different movies. And this could give you a few different movies to recommend to your user. </p>
<p>So with that, hopefully, you now <strong>know how to use a vectorized implementation to compute all the predicted ratings of all the users and all the movies, and also how to do things like use learned features to find what might be movies and what might be products that aren’t related to each other</strong>.</p>
<h4 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h4><p><strong>Vectorization: Low Rank Matrix Factorization</strong><br>Given matrices X (each row containing features of a particular movie) and Θ (each row containing the weights for those features for a given user), then the full matrix Y of all predicted ratings of all movies by all users is given simply by: $$Y = X\Theta^T$$.<br>Predicting how similar two movies i and j are can be done using the distance between their respective feature vectors x. Specifically, we are looking for a small value of $||x^{(i)} - x^{(j)}||$.</p>
<h3 id="02-implementational-detail-mean-normalization"><a href="#02-implementational-detail-mean-normalization" class="headerlink" title="02_implementational-detail-mean-normalization"></a>02_implementational-detail-mean-normalization</h3><p>By now you’ve seen all of the main pieces of the recommender system algorithm or the collaborative filtering algorithm. In this video I want to just share one last implementational detail, namely <strong>mean normalization, which can sometimes just make the algorithm work a little bit better</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/15.png" alt="Users who have not rated any movies"></p>
<p>To motivate the idea of mean normalization, let’s consider an example of where there’s a user that has not rated any movies. So, in addition to our four users, Alice, Bob, Carol, and Dave, I’ve added <strong>a fifth user, Eve, who hasn’t rated any movies</strong>. Let’s see what our collaborative filtering algorithm will do on this user. Let’s say that n is equal to 2 and so we’re going to learn two features and we are going to have to learn a parameter vector theta 5, which is going to be in R2, remember this is now vectors in Rn not Rn+1, we’ll learn the parameter vector theta 5 for our user number 5, Eve. So if we look in the first term in this optimization objective, well the user Eve hasn’t rated any movies, so there are no movies for which Rij is equal to one for the user Eve and so this first term plays no role at all in determining theta 5 because there are no movies that Eve has rated. And so the only term that effects theta 5 is this term. And so we’re saying that we want to choose vector theta 5 so that the last regularization term is as small as possible. In other words we want to minimize this lambda over 2 theta 5 subscript 1 squared plus theta 5 subscript 2 squared so that’s the component of the regularization term that corresponds to user 5, and of course if your goal is to minimize this term, then what you’re going to end up with is just theta 5 equals 0 0. Because a regularization term is encouraging us to set parameters close to 0 and if there is no data to try to pull the parameters away from 0, because this first term doesn’t effect theta 5, we just end up with theta 5 equals the vector of all zeros. And so when we go to predict how user 5 would rate any movie, we have that theta 5 transpose xi, for any i, that’s just going to be equal to zero. Because theta 5 is 0 for any value of x, this inner product is going to be equal to 0. And what we’re going to have therefore, is that we’re going to predict that Eve is going to rate every single movie with zero stars. But this doesn’t seem very useful does it? I mean if you look at the different movies, Love at Last, this first movie, a couple people rated it 5 stars. And for even the Swords vs. Karate, someone rated it 5 stars. So some people do like some movies. It seems not useful to just predict that Eve is going to rate everything 0 stars. <strong>And in fact if we’re predicting that eve is going to rate everything 0 stars, we also don’t have any good way of recommending any movies to her, because you know all of these movies are getting exactly the same predicted rating for Eve so there’s no one movie with a higher predicted rating that we could recommend to her, so, that’s not very good. The idea of mean normalization will let us fix this problem.</strong> So here’s how it works. As before let me group all of my movie ratings into this matrix Y, so just take all of these ratings and group them into matrix Y.  And this column over here of all question marks corresponds to Eve’s not having rated any movies. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/16/16.png" alt="mean normalization"><br>Now to perform <strong>mean normalization</strong> what I’m going to do is compute the average rating that each movie obtained. And I’m going to store that in a vector that we’ll call mu. So the first movie got two 5-star and two 0-star ratings, so the average of that is a 2.5-star rating. The second movie had an average of 2.5-stars and so on. And the final movie that has 0, 0, 5, 0. And the average of 0, 0, 5, 0, that averages out to an average of 1.25 rating. And what I’m going to do is look at all the movie ratings and I’m going to subtract off the mean rating. So this first element 5 I’m going to subtract off 2.5 and that gives me 2.5. And the second element 5 subtract off of 2.5, get a 2.5. And then the 0, 0, subtract off 2.5 and you get -2.5, -2.5. In other words, what I’m going to do is take my matrix of movie ratings, take this wide matrix, and subtract form each row the average rating for that movie. So, what I’m doing is just normalizing each movie to have an average rating of zero. And so just one last example. If you look at this last row, 0 0 5 0. We’re going to subtract 1.25, and so I end up with these values over here. So now and of course the question marks stay a question mark. So each movie in this new matrix Y has an average rating of 0. What I’m going to do then, is take this set of ratings and use it with my collaborative filtering algorithm. So I’m going to pretend that this was the data that I had gotten from my users, or pretend that these are the actual ratings I had gotten from the users, and I’m going to use this as my data set with which to learn my parameters theta J and my features XI - from these mean normalized movie ratings. When I want to make predictions of movie ratings, what I’m going to do is the following:  for user J on movie I, I’m gonna predict theta J transpose XI, where X and theta are the parameters that I’ve learned from this mean normalized data set. But, because on the data set, I had subtracted off the means in order to make a prediction on movie i, I’m going to need to add back in the mean, and so i’m going to add back in mu i. And so that’s going to be my prediction where in my training data subtracted off all the means and so when we make predictions and we need to add back in these means mu i for movie i.  And so specifically if you user 5 which is Eve, the same argument as the previous slide still applies in the sense that Eve had not rated any movies and so the learned parameter for user 5 is still going to be equal to 0, 0. And so what we’re going to get then is that on a particular movie i we’re going to predict for Eve theta 5, transpose xi plus add back in mu i and so this first component is going to be equal to zero, if theta five is equal to zero. And so on movie i, we are going to end a predicting mu i. And, this actually makes sense. It means that on movie 1 we’re going to predict Eve rates it 2.5. On movie 2 we’re gonna predict Eve rates it 2.5. On movie 3 we’re gonna predict Eve rates it at 2 and so on. This actually makes sense, because it says that if Eve hasn’t rated any movies and we just don’t know anything about this new user Eve, what we’re going to do is just predict for each of the movies, what are the average rating that those movies got. Finally, as an aside, in this video we talked about mean normalization, where we normalized each row of the matrix y, to have mean 0. In case you have some movies with no ratings, so it is analogous to a user who hasn’t rated anything, but in case you have some movies with no ratings, you can also play with versions of the algorithm, where you normalize the different columns to have means zero, instead of normalizing the rows to have mean zero, although that’s maybe less important, because if you really have a movie with no rating, maybe you just shouldn’t recommend that movie to anyone, anyway. And so, taking care of the case of a user who hasn’t rated anything might be more important than taking care of the case of a movie that hasn’t gotten a single rating. </p>
<p>So to summarize, <strong>that’s how you can do mean normalization as a sort of pre-processing step for collaborative filtering. Depending on your data set, this might some times make your implementation work just a little bit better</strong>.</p>
<h4 id="summary-5"><a href="#summary-5" class="headerlink" title="summary"></a>summary</h4><p>If the ranking system for movies is used from the previous lectures, then new users (who have watched no movies), will be assigned new movies incorrectly. Specifically, they will be assigned θ with all components equal to zero due to the minimization of the regularization term. That is, we assume that the new user will rank all movies 0, which does not seem intuitively correct.<br>We rectify this problem by normalizing the data relative to the mean. <strong>First</strong>, we use a <strong>matrix Y to store the data from previous ratings, where the ith row of Y is the ratings for the ith movie and the jth column corresponds to the ratings for the jth user.</strong><br>We can now define a vector $\mu = [\mu_1, \mu_2, \dots , \mu_{n_m}]$ such that $\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}$<br>Which is effectively the mean of the previous ratings for the ith movie (where only movies that have been watched by users are counted). We now can normalize the data by subtracting u, the mean rating, from the actual ratings for each user (column in matrix Y):<br>As an example, consider the following matrix Y and mean ratings μ:<br>$$Y = \begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \ 4 &amp; ? &amp; ? &amp; 0 \ 0 &amp; 0 &amp; 5 &amp; 4 \ 0 &amp; 0 &amp; 5 &amp; 0 \ \end{bmatrix}, \quad \mu = \begin{bmatrix} 2.5 \ 2 \ 2.25 \ 1.25 \ \end{bmatrix}$$<br>The <strong>resulting Y′ vector</strong> is:<br>$$Y’ = \begin{bmatrix} 2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 \ 2 &amp; ? &amp; ? &amp; -2 \ -.2.25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \ -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 \end{bmatrix}$$<br>Now we must <strong>slightly modify the linear regression prediction</strong> to include the mean normalization term:<br>$$(\theta^{(j)})^T x^{(i)} + \mu_i$$<br>Now, <strong>for a new user, the initial predicted values will be equal to the μ term instead of simply being initialized to zero, which is more accurate.</strong> </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/15/15_anomaly-detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/15/15_anomaly-detection/" class="post-title-link" itemprop="url">15_anomaly-detection note15</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-15 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-15T00:00:00+05:30">2018-01-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>76k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:09</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This personal note is written after studying the opening course on <a href="https://www.coursera.org" target="_blank" rel="noopener">the coursera website</a>, <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew NG</a> . And images, audios of this note all comes from the opening course. </p>
<h2 id="01-density-estimation"><a href="#01-density-estimation" class="headerlink" title="01_density-estimation"></a>01_density-estimation</h2><h3 id="01-problem-motivation"><a href="#01-problem-motivation" class="headerlink" title="01_problem-motivation"></a>01_problem-motivation</h3><p>In this next set of videos, I’d like to tell you about <strong>a problem called Anomaly Detection</strong>. This is a reasonably commonly use you type machine learning. And one of the interesting aspects is that it’s mainly for unsupervised problem, that there’s some aspects of it that are also very similar to sort of the supervised learning problem. So, what is anomaly detection?</p>
<h4 id="example"><a href="#example" class="headerlink" title="example"></a>example</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/1.png" alt="anomaly detection example"></p>
<p>To explain it. Let me use the motivating example of: Imagine that you’re a manufacturer of aircraft engines, and let’s say that as your aircraft engines roll off the assembly line, you’re doing, you know, QA or quality assurance testing, and as part of that testing you measure features of your aircraft engine, like maybe, you measure the heat generated, things like the vibrations and so on. I share some friends that worked on this problem a long time ago, and these were actually the sorts of features that they were collecting off actual aircraft engines so you now have a data set of X1 through Xm, if you have manufactured m aircraft engines, and if you plot your data, maybe it looks like this. So, each point here, each cross here as one of your unlabeled examples. So, the anomaly detection problem is the following. Let’s say that on, you know, the next day, you have a new aircraft engine that rolls off the assembly line and your new aircraft engine has some set of features x-test. What the anomaly detection problem is, we want to know if this aircraft engine is anomalous in any way, in other words, we want to know if, maybe, this engine should undergo further testing because, or if it looks like an okay engine, and so it’s okay to just ship it to a customer without further testing. So, if your new aircraft engine looks like a point over there, well, you know, that looks a lot like the aircraft engines we’ve seen before, and so maybe we’ll say that it looks okay. Whereas, if your new aircraft engine, if x-test, you know, were a point that were out here, so that if X1 and X2 are the features of this new example. If x-tests were all the way out there, then we would call that an anomaly. and maybe send that aircraft engine for further testing before we ship it to a customer, since it looks very different than the rest of the aircraft engines we’ve seen before. </p>
<h4 id="Desity-estimation"><a href="#Desity-estimation" class="headerlink" title="Desity_estimation"></a>Desity_estimation</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/2.png" alt="density_estimation"></p>
<p>More formally in the anomaly detection problem, we’re give some data sets, x1 through Xm of examples, and we usually assume that these end examples are normal or non-anomalous examples, and we want an algorithm to tell us if some new example x-test is anomalous. The approach that we’re going to take is that given this training set, given the unlabeled training set, we’re going to build a model for p of x. In other words, we’re going to build a model for the probability of x, where x are these features of, say, aircraft engines. And so, having built a model of the probability of x we’re then going to say that for the new aircraft engine, if p of x-test is less than some epsilon then we flag this as an anomaly. So we see a new engine that, you know, has very low probability under a model p of x that we estimate from the data, then we flag this anomaly, whereas if p of x-test is, say, greater than or equal to some small threshold. Then we say that, you know, okay, it looks okay. And so, given the training set, like that plotted here, if you build a model, hopefully you will find that aircraft engines, or hopefully the model p of x will say that points that lie, you know, somewhere in the middle, that’s pretty high probability, whereas points a little bit further out have lower probability. Points that are even further out have somewhat lower probability, and the point that’s way out here, the point that’s way out there, would be an anomaly. Whereas the point that’s way in there, right in the middle, this would be okay because p of x right in the middle of that would be very high cause we’ve seen a lot of points in that region. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/3.png" alt="Anomaly_detecttion_examples"></p>
<p>Here are some examples of applications of anomaly detection. Perhaps the most common application of anomaly detection is actually for detection if you have many users, and if each of your users take different activities, you know maybe on your website or in the physical plant or something, you can compute features of the different users activities. And what you can do is build a model to say, you know, what is the probability of different users behaving different ways. What is the probability of a particular vector of features of a users behavior so you know examples of features of a users activity may be on the website it’d be things like, maybe x1 is how often does this user log in, x2, you know, maybe the number of what pages visited, or the number of transactions, maybe x3 is, you know, the number of posts of the users on the forum, feature x4 could be what is the typing speed of the user and some websites can actually track that was the typing speed of this user in characters per second. And so you can model p of x based on this sort of data. And finally having your model p of x, you can try to identify users that are behaving very strangely on your website by checking which ones have probably effects less than epsilon and maybe send the profiles of those users for further review. Or demand additional identification from those users, or some such to guard against you know, strange behavior or fraudulent behavior on your website. This sort of technique will tend of flag the users that are behaving unusually, not just users that maybe behaving fraudulently. So not just constantly having stolen or users that are trying to do funny things, or just find unusual users. But this is actually the technique that is used by many online websites that sell things to try identify users behaving strangely that might be indicative of either fraudulent behavior or of computer accounts that have been stolen. Another example of anomaly detection is manufacturing. So, already talked about the aircraft engine thing where you can find unusual, say, aircraft engines and send those for further review. A third application would be monitoring computers in a data center. I actually have some friends who work on this too. So if you have a lot of machines in a computer cluster or in a data center, we can do things like compute features at each machine. So maybe some features capturing you know, how much memory used, number of disc accesses, CPU load. As well as more complex features like what is the CPU load on this machine divided by the amount of network traffic on this machine? Then given the dataset of how your computers in your data center usually behave, you can model the probability of x, so you can model the probability of these machines having different amounts of memory use or probability of these machines having different numbers of disc accesses or different CPU loads and so on. And if you ever have a machine whose probability of x, p of x, is very small then you know that machine is behaving unusually and maybe that machine is about to go down, and you can flag that for review by a system administrator. And this is actually being used today by various data centers to watch out for unusual things happening on their machines. So, that’s anomaly detection. In the next video, I’ll talk a bit about the Gaussian distribution and review properties of the Gaussian probability distribution, and in videos after that, we will apply it to develop an anomaly detection algorithm.</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p><strong>Problem Motivation</strong><br>Just like in other learning problems, we are given a dataset ${x^{(1)}, x^{(2)},\dots,x^{(m)}}$.<br>We are then given a new example, $x_{test}$, and we want to know whether this new example is abnormal/anomalous.<br>We define a “model” $p(x)$ that tells us the probability the example is not anomalous. We also use a threshold $ϵ$ (epsilon) as a dividing line so we can say which examples are anomalous and which are not.<br>A very common application of anomaly detection is detecting fraud: </p>
<ul>
<li>$x^{(i)} =$ features of user i’s activities </li>
<li>Model $p(x)$ from the data. </li>
<li>Identify unusual users by checking which have $p(x)&lt;ϵ$. </li>
</ul>
<p>If our anomaly detector is flagging <strong>too many</strong> anomalous examples, then we need to <strong>decrease</strong> our threshold $ϵ$ </p>
<h3 id="02-gaussian-distribution"><a href="#02-gaussian-distribution" class="headerlink" title="02_gaussian-distribution"></a>02_gaussian-distribution</h3><p>In this video, I’d like to talk about<br>the <strong>Gaussian distribution</strong> which is also called the <strong>normal distribution</strong>. In case you’re already intimately familiar with the Gaussian distribution, it’s probably okay to skip this video,but if you’re not sure or if it has been a while since you’ve worked with the Gaussian distribution or normal distribution then please do watch this video all the way to the end. And in the video after this we’ll start applying the Gaussian distribution to developing an anomaly detection algorithm.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/4.png" alt="Gaussian distribution"></p>
<p>Let’s say x is a row value’s random variable, so x is a row number. If the probability distribution of x is Gaussian with mean mu and variance sigma squared. Then, we’ll write this as x, the random variable. Tilde, this little tilde, this is distributed as. And then to denote a Gaussian distribution, sometimes I’m going to write script N parentheses mu comma sigma script. So this script N stands for normal since Gaussian and normal they mean the thing are synonyms. And the Gaussian distribution is parametarized by two parameters, by a mean parameter which we denote mu and a variance parameter which we denote via sigma squared. If we plot the Gaussian distribution or Gaussian probability density. It’ll look like the bell shaped curve which you may have seen before. And so this bell shaped curve is paramafied by those two parameters, mu and sequel. And the location of the center of this bell shaped curve is the mean mu. And the width of this bell shaped curve, roughly that, is this parameter, sigma, is also called one standard deviation, and so this specifies the probability of x taking on different values. So, x taking on values here in the middle here it’s pretty high, since the Gaussian density here is pretty high, whereas x taking on values further, and further away will be diminishing in probability. Finally just for completeness let me write out the formula for the Gaussian distribution. So the probability of x, and I’ll sometimes write this as the p (x) when we write this as P ( x ; mu, sigma squared), and so this denotes that the probability of X is parameterized by the two parameters mu and sigma squared. And the formula for the Gaussian density is this 1/ root 2 pi, sigma e (-(x-mu/g) squared/2 sigma squared. So there’s no need to memorize this formula. This is just the formula for the bell-shaped curve over here on the left. There’s no need to memorize it, and if you ever need to use this, you can always look this up. And so that figure on the left, that is what you get if you take a fixed value of mu and take a fixed value of sigma, and you plot P(x) so this curve here. This is really p(x) plotted as a function of X for a fixed value of Mu and of sigma squared. And by the way sometimes it’s easier to think in terms of sigma squared that’s called the variance. And sometimes is easier to think in terms of sigma. So sigma is called the standard deviation, and so it specifies the width of this Gaussian probability density, where as the square sigma, or sigma squared, is called the variance. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/5.png" alt="Gaussian_distribution_examples"></p>
<p>Let’s look at some examples of what the Gaussian distribution looks like. If mu equals zero, sigma equals one. Then we have a Gaussian distribution that’s centered around zero, because that’s mu and the width of this Gaussian, so that’s one standard deviation is sigma over there. Let’s look at some examples of Gaussians. If mu is equal to zero and sigma equals one, then that corresponds to a Gaussian distribution that is centered at zero, since mu is zero, and the width of this Gaussian is is controlled by sigma by that variance parameter sigma. Here’s another example. That same mu is equal to 0 and sigma is equal to .5 so the standard deviation is .5 and the variance sigma squared would therefore be the square of 0.5 would be 0.25 and in that case the Gaussian distribution, the Gaussian probability density goes like this. Is also sent as zero. But now the width of this is much smaller because the smaller the area is, the width of this Gaussian density is roughly half as wide. But because this is a probability distribution, the area under the curve, that’s the shaded area there. That area must integrate to one this is a property of probability distributing. So this is a much taller Gaussian density because this half is Y but half the standard deviation but it twice as tall. Another example is sigma is equal to 2 then you get a much fatter a much wider Gaussian density and so here the sigma parameter controls that Gaussian distribution has a wider width. And once again, the area under the curve, that is the shaded area, will always integrate to one, that’s the property of probability distributions and because it’s wider it’s also half as tall in order to still integrate to the same thing. And finally one last example would be if we now change the mu parameters as well. Then instead of being centered at 0 we now have a Gaussian distribution that’s centered at 3 because this shifts over the entire Gaussian distribution. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/6.png" alt="parameter_estimation"></p>
<p>Next, let’s talk about the Parameter estimation problem. So what’s the parameter estimation problem? Let’s say we have a dataset of m examples so exponents x m and lets say each of this example is a row number. Here in the figure I’ve plotted an example of the dataset so the horizontal axis is the x axis and either will have a range of examples of x, and I’ve just plotted them on this figure here. And the parameter estimation problem is, let’s say I suspect that these examples came from a Gaussian distribution. So let’s say I suspect that each of my examples, x i, was distributed. That’s what this tilde thing means. Let’s not suspect that each of these examples were distributed according to a normal distribution, or Gaussian distribution, with some parameter mu and some parameter sigma square. But I don’t know what the values of these parameters are. The problem of parameter estimation is, given my data set, I want to try to figure out, well I want to estimate what are the values of mu and sigma squared. So if you’re given a data set like this, it looks like maybe if I estimate what Gaussian distribution the data came from, maybe that might be roughly the Gaussian distribution it came from. With mu being the center of the distribution, sigma standing for the deviation controlling the width of this Gaussian distribution. Seems like a reasonable fit to the data. Because, you know, looks like the data has a very high probability of being in the central region, and a low probability of being further out, even though probability of being further out, and so on. So maybe this is a reasonable estimate of mu and sigma squared. That is, if it corresponds to a Gaussian distribution function that looks like this. So what I’m going to do is just write out the formula the standard formulas for estimating the parameters Mu and sigma squared. Our estimate or the way we’re going to estimate mu is going to be just the average of my example. So mu is the mean parameter. Just take my training set, take my m examples and average them. And that just means the center of this distribution. How about sigma squared? Well, the variance, I’ll just write out the standard formula again, I’m going to estimate as sum over one through m of x i minus mu squared. And so this mu here is actually the mu that I compute over here using this formula. And what the variance is, or one interpretation of the variance is that if you look at this term, that’s the square difference between the value I got in my example minus the mean. Minus the center, minus the mean of the distribution. And so in the variance I’m gonna estimate as just the average of the square differences between my examples, minus the mean. And as a side comment, only for those of you that are experts in statistics. <strong>If you’re an expert in statistics, and if you’ve heard of maximum likelihood estimation, then these parameters, these estimates, are actually the maximum likelihood estimates of the parameters of mu and sigma squared but if you haven’t heard of that before don’t worry about it, all you need to know is that these are the two standard formulas for how to figure out what are mu and Sigma squared given the data set.</strong> Finally one last side comment again only for those of you that have maybe taken the statistics class before but if you’ve taken statistics This class before. Some of you may have seen the formula here where this is M-1 instead of M so this first term becomes 1/M-1 instead of 1/M. <strong>In machine learning people tend to learn 1/M formula but in practice whether it is 1/M or 1/M-1 it makes essentially no difference assuming M is reasonably large. a reasonably large training set size.</strong> So just in case you’ve seen this other version before. In either version it works just about equally well but in machine learning most people tend to use 1/M in this formula.And the two versions have slightly different theoretical properties like these are different math properties. Bit of practice it really makes makes very little difference, if any. </p>
<p>So, hopefully you now have a good sense of what the Gaussian distribution looks like, as well as how to estimate the parameters mu and sigma squared of Gaussian distribution if you’re given a training set, that is if you’re given a set of data that you suspect comes from a Gaussian distribution with unknown parameters, mu and sigma squared. In the next video, we’ll start to take this and apply it to develop an anomaly detection algorithm. </p>
<h4 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h4><p>The Gaussian Distribution is a familiar bell-shaped curve that can be described by a function $\mathcal{N}(\mu,\sigma^2)$<br>Let x∈ℝ. If the probability distribution of x is Gaussian with mean μ, variance $\sigma^2$, then:<br>$$x \sim \mathcal{N}(\mu, \sigma^2)$$<br>The little ∼ or ‘tilde’ can be read as “distributed as.”<br>The Gaussian Distribution is parameterized by a mean and a variance.<br>Mu, or μ, describes the center of the curve, called the mean. The width of the curve is described by sigma, or σ, called the standard deviation.<br>The full function is as follows:<br>$$\large p(x;\mu,\sigma^2) = \dfrac{1}{\sigma\sqrt{(2\pi)}}e^{-\dfrac{1}{2}(\dfrac{x - \mu}{\sigma})^2}$$<br>We can estimate the parameter μ from a given dataset by simply taking the average of all the examples:<br>$$\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}$$<br>We can estimate the other parameter, $\sigma^2$, with our familiar squared error formula:<br>$$\sigma^2 = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x^{(i)} - \mu)^2$$ </p>
<h3 id="03-algorithm"><a href="#03-algorithm" class="headerlink" title="03_algorithm"></a>03_algorithm</h3><p>In the last video, we talked about the Gaussian distribution. In this video <strong>lets apply that to develop an anomaly detection algorithm.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/7.png" alt="density_estimation"></p>
<p>Let’s say that we have an unlabeled training set of M examples, and each of these examples is going to be a feature in Rn so your training set could be, feature vectors from the last M aircraft engines being manufactured. Or it could be features from m users or something else. The way we are going to address anomaly detection, is we are going to model p of x from the data sets. We’re going to try to figure out what are high probability features, what are lower probability types of features. So, x is a vector and what we are going to do is model p of x, as probability of x1, that is of the first component of x, times the probability of x2, that is the probability of the second feature, times the probability of the third feature, and so on up to the probability of the final feature of Xn. Now I’m leaving space here cause I’ll fill in something in a minute. So, how do we model each of these terms, p of X1, p of X2, and so on. What we’re going to do, is assume that the feature, X1, is distributed according to a Gaussian distribution, with some mean, which you want to write as mu1 and some variance, which I’m going to write as sigma squared 1, and so p of X1 is going to be a Gaussian probability distribution, with mean mu1 and variance sigma squared 1. And similarly I’m going to assume that X2 is distributed, Gaussian, that’s what this little tilda stands for, that means distributed Gaussian with mean mu2 and Sigma squared 2, so it’s distributed according to a different Gaussian, which has a different set of parameters, mu2 sigma square 2. And similarly, you know, X3 is yet another Gaussian, so this can have a different mean and a different standard deviation than the other features, and so on, up to XN. And so that’s my model. <strong>Just as a side comment for those of you that are experts in statistics, it turns out that this equation that I just wrote out actually corresponds to an independence assumption on the values of the features x1 through xn. But in practice it turns out that the algorithm of this fragment, it works just fine, whether or not these features are anywhere close to independent and even if independence assumption doesn’t hold true this algorithm works just fine. But in case you don’t know those terms I just used independence assumptions and so on, don’t worry about it. You’ll be able to understand it and implement this algorithm just fine and that comment was really meant only for the experts in statistics.</strong> </p>
<p>Finally, in order to wrap this up, let me take this expression and write it a little bit more compactly. So, we’re going to write this is a product from J equals one through N, of P of XJ parameterized by mu j comma sigma squared j. So this funny symbol here, there is capital Greek alphabet pi, that funny symbol there corresponds to taking the product of a set of values. And so, you’re familiar with the summation notation, so the sum from i equals one through n, of i. This means 1 + 2 + 3 plus dot dot dot, up to n. Where as this funny symbol here, this product symbol, right product from i equals 1 through n of i.  Then this means that, it’s just like summation except that we’re now multiplying. This becomes 1 times 2 times 3 times up to N. And so using this product notation, this product from j equals 1 through n of this expression. It’s just more compact, it’s just shorter way for writing out this product of of all of these terms up there. Since we’re are taking these p of x j given mu j comma sigma squared j terms and multiplying them together. And, by the way the problem of estimating this distribution p of x, they’re sometimes called the problem of density estimation. Hence the title of the slide. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/8.png" alt="anomaly_detection_algorithm"></p>
<p>So putting everything together, here is our anomaly detection algorithm. The first step is to choose features, or come up with features xi that we think might be indicative of anomalous examples. So what I mean by that, is, try to come up with features, so that when there’s an unusual user in your system that may be doing fraudulent things, or when the aircraft engine examples, you know there’s something funny, something strange about one of the aircraft engines. Choose features X I, that you think might take on unusually large values, or unusually small values, for what an anomalous example might look like. But more generally, just try to choose features that describe general properties of the things that you’re collecting data on. Next, given a training set, of M, unlabled examples, X1 through X M, we then fit the parameters, mu 1 through mu n, and sigma squared 1 through sigma squared n, and so these were the formulas similar to the formulas we have in the previous video, that we’re going to use the estimate each of these parameters, and just to give some interpretation, mu J, that’s my average value of the j feature. Mu j goes in this term p of xj. which is parametrized by mu J and sigma squared J. And so this says for the mu J just take the mean over my training set of the values of the j feature. And, just to mention, that you do this, you compute these formulas for j equals one through n. So use these formulas to estimate mu 1, to estimate mu 2, and so on up to mu n, and similarly for sigma squared, and it’s also possible to come up with vectorized versions of these. So if you think of mu as a vector, so mu if is a vector there’s mu 1, mu 2, down to mu n, then a vectorized version of that set of parameters can be written like so sum from 1 equals one through n xi. So, this formula that I just wrote out estimates this xi as the feature vectors that estimates mu for all the values of n simultaneously. And it’s also possible to come up with a vectorized formula for estimating sigma squared j. Finally, when you’re given a new example, so when you have a new aircraft engine and you want to know is this aircraft engine anomalous. What we need to do is then compute p of x, what’s the probability of this new example? So, p of x is equal to this product, and what you implement, what you compute, is this formula and where over here, this thing here this is just the formula for the Gaussian probability, so you compute this thing, and finally if this probability is very small, then you flag this thing as an anomaly. Here’s an example of an application of this method. Let’s say we have this data set plotted on the upper left of this slide. if you look at this, well, lets look the feature of x1. If you look at this data set, it looks like on average, the features x1 has a mean of about 5 and the standard deviation, if you only look at just the x1 values of this data set has the standard deviation of maybe 2. So that sigma 1 and looks like x2 the values of the features as measured on the vertical axis, looks like it has an average value of about 3, and a standard deviation of about 1. So if you take this data set and if you estimate mu1, mu2, sigma1, sigma2, this is what you get. And again, I’m writing sigma here, I’m think about standard deviations, but the formula on the previous 5 actually gave the estimates of the squares of theses things, so sigma squared 1 and sigma squared 2. So, just be careful whether you are using sigma 1, sigma 2, or sigma squared 1 or sigma squared 2. So, sigma squared 1 of course would be equal to 4, for example, as the square of 2. And in pictures what p of x1 parametrized by mu1 and sigma squared 1 and p of x2, parametrized by mu 2 and sigma squared 2, that would look like these two distributions over here. And, turns out that if were to plot of p of x, right, which is the product of these two things, you can actually get a surface plot that looks like this. This is a plot of p of x, where the height above of this, where the height of this surface at a particular point, so given a particular x1 x2 values of x2 if x1 equals 2, x equal 2, that’s this point. And the height of this 3-D surface here, that’s p of x. So p of x, that is the height of this plot, is literally just p of x1 parametrized by mu 1 sigma squared 1, times p of x2 parametrized by mu 2 sigma squared 2. Now, so this is how we fit the parameters to this data. Let’s see if we have a couple of new examples. Maybe I have a new example there. Is this an anomaly or not? Or, maybe I have a different example, maybe I have a different second example over there. So, is that an anomaly or not? They way we do that is, we would set some value for Epsilon, let’s say I’ve chosen Epsilon equals 0.02. I’ll say later how we choose Epsilon. But let’s take this first example, let me call this example X1 test. And let me call the second example X2 test. What we do is, we then compute p of X1 test, so we use this formula to compute it and this looks like a pretty large value. In particular, this is greater than, or greater than or equal to epsilon. And so this is a pretty high probability at least bigger than epsilon, so we’ll say that X1 test is not an anomaly. Whereas, if you compute p of X2 test, well that is just a much smaller value. So this is less than epsilon and so we’ll say that that is indeed an anomaly, because it is much smaller than that epsilon that we then chose. And in fact, I’d improve it here. What this is really saying is that, you look through the 3d surface plot. It’s saying that all the values of x1 and x2 that have a high height above the surface, corresponds to an a non-anomalous example of an OK or normal example. Whereas all the points far out here, all the points out here, all of those points have very low probability, so we are going to flag those points as anomalous, and so it’s gonna define some region, that maybe looks like this, so that everything outside this, it flags as anomalous, whereas the things inside this ellipse I just drew, if it considers okay, or non-anomalous, not anomalous examples. And so this example x2 test lies outside that region, and so it has very small probability, and so we consider it an anomalous example. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/9.png" alt="Anomaly_detecttion_algorithm_example"></p>
<p>In this video we talked about how to estimate p of x, the probability of x, for the purpose of developing an anomaly detection algorithm. And in this video, we also stepped through an entire process of giving data set, we have, fitting the parameters, doing parameter estimations. We get mu and sigma parameters, and then taking new examples and deciding if the new examples are anomalous or not. In the next few videos we will delve deeper into this algorithm, and talk a bit more about how to actually get this to work well.</p>
<h4 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h4><p>Given a training set of examples, $\lbrace x^{(1)},\dots,x^{(m)}\rbrace$ where each example is a vector, $x \in \mathbb{R}^n$.<br>$$p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)$$<br>In statistics, this is called an “independence assumption” on the values of the features inside training example x.<br>More compactly, the above expression can be written as follows:<br>$$= \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)$$<br><strong>The algorithm</strong><br>Choose features $x_i$ that you think might be indicative of anomalous examples.<br>Fit parameters $$\mu_1,\dots,\mu_n,\sigma_1^2,\dots,\sigma_n^2$$<br>Calculate $\mu_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x_j^{(i)}$<br>Calculate $\sigma^2_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x_j^{(i)} - \mu_j)^2$<br>Given a new example x, compute p(x):<br>$$p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})$$<br>Anomaly if p(x)&lt;ϵ<br>A vectorized version of the calculation for μ is $\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}$. You can vectorize $\sigma^2$ similarly. </p>
<h2 id="02-building-an-anomaly-detection-system"><a href="#02-building-an-anomaly-detection-system" class="headerlink" title="02_building-an-anomaly-detection-system"></a>02_building-an-anomaly-detection-system</h2><h3 id="01-developing-and-evaluating-an-anomaly-detection-system"><a href="#01-developing-and-evaluating-an-anomaly-detection-system" class="headerlink" title="01_developing-and-evaluating-an-anomaly-detection-system"></a>01_developing-and-evaluating-an-anomaly-detection-system</h3><p>In the last video, we developed an anomaly detection algorithm. In this video, I like to talk about the process of how to go about developing a specific application of anomaly detection to a problem and in particular this will focus on <strong>the problem of how to evaluate an anomaly detection algorithm</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/10.png" alt="The_importance_of_real-number_evaluation"></p>
<p>In previous videos, we’ve already talked about the importance of real number evaluation and this captures the idea that when you’re trying to develop a learning algorithm for a specific application, you need to often make a lot of choices like, you know, choosing what features to use and then so on. And making decisions about all of these choices is often much easier, and if you have a way to evaluate your learning algorithm that just gives you back a number. So if you’re trying to decide, you know, I have an idea for one extra feature, do I include this feature or not. If you can run the algorithm with the feature, and run the algorithm without the feature, and just get back a number that tells you, you know, did it improve or worsen performance to add this feature? Then it gives you a much better way, a much simpler way, with which to decide whether or not to include that feature. So in order to be able to develop an anomaly detection system quickly, it would be a really helpful to have a way of evaluating an anomaly detection system. In order to do this, in order to evaluate an anomaly detection system, we’re actually going to assume have some labeled data. So, so far, we’ll be treating anomaly detection as an unsupervised learning problem, using unlabeled data. But if you have some labeled data that specifies what are some anomalous examples, and what are some non-anomalous examples, then this is how we actually think of as the standard way of evaluating an anomaly detection algorithm. So taking the aircraft engine example again. Let’s say that, you know, we have some label data of just a few anomalous examples of some aircraft engines that were manufactured in the past that turns out to be anomalous. Turned out to be flawed or strange in some way. Let’s say we use we also have some non-anomalous examples, so some perfectly okay examples. I’m going to use y equals 0 to denote the normal or the non-anomalous example and y equals 1 to denote the anomalous examples. The process of developing and evaluating an anomaly detection algorithm is as follows. We’re going to think of it as a training set and talk about the cross validation in test sets later, but the training set we usually think of this as still the unlabeled training set. And so this is our large collection of normal, non-anomalous or not anomalous examples. And usually we think of this as being as non-anomalous, but it’s actually okay even if a few anomalies slip into your unlabeled training set. And next we are going to define a cross validation set and a test set, with which to evaluate a particular anomaly detection algorithm. So, specifically, for both the cross validation test sets we’re going to assume that, you know, we can include a few examples in the cross validation set and the test set that contain examples that are known to be anomalous. So the test sets say we have a few examples with y equals 1 that correspond to anomalous aircraft engines. So here’s a specific example. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/11.png" alt="Aircraft_engines_motivating_example"></p>
<p>Let’s say that, altogether, this is the data that we have. We have manufactured 10,000 examples of engines that, as far as we know we’re perfectly normal, perfectly good aircraft engines. And again, it turns out to be okay even if a few flawed engine slips into the set of 10,000 is actually okay, but we kind of assumed that the vast majority of these 10,000 examples are, you know, good and normal non-anomalous engines. And let’s say that, you know, historically, however long we’ve been running on manufacturing plant, let’s say that we end up getting features, getting 24 to 28 anomalous engines as well. And for a pretty typical application of anomaly detection, you know, the number non-anomalous examples, that is with y equals 1, we may have anywhere from, you know, 20 to 50. It would be a pretty typical range of examples, number of examples that we have with y equals 1. And usually we will have a much larger number of good examples. So, given this data set, a fairly typical way to split it into the training set, cross validation set and test set would be as follows. Let’s take 10,000 good aircraft engines and put 6,000 of that into the unlabeled training set. So, I’m calling this an unlabeled training set but all of these examples are really ones that correspond to y equals 0, as far as we know. And so, we will use this to fit p of x, right. So, we will use these 6000 engines to fit p of x, which is that p of x one parametrized by Mu 1, sigma squared 1, up to p of Xn parametrized by Mu N sigma squared n. And so it would be these 6,000 examples that we would use to estimate the parameters Mu 1, sigma squared 1, up to Mu N, sigma squared N. And so that’s our training set of all, you know, good, or the vast majority of good examples. Next we will take our good aircraft engines and put some number of them in a cross validation set plus some number of them in the test sets. So 6,000 plus 2,000 plus 2,000, that’s how we split up our 10,000 good aircraft engines. And then we also have 20 flawed aircraft engines, and we’ll take that and maybe split it up, you know, put ten of them in the cross validation set and put ten of them in the test sets. And in the next slide we will talk about how to actually use this to evaluate the anomaly detection algorithm. So what I have just described here is a you know probably the recommend a good way of splitting the labeled and unlabeled example. The good and the flawed aircraft engines. Where we use like a 60, 20, 20% split for the good engines and we take the flawed engines, and we put them just in the cross validation set, and just in the test set, then we’ll see in the next slide why that’s the case. Just as an aside, if you look at how people apply anomaly detection algorithms, sometimes you see other peoples’ split the data differently as well. So, another alternative, this is really not a recommended alternative, but some people want to take off your 10,000 good engines, maybe put 6000 of them in your training set and then put the same 4000 in the cross validation set and the test set. And so, you know, we like to think of the cross validation set and the test set as being completely different data sets to each other. But you know, in anomaly detection, you know, for sometimes you see people, sort of, use the same set of good engines in the cross validation sets, and the test sets, and sometimes you see people use exactly the same sets of anomalous engines in the cross validation set and the test set. And so, all of these are considered, you know, less good practices and definitely less recommended. Certainly using the same data in the cross validation set and the test set, that is not considered a good machine learning practice. But, sometimes you see people do this too. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/12.png" alt="Algorithm_evaluation"></p>
<p>So, given the training cross validation and test sets, here’s how you evaluate or here is how you develop and evaluate an algorithm. First, we take the training sets and we fit the model p of x. So, we fit, you know, all these Gaussians to my m unlabeled examples of aircraft engines, and these, I am calling them unlabeled examples, but these are really examples that we’re assuming our goods are the normal aircraft engines. Then imagine that your anomaly detection algorithm is actually making prediction. So, on the cross validation of the test set, given that, say, test example X, think of the algorithm as predicting that y is equal to 1, p of x is less than epsilon, we must be taking zero, if p of x is greater than or equal to epsilon. So, given x, it’s trying to predict, what is the label, given y equals 1 corresponding to an anomaly or is it y equals 0 corresponding to a normal example? So given the training, cross validation, and test sets. How do you develop an algorithm? And more specifically, how do you evaluate an anomaly detection algorithm? Well, to this whole, the first step is to take the unlabeled training set, and to fit the model p of x lead training data. So you take this, you know on I’m coming, unlabeled training set, but really, these are examples that we are assuming, vast majority of which are normal aircraft engines, not because they’re not anomalies and it will fit the model p of x. It will fit all those parameters for all the Gaussians on this data. Next on the cross validation of the test set, we’re going to think of the anomaly detention algorithm as trying to predict the value of y. So in each of like say test examples. We have these X-I tests, Y-I test, where y is going to be equal to 1 or 0 depending on whether this was an anomalous example. So given input x in my test set, my anomaly detection algorithm think of it as predicting the y as 1 if p of x is less than epsilon. So predicting that it is an anomaly, it is probably is very low. And we think of the algorithm is predicting that y is equal to 0. If p of x is greater then or equals epsilon. So predicting those normal example if the p of x is reasonably large. And so we can now think of the anomaly detection algorithm as making predictions for what are the values of these y labels in the test sets or on the cross validation set. And this puts us somewhat more similar to the supervised learning setting, right? Where we have label test set and our algorithm is making predictions on these labels and so we can evaluate it you know by seeing how often it gets these labels right. Of course these labels are will be very skewed because y equals zero, that is normal examples, usually be much more common than y equals 1 than anomalous examples. But, you know, this is much closer to the source of evaluation metrics we can use in supervised learning. So what’s a good evaluation metric to use. Well, because the data is very skewed, because y equals 0 is much more common, classification accuracy would not be a good the evaluation metrics. So, we talked about this in the earlier video. So, if you have a very skewed data set, then predicting y equals 0 all the time, will have very high classification accuracy. Instead, we should use evaluation metrics, like computing the fraction of true positives, false positives, false negatives, true negatives or compute the position of the v curve of this algorithm or do things like compute the f1 score, right, which is a single real number way of summarizing the position and the recall numbers. And so these would be ways to evaluate an anomaly detection algorithm on your cross validation set or on your test set. Finally, earlier in the anomaly detection algorithm, we also had this parameter epsilon, right? So, epsilon is this threshold that we would use to decide when to flag something as an anomaly. And so, if you have a cross validation set, another way to and to choose this parameter epsilon, would be to try a different, try many different values of epsilon, and then pick the value of epsilon that, let’s say, maximizes f1 score, or that otherwise does well on your cross validation set. And more generally, the way to reduce the training, testing, and cross validation sets, is that when we are trying to make decisions, like what features to include, or trying to, you know, tune the parameter epsilon, we would then continually evaluate the algorithm on the cross validation sets and make all those decisions like what features did you use, you know, how to set epsilon, use that, evaluate the algorithm on the cross validation set, and then when we’ve picked the set of features, when we’ve found the value of epsilon that we’re happy with, we can then take the final model and evaluate it, you know, do the final evaluation of the algorithm on the test sets. So, in this video, we talked about the process of how to evaluate an anomaly detection algorithm, and again, having being able to evaluate an algorithm, you know, with a single real number evaluation, with a number like an F1 score that often allows you to much more efficient use of your time when you are trying to develop an anomaly detection system. And we try to make these sorts of decisions. I have to chose epsilon, what features to include, and so on. </p>
<p>In this video, we started to use a bit of labeled data in order to evaluate the anomaly detection algorithm and this takes us a little bit closer to a supervised learning setting. In the next video, I’m going to say a bit more about that. And in particular we’ll talk about when should you be using an anomaly detection algorithm and when should we be thinking about using supervised learning instead, and what are the differences between these two formalisms.</p>
<h4 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h4><p>To evaluate our learning algorithm, we take some labeled data, categorized into anomalous and non-anomalous examples ( y = 0 if normal, y = 1 if anomalous).<br>Among that data, take a large proportion of good , non-anomalous data for the training set on which to train p(x).<br>Then, take a smaller proportion of mixed anomalous and non-anomalous examples (you will usually have many more non-anomalous examples) for your cross-validation and test sets.<br>For example, we may have a set where 0.2% of the data is anomalous. We take 60% of those examples, all of which are good (y=0) for the training set. We then take 20% of the examples for the cross-validation set (with 0.1% of the anomalous examples) and another 20% from the test set (with another 0.1% of the anomalous).<br>In other words, we split the data 60/20/20 training/CV/test and then split the anomalous examples 50/50 between the CV and test sets.<br><strong>Algorithm evaluation:</strong><br>Fit model p(x) on training set $$\lbrace x^{(1)},\dots,x^{(m)} \rbrace$$<br>On a cross validation/test example x, predict: </p>
<ul>
<li>If $p(x) &lt; ϵ$ ( <strong>anomaly</strong> ), then $y = 1$ </li>
<li>If $p(x) ≥ ϵ$ ( <strong>normal</strong> ), then $y = 0$ </li>
</ul>
<p>Possible evaluation metrics (see “Machine Learning System Design” section): </p>
<ul>
<li>True positive, false positive, false negative, true negative. </li>
<li>Precision/recall </li>
<li>$F_1$ score </li>
</ul>
<p>Note that we use the cross-validation set to choose parameter $ϵ$ </p>
<h3 id="02-anomaly-detection-vs-supervised-learning"><a href="#02-anomaly-detection-vs-supervised-learning" class="headerlink" title="02_anomaly-detection-vs-supervised-learning"></a>02_anomaly-detection-vs-supervised-learning</h3><p>In the last video we talked about the process of evaluating an anomaly detection algorithm. And there we started to use some label data with examples that we knew were either anomalous or not anomalous with Y equals one, or Y equals 0. And so, <strong>the question then arises of, and if we have the label data, that we have some examples and know the anomalies, and some of them will not be anomalies. Why don’t we just use a supervisor on half of them? So why don’t we just use logistic regression, or a neuro network to try to learn directly from our labeled data to predict whether Y equals one or Y equals 0. In this video, I’ll try to share with you some of the thinking and some guidelines for when you should probably use an anomaly detection algorithm, and whether it might be more fruitful instead of using a supervisor in the algorithm.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/13.png" alt="Anomaly_detection_VS_supervised_learning"></p>
<p>This slide shows what are the settings under which you should maybe use anomaly detection versus when supervised learning might be more fruitful. If you have a problem with a very small number of positive examples, and remember the examples of y equals one are the anomaly examples. Then you might consider using an anomaly detection algorithm instead. So, having 0 to 20, it may be up to 50 positive examples, might be pretty typical. And usually we have such a small positive, set of positive examples, we’re going to save the positive examples just for the cross validation set in the test set. And in contrast, in a typical normal anomaly detection setting, we will often have a relatively large number of negative examples of the normal examples of normal aircraft engines. And we can then use this very large number of negative examples With which to fit the model p(x). And so there’s this idea that in many anomaly detection applications, you have very few positive examples and lots of negative examples. And when we’re doing the process of estimating p(x), affecting all those Gaussian parameters, we need only negative examples to do that. So if you have a lot negative data, we can still fit p(x) pretty well. In contrast, for supervised learning, more typically we would have a reasonably large number of both positive and negative examples. And so this is one way to look at your problem and decide if you should use an anomaly detection algorithm or a supervised. Here’s another way that people often think about anomaly detection. So for anomaly detection applications, often there are very different types of anomalies. So think about so many different ways for go wrong. There are so many things that could go wrong that could the aircraft engine. And so if that’s the case, and if you have a pretty small set of positive examples, then it can be hard for an algorithm, difficult for an algorithm to learn from your small set of positive examples what the anomalies look like. And in particular, you know future anomalies may look nothing like the ones you’ve seen so far. So maybe in your set of positive examples, maybe you’ve seen 5 or 10 or 20 different ways that an aircraft engine could go wrong. But maybe tomorrow, you need to detect a totally new set, a totally new type of anomaly. A totally new way for an aircraft engine to be broken, that you’ve just never seen before. And if that’s the case, it might be more promising to just model the negative examples with this sort of calcium model p of x rather than try to hard to model the positive examples. Because tomorrow’s anomaly may be nothing like the ones you’ve seen so far. In contrast, in some other problems, you have enough positive examples for an algorithm to get a sense of what the positive examples are like. In particular, if you think that future positive examples are likely to be similar to ones in the training set; then in that setting, it might be more reasonable to have a supervisor in the algorithm that looks at all of the positive examples, looks at all of the negative examples, and uses that to try to distinguish between positives and negatives. Hopefully, this gives you a sense of if you have a specific problem, should you think about using an anomaly detection algorithm, or a supervised learning algorithm. <strong>And a key difference really is that in anomaly detection, often we have such a small number of positive examples that it is not possible for a learning algorithm to learn that much from the positive examples. And so what we do instead is take a large set of negative examples and have it just learn a lot, learn p(x) from just the negative examples.</strong> Of the normal aircraft engines and we’ve reserved the small number of positive examples for evaluating our algorithms to use in the either the transvalidation set or the test set. And just as a side comment about this many different types of easier. <strong>In some earlier videos we talked about the email spam examples. In those examples, there are actually many different types of spam email, right? There’s spam email that’s trying to sell you things. Spam email trying to steal your passwords, this is called fishing emails and many different types of spam emails. But for the spam problem we usually have enough examples of spam email to see most of these different types of spam email because we have a large set of examples of spam. And that’s why we usually think of spam as a supervised learning setting even though there are many different types of.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/14.png" alt="Anomaly_detectionVSsupervised_learning"></p>
<p>And so if we look at some applications of anomaly detection versus supervised learning we’ll find fraud detection. If you have many different types of ways for people to try to commit fraud and a relatively small number of fraudulent users on your website, then I use an anomaly detection algorithm. I should say, if you have, if you’re a very major online retailer and if you actually have had a lot of people commit fraud on your website, <strong>so you actually have a lot of examples of y=1, then sometimes fraud detection could actually shift over to the supervised learning algorithm. But, if you haven’t seen that many examples of users doing strange things on your website, then more frequently fraud detection is actually treated as an anomaly detection algorithm rather than a supervised learning algorithm.</strong> Other examples, we’ve talked about manufacturing already. Hopefully, you see more and more examples are not that many anomalies but if again for some manufacturing processes, if you manufacture in very large volumes and you see a lot of bad examples, maybe manufacturing can shift to the supervised learning column as well. But if you haven’t seen that many bad examples of so to do the anomaly detection monitoring machines in a data center [INAUDIBLE] similar source of apply. Whereas, you must have classification, weather prediction, and classifying cancers. If you have equal numbers of positive and negative examples. Your positive and your negative examples, then we would tend to treat all of these as supervisor problems. </p>
<p><strong>So hopefully, that gives you a sense of one of the properties of a learning problem that would cause you to treat it as an anomaly detection problem versus a supervisory problem. And for many other problems that are faced by various technology companies and so on, we actually are in the settings where we have very few or sometimes zero positive training examples. There’s just so many different types of anomalies that we’ve never seen them before. And for those sorts of problems, very often the algorithm that is used is an anomaly detection algorithm.</strong></p>
<h4 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h4><p>When do we use anomaly detection and when do we use supervised learning? </p>
<p><strong>Use anomaly detection when…</strong></p>
<ul>
<li>We have a very small number of positive examples (y=1 … 0-20 examples is common) and a large number of negative (y=0) examples. </li>
<li>We have many different “types” of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we’ve seen so far. </li>
</ul>
<p><strong>Use supervised learning when…</strong></p>
<ul>
<li>We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes. </li>
<li>We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set. </li>
</ul>
<h3 id="03-choosing-what-features-to-use"><a href="#03-choosing-what-features-to-use" class="headerlink" title="03_choosing-what-features-to-use"></a>03_choosing-what-features-to-use</h3><p>By now you’ve seen the anomaly detection algorithm and we’ve also talked about how to evaluate an anomaly detection algorithm. <strong>It turns out, that when you’re applying anomaly detection, one of the things that has a huge effect on how well it does, is</strong> <em><strong>what features you use, and what features you choose, to give the anomaly detection algorithm.</strong></em> So in this video, what I’d like to do is say a few words, give some suggestions and guidelines for <strong>how to go about designing or selecting features give to an anomaly detection algorithm.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/15.png" alt="non-gaussian_features"></p>
<p>In our anomaly detection algorithm, one of the things we did was model the features using this sort of Gaussian distribution. With xi to mu i, sigma squared i, lets say. And so one thing that I often do would be to plot the data or the histogram of the data, to make sure that the data looks vaguely Gaussian before feeding it to my anomaly detection algorithm. And, it’ll usually work okay, even if your data isn’t Gaussian, but this is sort of a nice sanitary check to run. And by the way, in case your data looks non-Gaussian, the algorithms will often work just find. But, concretely if I plot the data like this, and if it looks like a histogram like this, and the way to plot a histogram is to use the HIST, or the HIST command in Octave, but it looks like this, this looks vaguely Gaussian, so if my features look like this, I would be pretty happy feeding into my algorithm. But if i were to plot a histogram of my data, and it were to look like this well, this doesn’t look at all like a bell shaped curve, this is a very asymmetric distribution, it has a peak way off to one side. If this is what my data looks like, what I’ll often do is play with different transformations of the data in order to make it look more Gaussian. And again the algorithm will usually work okay, even if you don’t. But if you use these transformations to make your data more gaussian, it might work a bit better. So given the data set that looks like this, what I might do is take a log transformation of the data and if i do that and re-plot the histogram, what I end up with in this particular example, is a histogram that looks like this. And this looks much more Gaussian, right? This looks much more like the classic bell shaped curve, that we can fit with some mean and variance paramater sigma. So what I mean by taking a log transform, is really that if I have some feature x1 and then the histogram of x1 looks like this then I might take my feature x1 and replace it with log of x1 and this is my new x1 that I’ll plot to the histogram over on the right, and this looks much more Guassian. Rather than just a log transform some other things you can do, might be, let’s say I have a different feature x2, maybe I’ll replace that will log x plus 1, or more generally with log x with x2 and some constant c and this constant could be something that I play with, to try to make it look as Gaussian as possible. Or for a different feature x3, maybe I’ll replace it with x3, I might take the square root. The square root is just x3 to the power of one half, right? And this one half is another example of a parameter I can play with. So, I might have x4 and maybe I might instead replace that with x4 to the power of something else, maybe to the power of 1/3. And these, all of these, this one, this exponent parameter, or the C parameter, all of these are examples of parameters that you can play with in order to make your data look a little bit more Gaussian. </p>
<h4 id="live-demo"><a href="#live-demo" class="headerlink" title="live demo"></a>live demo</h4><p>So, let me show you a live demo of how I actually go about playing with my data to make it look more Gaussian. So, I have already loaded in to octave here a set of features x I have a thousand examples loaded over there. So let’s pull up the histogram of my data. Use the hist x command. So there’s my histogram. By default, I think this uses 10 bins of histograms, but I want to see a more fine grid histogram. So we do hist to the x, 50, so, this plots it in 50 different bins.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/16.png" alt="choosing_featrues_anomaly_detection_1"></p>
<p>Okay, that looks better. Now, this doesn’t look very Gaussian, does it? So, lets start playing around with the data. Lets try a hist of x to the 0.5. So we take the square root of the data, and plot that histogram. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/17.png" alt="choosing_featrues_anomaly_detection_2"></p>
<p>And, okay, it looks a little bit more Gaussian, but not quite there, so let’s play at the 0.5 parameter. Let’s see. Set this to 0.2. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/18.png" alt="choosing_featrues_anomaly_detection_3"></p>
<p>Looks a little bit more Gaussian. Let’s reduce a little bit more 0.1. Yeah, that looks pretty good. I could actually just use 0.1.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/19.png" alt="choosing_featrues_anomaly_detection_4"></p>
<p>Well, let’s reduce it to 0.05. And, you know? Okay, this looks pretty Gaussian, so I can define a new feature which is x mu equals x to the 0.05, and now my new feature x Mu looks more Gaussian than my previous one and then I might instead use this new feature to feed into my anomaly detection algorithm. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/20.png" alt="choosing_featrues_anomaly_detection_6"></p>
<p>And of course, there is more than one way to do this. You could also have hist of log of x, that’s another example of a transformation you can use. And, you know, that also look pretty Gaussian. So, I can also define x mu equals log of x. and that would be another pretty good choice of a feature to use. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/21.png" alt="choosing_featrues_anomaly_detection_7"></p>
<p><strong>So to summarize, if you plot a histogram with the data, and find that it looks pretty non-Gaussian, it’s worth playing around a little bit with different transformations like these, to see if you can make your data look a little bit more Gaussian, before you feed it to your learning algorithm, although even if you don’t, it might work okay. But I usually do take this step.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/22.png" alt="error_analysis_for_anomaly_detection"></p>
<p>Now, the second thing I want to talk about is, <strong>how do you come up with features for an anomaly detection algorithm.</strong> And the way I often do so, is via an error analysis procedure. So what I mean by that, is that this is really similar to the error analysis procedure that we have for supervised learning, where we would train a complete algorithm, and run the algorithm on a cross validation set, and look at the examples it gets wrong, and see if we can come up with extra features to help the algorithm do better on the examples that it got wrong in the cross-validation set. So lets try to reason through an example of this process. In anomaly detection, we are hoping that p of x will be large for the normal examples and it will be small for the anomalous examples. And so a pretty common problem would be if p of x is comparable, maybe both are large for both the normal and the anomalous examples. Lets look at a specific example of that. Let’s say that this is my unlabeled data. So, here I have just one feature, x1 and so I’m gonna fit a Gaussian to this. And maybe my Gaussian that I fit to my data looks like that. And now let’s say I have an anomalous example, and let’s say that my anomalous example takes on an x value of 2.5. So I plot my anomalous example there. And you know, it’s kind of buried in the middle of a bunch of normal examples, and so, just this anomalous example that I’ve drawn in green, it gets a pretty high probability, where it’s the height of the blue curve, and the algorithm fails to flag this as an anomalous example. Now, if this were maybe aircraft engine manufacturing or something, what I would do is, I would actually look at my training examples and look at what went wrong with that particular aircraft engine, and see, if looking at that example can inspire me to come up with a new feature x2, that helps to distinguish between this bad example, compared to the rest of my red examples, compared to all of my normal aircraft engines. And if I managed to do so, the hope would be then, that, if I can create a new feature, X2, so that when I re-plot my data, if I take all my normal examples of my training set, hopefully I find that all my training examples are these red crosses here. And hopefully, if I find that for my anomalous example, the feature x2 takes on the the unusual value. So for my green example here, this anomaly, right, my X1 value, is still 2.5. Then maybe my X2 value, hopefully it takes on a very large value like 3.5 over there, or a very small value. But now, if I model my data, I’ll find that my anomaly detection algorithm gives high probability to data in the central regions, slightly lower probability to that, sightly lower probability to that. An example that’s all the way out there, my algorithm will now give very low probability to. And so, the process of this is, really look at the mistakes that it is making. <strong>Look at the anomaly that the algorithm is failing to flag, and see if that inspires you to create some new feature. So find something unusual about that aircraft engine and use that to create a new feature, so that with this new feature it becomes easier to distinguish the anomalies from your good examples. And so that’s the process of error analysis and using that to create new features for anomaly detection.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/23.png" alt="monitoring_companies_in_a_data_center"></p>
<p>Finally, let me share with you my thinking on <strong>how I usually go about choosing features for anomaly detection</strong>. So, usually, the way I think about choosing features is I want to choose features that will take on either very, very large values, or very, very small values, for examples that I think might turn out to be anomalies. So let’s use our example again of monitoring the computers in a data center. And so you have lots of machines, maybe thousands, or tens of thousands of machines in a data center. And we want to know if one of the machines, one of our computers is acting up, so doing something strange. So here are examples of features you may choose, maybe memory used, number of disc accesses, CPU load, network traffic. But now, lets say that I suspect one of the failure cases, let’s say that in my data set I think that CPU load the network traffic tend to grow linearly with each other. Maybe I’m running a bunch of web servers, and so, here if one of my servers is serving a lot of users, I have a very high CPU load, and have a very high network traffic. But let’s say, I think, let’s say I have a suspicion, that one of the failure cases is if one of my computers has a job that gets stuck in some infinite loop. So if I think one of the failure cases, is one of my machines, one of my web servers–server code– gets stuck in some infinite loop, and so the CPU load grows, but the network traffic doesn’t because it’s just spinning it’s wheels and doing a lot of CPU work, you know, stuck in some infinite loop. In that case, to detect that type of anomaly, I might create a new feature, X5, which might be CPU load divided by network traffic. And so here X5 will take on a unusually large value if one of the machines has a very large CPU load but not that much network traffic and so this will be a feature that will help your anomaly detection capture, a certain type of anomaly. And you can also get creative and come up with other features as well. Like maybe I have a feature x6 thats CPU load squared divided by network traffic. And this would be another variant of a feature like x5 to try to capture anomalies where one of your machines has a very high CPU load, that maybe doesn’t have a commensurately large network traffic. And by creating features like these, you can start to capture anomalies that correspond to unusual combinations of values of the features. </p>
<p>So in this video we talked about <strong>how to and take a feature</strong>, and maybe <strong>transform</strong> it a little bit, so that it becomes a bit more Gaussian, before feeding into an anomaly detection algorithm. And also <strong>the error analysis in this process of creating features to try to capture different types of anomalies</strong>. And with these sorts of guidelines hopefully that will help you to choose good features, to give to your anomaly detection algorithm, to help it capture all sorts of anomalies.</p>
<h4 id="summary-5"><a href="#summary-5" class="headerlink" title="summary"></a>summary</h4><p>The features will greatly affect how well your anomaly detection algorithm works.<br>We can check that our features are <strong>gaussian</strong> by plotting a histogram of our data and checking for the bell-shaped curve.<br>Some <strong>transforms</strong> we can try on an example feature x that does not have the bell-shaped curve are: </p>
<ul>
<li>$log(x)$</li>
<li>$log(x+1)$ </li>
<li>$log(x+c)$ for some constant </li>
<li>$\sqrt{x}$</li>
<li>$x^{1/3}$ </li>
</ul>
<p>We can play with each of these to try and achieve the gaussian shape in our data.<br>There is an <strong>error analysis procedure</strong> for anomaly detection that is very similar to the one in supervised learning.<br>Our goal is for $p(x)$ to be large for normal examples and small for anomalous examples.<br>One common problem is when $p(x)$ is similar for both types of examples. In this case, you need to examine the anomalous examples that are giving high probability in detail and try to figure out new features that will better distinguish the data.<br>In general, choose features that might take on unusually large or small values in the event of an anomaly. </p>
<h2 id="03-multivariate-gaussian-distribution-optional"><a href="#03-multivariate-gaussian-distribution-optional" class="headerlink" title="03_multivariate-gaussian-distribution-optional"></a>03_multivariate-gaussian-distribution-optional</h2><h3 id="01-multivariate-gaussian-distribution"><a href="#01-multivariate-gaussian-distribution" class="headerlink" title="01_multivariate-gaussian-distribution"></a>01_multivariate-gaussian-distribution</h3><p>In this and the next video, I’d like to tell you about one possible extension to the anomaly detection algorithm that we’ve developed so far. This extension uses something called the <strong>multivariate Gaussian distribution</strong>, and it has some advantages, and some disadvantages, and it can sometimes catch some anomalies that the earlier algorithm didn’t.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/24.png" alt="Motivating_example_monitoring_machine_in_a_data_center"></p>
<p>To motivate this, let’s start with an example. Let’s say that so our unlabeled data looks like what I have plotted here. And I’m going to use the example of monitoring machines in the data center, monitoring computers in the data center. So my two features are x1 which is the CPU load and x2 which is maybe the memory use. So if I take my two features, x1 and x2, and I model them as Gaussians then here’s a plot of my X1 features, here’s a plot of my X2 features, and so if I fit a Gaussian to that, maybe I’ll get a Gaussian like this, so here’s P of X 1, which depends on the parameters mu 1, and sigma squared 1, and here’s my memory used, and, you know, maybe I’ll get a Gaussian that looks like this, and this is my P of X 2, which depends on mu 2 and sigma squared 2. And so this is how the anomaly detection algorithm models X1 and X2. Now let’s say that in the test sets I have an example that looks like this. The location of that green cross, so the value of X 1 is about 0.4, and the value of X 2 is about 1.5. Now, if you look at the data, it looks like, yeah, most of the data data lies in this region, and so that green cross is pretty far away from any of the data I’ve seen. It looks like that should be raised as an anomaly. So, in my data, in my, in the data of my good examples, it looks like, you know, the CPU load, and the memory use, they sort of grow linearly with each other. So if I have a machine using lots of CPU, you know memory use will also be high, whereas this example, this green example it looks like here, the CPU load is very low, but the memory use is very high, and I just have not seen that before in my training set. It looks like that should be an anomaly. But let’s see what the anomaly detection algorithm will do. Well, for the CPU load, it puts it at around there 0.5 and this reasonably high probability is not that far from other examples we’ve seen, maybe, whereas, for the memory use, this appointment, 0.5, whereas for the memory use, it’s about 1.5, which is there. Again, you know, it’s all to us, it’s not terribly Gaussian, but the value here and the value here is not that different from many other examples we’ve seen, and so P of X 1, will be pretty high, reasonably high. P of X 2 reasonably high. I mean, if you look at this plot right, this point here, it doesn’t look that bad, and if you look at this plot, you know across here, doesn’t look that bad. I mean, I have had examples with even greater memory used, or with even less CPU use, and so this example doesn’t look that anomalous. And so, an anomaly detection algorithm will fail to flag this point as an anomaly. And it turns out what our anomaly detection algorithm is doing is that it is not realizing that this blue ellipse shows the high probability region, is that, one of the thing is that, examples here, a high probability, and the examples, the next circle of from a lower probably, and examples here are even lower probability, and somehow, here are things that are, green cross there, it’s pretty high probability, and in particular, it tends to think that, you know, everything in this region, everything on the line that I’m circling over, has, you know, about equal probability, and it doesn’t realize that something out here actually has much lower probability than something over there. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/25.png" alt="multivariate_gaussian_normal_distribution"></p>
<p>So, in order to fix this, we can, we’re going to develop a modified version of the anomaly detection algorithm, using something called the multivariate Gaussian distribution also called the multivariate normal distribution. So here’s what we’re going to do. We have features x which are in Rn and instead of P of X 1, P of X 2, separately, we’re going to model P of X, all in one go, so model P of X, you know, all at the same time. So the parameters of the multivariate Gaussian distribution are mu, which is a vector, and sigma, which is an n by n matrix, called a covariance matrix, and this is similar to the covariance matrix that we saw when we were working with the PCA, with the principal components analysis algorithm. For the second complete is, let me just write out the formula for the multivariate Gaussian distribution. So we say that probability of X, and this is parameterized by my parameters mu and sigma that the probability of x is equal to once again there’s absolutely no need to memorize this formula. You know, you can look it up whenever you need to use it, but this is what the probability of X looks like. Transverse, 2nd inverse, X minus mu. And this thing here, the absolute value of sigma, this thing here when you write this symbol, this is called the determent of sigma and this is a mathematical function of a matrix and you really don’t need to know what the determinant of a matrix is, but really all you need to know is that you can compute it in octave by using the octave command DET of sigma. Okay, and again, just be clear, alright? In this expression, these sigmas here, these are just n by n matrix. This is not a summation and you know, the sigma there is an n by n matrix. So that’s the formula for P of X, but it’s more interestingly, or more importantly, what does P of X actually looks like? Lets look at some examples of multivariate Gaussian distributions. </p>
<p>$$p(x)=∏<em>{j=1}^{n}p(x_j;μ_j,σ^2_j)=∏</em>{j=1}^{n}\frac{1}{\sqrt{2π}σ<em>j}exp(-\frac{(x_j-μ_j)^2}{2σ_j^2}), μ=\frac{1}{m}\sum</em>{i=1}^{m}x^{(i)} \ p(x)=\frac{1}{(2π)^{\frac{n}{2}} |Σ|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-μ)^TΣ^{-1}(x-μ)), Σ=\frac{1}{m}(X-μ)^T(X-μ)$$</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/26.png" alt=""></p>
<p>So let’s take a two dimensional example, say if I have N equals 2, I have two features, X1 and X2. Lets say I set MU to be equal to 0 and sigma to be equal to this matrix here. With 1s on the diagonals and 0s on the off-diagonals, this matrix is sometimes also called the identity matrix. In that case, p of x will look like this, and what I’m showing in this figure is, you know, for a specific value of X1 and for a specific value of X2, the height of this surface the value of p of x. And so with this setting the parameters p of x is highest when X1 and X2 equal zero 0, so that’s the peak of this Gaussian distribution, and the probability falls off with this sort of two dimensional Gaussian or this bell shaped two dimensional bell-shaped surface. Down below is the same thing but plotted using a contour plot instead, or using different colors, and so this heavy intense red in the middle, corresponds to the highest values, and then the values decrease with the yellow being slightly lower values the cyan being lower values and this deep blue being the lowest values so this is really the same figure but plotted viewed from the top instead, using colors instead. And so, with this distribution, you see that it faces most of the probability near 0,0 and then as you go out from 0,0 the probability of X1 and X2 goes down. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/27.png" alt="Multivariate_Gaussian_normal_example_2"></p>
<p>Now lets try varying some of the parameters and see what happens. So let’s take sigma and change it so let’s say sigma shrinks a little bit. Sigma is a covariance matrix and so it measures the variance or the variability of the features X1 X2. So if the shrink sigma then what you get is what you get is that the width of this bump diminishes and the height also increases a bit, because the area under the surface is equal to 1. So the integral of the volume under the surface is equal to 1, because probability distribution must integrate to one. But, if you shrink the variance, it’s kinda like shrinking sigma squared, you end up with a narrower distribution, and one that’s a little bit taller. And so you see here also the concentric ellipsis has shrunk a little bit. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/28.png" alt="Multivariate_Gaussian_normal_example_3"></p>
<p>Whereas in contrast if you were to increase sigma to 2 2 on the diagonals, so it is now two times the identity then you end up with a much wider and much flatter Gaussian. And so the width of this is much wider. This is hard to see but this is still a bell shaped bump, it’s just flattened down a lot, it has become much wider and so the variance or the variability of X1 and X2 just becomes wider. Here are a few more examples. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/29.png" alt="Multivariate_Gaussian_normal_example_4"></p>
<p>Now lets try varying one of the elements of sigma at the time. Let’s say I send sigma to 0.6 there, and 1 over there. What this does, is this reduces the variance of the first feature, X 1, while keeping the variance of the second feature X 2, the same. And so with this setting of parameters, you can model things like that. X 1 has smaller variance, and X 2 has larger variance. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/30.png" alt="Multivariate_Gaussian_normal_example_5"></p>
<p>Whereas if I do this, if I set this matrix to 2, 1 then you can also model examples where you know here we’ll say X1 can have take on a large range of values whereas X2 takes on a relatively narrower range of values. And that’s reflected in this figure as well, you know where, the distribution falls off more slowly as X 1 moves away from 0, and falls off very rapidly as X 2 moves away from 0. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/38.png" alt="Multivariate_Gaussian_normal_example_6"><br>And similarly if we were to modify this element of the matrix instead, then similar to the previous slide, except that here where you know playing around here saying that X2 can take on a very small range of values and so here if this is 0.6, we notice now X2 tends to take on a much smaller range of values than the original example, whereas if we were to set sigma to be equal to 2 then that’s like saying X2 you know, has a much larger range of values.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/31.png" alt="Multivariate_Gaussian_normal_example_7"></p>
<p>Now, <strong>one of the cool things about the multivariate Gaussian distribution is that you can also use it to model correlations between the data.</strong> That is we can use it to model the fact that X1 and X2 tend to be highly correlated with each other for example. So specifically if you start to change the off diagonal entries of this covariance matrix you can get a different type of Gaussian distribution. And so as I increase the off-diagonal entries from .5 to .8, what I get is this distribution that is more and more thinly peaked along this sort of x equals y line. And so here the contour says that x and y tend to grow together and the things that are with large probability are if either X1 is large and Y2 is large or X1 is small and Y2 is small. Or somewhere in between. And as this entry, 0.8 gets large, you get a Gaussian distribution, that’s sort of where all the probability lies on this sort of narrow region, where x is approximately equal to y. This is a very tall, thin distribution you know line mostly along this line central region where x is close to y. So this is if we set these entries to be positive entries. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/32.png" alt="Multivariate_Gaussian_normal_example_8"></p>
<p>In contrast if we set these to negative values, as I decreases it to -.5 down to -.8, then what we get is a model where we put most of the probability in this sort of negative X one in the next 2 correlation region, and so, most of the probability now lies in this region, where X 1 is about equal to -X 2, rather than X 1 equals X 2. And so this captures a sort of negative correlation between x1 and x2. And so this is a hopefully this gives you a sense of the different distributions that the multivariate Gaussian distribution can capture. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/33.png" alt="Multivariate_Gaussian_normal_example_9"></p>
<p>So follow up in varying, the covariance matrix sigma, the other thing you can do is also, vary the mean parameter mu, and so operationally, we have mu equal 0 0, and so the distribution was centered around X 1 equals 0, X2 equals 0, so the peak of the distribution is here, whereas, if we vary the values of mu, then that varies the peak of the distribution and so, if mu equals 0, 0.5, the peak is at, you know, X1 equals zero, and X2 equals 0.5, and so the peak or the center of this distribution has shifted, and if mu was 1.5 minus 0.5 then OK, and similarly the peak of the distribution has now shifted to a different location, corresponding to where, you know, X1 is 1.5 and X2 is -0.5, and so varying the mu parameter, just shifts around the center of this whole distribution.</p>
<p>So, hopefully, looking at all these different pictures gives you a sense of the sort of probability distributions that the Multivariate Gaussian Distribution allows you to capture. <strong>And the key advantage of it is it allows you to capture, when you’d expect two different features to be positively correlated, or maybe negatively correlated.</strong> </p>
<p>In the next video, we’ll take this multivariate Gaussian distribution and apply it to anomaly detection.</p>
<h4 id="summary-6"><a href="#summary-6" class="headerlink" title="summary"></a>summary</h4><p>The multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.<br>Instead of modeling $p(x_1),p(x_2),\dots$ separately, we will model p(x) all in one go. Our parameters will be: $\mu \in \mathbb{R}^n$ and $\Sigma \in \mathbb{R}^{n \times n}$<br>$$p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n\over 2} |\Sigma|^{1\over 2}} exp(-{1\over 2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$<br>The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours. </p>
<p><strong>Varying Σ changes the shape, width, and orientation of the contours. Changing μ will move the center of the distribution.</strong></p>
<p>Check also: </p>
<ul>
<li>The Multivariate Gaussian Distribution <a href="http://cs229.stanford.edu/section/gaussians.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/section/gaussians.pdf</a> Chuong B. Do, October 10, 2008. </li>
</ul>
<h3 id="02-anomaly-detection-using-the-multivariate-gaussian-distribution"><a href="#02-anomaly-detection-using-the-multivariate-gaussian-distribution" class="headerlink" title="02_anomaly-detection-using-the-multivariate-gaussian-distribution"></a>02_anomaly-detection-using-the-multivariate-gaussian-distribution</h3><p>In the last video we talked about the Multivariate Gaussian Distribution and saw some examples of the sorts of distributions you can model, as you vary the parameters, mu and sigma. In this video, let’s take those ideas, and apply them to develop a different anomaly detection algorithm. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/34.png" alt=""></p>
<p>To recap the multivariate Gaussian distribution and the multivariate normal distribution has two parameters, mu and sigma. Where mu this an n dimensional vector and sigma, the covariance matrix, is an n by n matrix. And here’s the formula for the probability of X, as parameterized by mu and sigma, and as you vary mu and sigma, you can get a range of different distributions, like, you know, these are three examples of the ones that we saw in the previous video. So let’s talk about the parameter fitting or the parameter estimation problem. The question, as usual, is if I have a set of examples X1 through XM and here each of these examples is an n dimensional vector and I think my examples come from a multivariate Gaussian distribution. How do I try to estimate my parameters mu and sigma? Well the standard formulas for estimating them is you set mu to be just the average of your training examples. And you set sigma to be equal to this. And this is actually just like the sigma that we had written out, when we were using the PCA or the Principal Components Analysis algorithm. So you just plug in these two formulas and this would give you your estimated parameter mu and your estimated parameter sigma. So given the data set here is how you estimate mu and sigma. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/35.png" alt="Anomaly_detection_with_the_multivariate_Gaussian"></p>
<p>Let’s take this method and just plug it into an anomaly detection algorithm. So how do we put all of this together to develop an anomaly detection algorithm? Here ‘s what we do. First we take our training set, and we fit the model, we fit P of X, by, you know, setting mu and sigma as described on the previous slide. Next when you are given a new example X. So if you are given a test example, lets take an earlier example to have a new example out here. And that is my test example. Given the new example X, what we are going to do is compute P of X, using this formula for the multivariate Gaussian distribution. And then, if P of X is very small, then we flagged it as an anomaly, whereas, if P of X is greater than that parameter epsilon, then we don’t flag it as an anomaly. So it turns out, if we were to fit a multivariate Gaussian distribution to this data set, so just the red crosses, not the green example, you end up with a Gaussian distribution that places lots of probability in the central region, slightly less probability here, slightly less probability here, slightly less probability here, and very low probability at the point that is way out here. And so, if you apply the multivariate Gaussian distribution to this example, it will actually correctly flag that example. as an anomaly. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/36.png" alt="relationship_to_original_model"></p>
<p>Finally it’s worth saying a few words about what is the relationship between the multivariate Gaussian distribution model, and the original model, where we were modeling P of X as a product of this P of X1, P of X2, up to P of Xn. It turns out that you can prove mathematically, I’m not going to do the proof here, but you can prove mathematically that this relationship, between the multivariate Gaussian model and this original one. And in particular, it turns out that the original model corresponds to multivariate Gaussians, where the contours of the Gaussian are always axis aligned. So all three of these are examples of Gaussian distributions that you can fit using the original model. It turns out that that corresponds to multivariate Gaussian, where, you know, the ellipsis here, the contours of this distribution–it turns out that this model actually corresponds to a special case of a multivariate Gaussian distribution. And in particular, this special case is defined by constraining the distribution of p of x, the multivariate a Gaussian distribution of p of x, so that the contours of the probability density function, of the probability distribution function, are axis aligned. And so you can get a p of x with a multivariate Gaussian that looks like this, or like this, or like this. And you notice, that in all 3 of these examples, these ellipses, or these ovals that I’m drawing, have their axes aligned with the X1 X2 axes. And what we do not have, is a set of contours that are at an angle, right? And this corresponded to examples where sigma is equal to 1 1, 0.8, 0.8. Let’s say, with non-0 elements on the off diagonals. So, it turns out that it’s possible to show mathematically that this model actually is the same as a multivariate Gaussian distribution but with a constraint. And the constraint is that the covariance matrix sigma must have 0’s on the off diagonal elements. In particular, the covariance matrix sigma, this thing here, it would be sigma squared 1, sigma squared 2, down to sigma squared n, and then everything on the off diagonal entries, all of these elements above and below the diagonal of the matrix, all of those are going to be zero. And in fact if you take these values of sigma, sigma squared 1, sigma squared 2, down to sigma squared n, and plug them into here, and you know, plug them into this covariance matrix, then the two models are actually identical. That is, this new model, using a multivariate Gaussian distribution, corresponds exactly to the old model, if the covariance matrix sigma, has only 0 elements off the diagonals, and in pictures that corresponds to having Gaussian distributions, where the contours of this distribution function are axis aligned. So you aren’t allowed to model the correlations between the diffrent features. So in that sense the original model is actually a special case of this multivariate Gaussian model. </p>
<p><strong>So when would you use each of these two models? So when would you the original model and when would you use the multivariate Gaussian model?</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/15/37.png" alt="how_choose_the_original-model_and_multivariate-model"></p>
<p>The original model is probably used somewhat more often, and whereas the multivariate Gaussian distribution is used somewhat less but it has the advantage of being able to capture correlations between features. So suppose you want to capture anomalies where you have different features say where features x1, x2 take on unusual combinations of values so in the earlier example, we had that example where the anomaly was with the CPU load and the memory use taking on unusual combinations of values, if you want to use the original model to capture that, then what you need to do is create an extra feature, such as X3 equals X1/X2, you know equals maybe the CPU load divided by the memory used, or something, and you need to create extra features if there’s unusual combinations of values where X1 and X2 take on an unusual combination of values even though X1 by itself and X2 by itself looks like it’s taking a perfectly normal value. But if you’re willing to spend the time to manually create an extra feature like this, then the original model will work fine.  Whereas in contrast, the multivariate Gaussian model can automatically capture correlations between different features. But the original model has some other more significant advantages, too, and one huge advantage of the original model is that it is computationally cheaper, and another view on this is that is scales better to very large values of n and very large numbers of features, and so even if n were ten thousand, or even if n were equal to a hundred thousand, the original model will usually work just fine. Whereas in contrast for the multivariate Gaussian model notice here, for example, that we need to compute the inverse of the matrix sigma where sigma is an n by n matrix and so computing sigma if sigma is a hundred thousand by a hundred thousand matrix that is going to be very computationally expensive. And so the multivariate Gaussian model scales less well to large values of N. And finally for the original model, it turns out to work out ok even if you have a relatively small training set this is the small unlabeled examples that we use to model p of x of course, and this works fine, even if M is, you know, maybe 50, 100, works fine. Whereas for the multivariate Gaussian, it is sort of a mathematical property of the algorithm that you must have m greater than n, so that the number of examples is greater than the number of features you have. And there’s a mathematical property of the way we estimate the parameters that if this is not true, so if m is less than or equal to n, then this matrix isn’t even invertible, that is this matrix is singular, and so you can’t even use the multivariate Gaussian model unless you make some changes to it. But a typical rule of thumb that I use is, I will use the multivariate Gaussian model only if m is much greater than n, so this is sort of the narrow mathematical requirement, but in practice, I would use the multivariate Gaussian model, only if m were quite a bit bigger than n.  So if m were greater than or equal to 10 times n, let’s say, might be a reasonable rule of thumb, and if it doesn’t satisfy this, then the multivariate Gaussian model has a lot of parameters, right, so this covariance matrix sigma is an n by n matrix, so it has, you know, roughly n squared parameters, because it’s a symmetric matrix, it’s actually closer to n squared over 2 parameters, but this is a lot of parameters, so you need make sure you have a fairly large value for m, make sure you have enough data to fit all these parameters. And m greater than or equal to 10 n would be a reasonable rule of thumb to make sure that you can estimate this covariance matrix sigma reasonably well. So in practice the original model shown on the left that is used more often. And if you suspect that you need to capture correlations between features what people will often do is just manually design extra features like these to capture specific unusual combinations of values. But in problems where you have a very large training set or m is very large and n is not too large, then the multivariate Gaussian model is well worth considering and may work better as well, and can save you from having to spend your time to manually create extra features in case the anomalies turn out to be captured by unusual combinations of values of the features. Finally I just want to briefly mention one somewhat technical property, but if you’re fitting multivariate Gaussian model, and if you find that the covariance matrix sigma is singular, or you find it’s non-invertible, they’re usually 2 cases for this. One is if it’s failing to satisfy this m greater than n condition, and the second case is if you have redundant features. So by redundant features, I mean, if you have 2 features that are the same. Somehow you accidentally made two copies of the feature, so your x1 is just equal to x2. Or if you have redundant features like maybe your features X3 is equal to feature X4, plus feature X5. Okay, so if you have highly redundant features like these, you know, where if X3 is equal to X4 plus X5, well X3 doesn’t contain any extra information, right? You just take these 2 other features, and add them together. And if you have this sort of redundant features, duplicated features, or this sort of features, than sigma may be non-invertible. And so there’s a debugging set– this should very rarely happen, so you probably won’t run into this, it is very unlikely that you have to worry about this– but in case you implement a multivariate Gaussian model you find that sigma is non-invertible. What I would do is first make sure that M is quite a bit bigger than N, and if it is then, the second thing I do, is just check for redundant features. And so if there are 2 features that are equal, just get rid of one of them, or if you have redundant if these , X3 equals X4 plus X5, just get rid of the redundant feature, and then it should work fine again. As an aside for those of you who are experts in linear algebra, by redundant features, what I mean is the formal term is features that are linearly dependent. But in practice what that really means is one of these problems tripping up the algorithm if you just make you features non-redundant., that should solve the problem of sigma being non-invertable. But once again the odds of your running into this at all are pretty low so chances are, you can just apply the multivariate Gaussian model, without having to worry about sigma being non-invertible, so long as m is greater than or equal to n. So that’s it for anomaly detection, with the multivariate Gaussian distribution. And if you apply this method you would be able to have an anomaly detection algorithm that automatically captures positive and negative correlations between your different features and flags an anomaly if it sees is unusual combination of the values of the features. </p>
<h4 id="summary-7"><a href="#summary-7" class="headerlink" title="summary"></a>summary</h4><p>When doing anomaly detection with multivariate gaussian distribution, we compute $μ$ and $Σ$ normally. We then compute $p(x)$ using the new formula in the previous section and flag an anomaly if $p(x) &lt; ϵ$.<br>The original model for p(x) corresponds to a multivariate Gaussian where the contours of $p(x;\mu,\Sigma)$ are axis-aligned.<br>The multivariate Gaussian model can automatically capture correlations between different features of $x$.<br>However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for $Σ$ to be invertible</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/14/14_dimensionality-reduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/14/14_dimensionality-reduction/" class="post-title-link" itemprop="url">14_dimensionality-reduction note14</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-14 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-14T00:00:00+05:30">2018-01-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>56k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>51 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This personal note is written after studying the opening course on <a href="https://www.coursera.org" target="_blank" rel="noopener">the coursera website</a>, <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew NG</a> . And images, audios of this note all comes from the opening course. </p>
<h2 id="01-motivation"><a href="#01-motivation" class="headerlink" title="01_motivation"></a>01_motivation</h2><h3 id="01-motivation-i-data-compression"><a href="#01-motivation-i-data-compression" class="headerlink" title="01_motivation-i-data-compression"></a>01_motivation-i-data-compression</h3><p>In this video, I’d like to start talking about <em>a second type of unsupervised learning problem called</em> <strong>dimensionality reduction</strong>. </p>
<p><strong>There are a couple of different reasons why one might want to do dimensionality reduction. One is data compression, and as we’ll see later, a few videos later, data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms.</strong></p>
<p> But first, let’s start by talking about <strong>what is dimensionality reduction</strong>. As a motivating example, let’s say that we’ve collected a data set with many, many, many features, and I’ve plotted just two of them here. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/1.png" alt="2D-1D-1"></p>
<p> And let’s say that unknown to us two of the features were actually the length of something in centimeters, and a different feature, x2, is the length of the same thing in inches. So, this gives us a <strong>highly redundant representation</strong> and maybe instead of having two separate features x1 then x2, both of which basically measure the length, maybe what we want to do is reduce the data to one-dimensional and just have one number measuring this length. In case this example seems a bit contrived, this centimeter and inches example is actually not that unrealistic, and not that different from things that I see happening in industry. If you have hundreds or thousands of features, it is often this easy to lose track of exactly what features you have. And sometimes may have a few different engineering teams, maybe one engineering team gives you two hundred features, a second engineering team gives you another three hundred features, and a third engineering team gives you five hundred features so you have a thousand features all together, and it actually becomes hard to keep track of you know, exactly which features you got from which team, and it’s actually not that want to have highly redundant features like these. And so if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch. Then, that’s why these examples don’t lie perfectly on a straight line, because of, you know, round-off error to the nearest centimeter or the nearest inch. And if we can reduce the data to one dimension instead of two dimensions, that reduces the redundancy.</p>
<p> For a different example, again maybe when there seems fairly less contrives. For may years I’ve been working with autonomous helicopter pilots. Or I’ve been working with pilots that fly helicopters. And so. If you were to measure–if you were to, you know, do a survey or do a test of these different pilots–you might have one feature, x1, which is maybe the skill of these helicopter pilots, and maybe “x2” could be the pilot enjoyment. That is, you know, how much they enjoy flying, and maybe these two features will be highly correlated. And what you really care about might be this sort of this sort of, this direction, a different feature that really measures pilot aptitude. And I’m making up the name aptitude of course, but again, if you highly correlated features, maybe you really want to reduce the dimension.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/2.png" alt="autonomous_helicopter_pilots"></p>
<p> So, let me say a little bit more about what it really means to reduce the dimension of the data from 2 dimensions down from 2D to 1 dimensional or to 1D. Let me color in these examples by using different colors.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/3.png" alt="Data_compression"></p>
<p> And in this case by reducing the dimension what I mean is that I would like to find maybe this line, this, you know, direction on which most of the data seems to lie and project all the data onto that line which is true, and by doing so, what I can do is just measure the position of each of the examples on that line. And what I can do is come up with a new feature, z1, and to specify the position on the line I need only one number, so it says z1 is a new feature that specifies the location of each of those points on this green line. And what this means, is that where as previously if i had an example x1, maybe this was my first example, x1. So in order to represent x1 originally x1. I needed a two dimensional number, or a two dimensional feature vector. Instead now I can represent z1. I could use just z1 to represent my first example, and that’s going to be a real number. And similarly x2 you know, if x2 is my second example there, then previously, whereas this required two numbers to represent if I instead compute the projection of that black cross onto the line. And now I only need one real number which is z2 to represent the location of this point z2 on the line. And so on through my M examples. So, just to summarize, if we allow ourselves to approximate the original data set by projecting all of my original examples onto this green line over here, then I need only one number, I need only real number to specify the position of a point on the line, and so what I can do is therefore use just one number to represent the location of each of my training examples after they’ve been projected onto that green line. So this is an approximation to the original training self because I have projected all of my training examples onto a line. But now, I need to keep around only one number for each of my examples. And so this halves the memory requirement, or a space requirement, or what have you, for how to store my data. And perhaps more interestingly, more importantly, what we’ll see later, in the later video as well is that this will allow us to make our learning algorithms run more quickly as well. And that is actually, perhaps, even the more interesting application of this data compression rather than reducing the memory or disk space requirement for storing the data. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/4.png" alt="Data_compression_3D-2D"></p>
<p>On the previous slide we showed an example of reducing data from 2D to 1D. On this slide, I’m going to show another example of reducing data from three dimensional 3D to two dimensional 2D. By the way, in the more typical example of dimensionality reduction we might have a thousand dimensional data or 1000D data that we might want to reduce to let’s say a hundred dimensional or 100D, but because of the limitations of what I can plot on the slide. I’m going to use examples of 3D to 2D, or 2D to 1D. So, let’s have a data set like that shown here. And so, I would have a set of examples x(i) which are points in r3. So, I have three dimension examples. I know it might be a little bit hard to see this on the slide, but I’ll show a 3D point cloud in a little bit. And it might be hard to see here, but all of this data maybe lies roughly on the plane, like so. And so what we can do with dimensionality reduction, is take all of this data and project the data down onto a two dimensional plane. So, here what I’ve done is, I’ve taken all the data and I’ve projected all of the data, so that it all lies on the plane. Now, finally, in order to specify the location of a point within a plane, we need two numbers, right? We need to, maybe, specify the location of a point along this axis, and then also specify it’s location along that axis. So, we need two numbers, maybe called z1 and z2 to specify the location of a point within a plane. And so, what that means, is that we can now represent each example, each training example, using two numbers that I’ve drawn here, z1, and z2. So, our data can be represented using vector z which are in r2. And these subscript, z subscript 1, z subscript 2, what I just mean by that is that my vectors here, z, you know, are two dimensional vectors, z1, z2. And so if I have some particular examples, z(i), or that’s the two dimensional vector, z(i)1, z(i)2. And on the previous slide when I was reducing data to one dimensional data then I had only z1, right? And that is what a z1 subscript 1 on the previous slide was, but here I have two dimensional data, so I have z1 and z2 as the two components of the data. </p>
<p>Now, let me just make sure that these figures make sense. So let me just reshow these exact three figures again but with 3D plots. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/5.png" alt="3d_plots"></p>
<p>So the process we went through was that shown in the lab is the optimal data set, in the middle the data set projects on the 2D, and on the right the 2D data sets with z1 and z2 as the axis. Let’s look at them a little bit further. Here’s my original data set, shown on the left, and so I had started off with a 3D point cloud like so, where the axis are labeled x1, x2, x3, and so there’s a 3D point but most of the data, maybe roughly lies on some, you know, not too far from some 2D plain. So, what we can do is take this data and here’s my middle figure. I’m going to project it onto 2D. So, I’ve projected this data so that all of it now lies on this 2D surface. As you can see all the data lies on a plane, ‘cause we’ve projected everything onto a plane, and so what this means is that now I need only two numbers, z1 and z2, to represent the location of point on the plane. And so that’s the process that we can go through to reduce our data from three dimensional to two dimensional. So that’s dimensionality reduction and how we can use it to compress our data. And as we’ll see later this will allow us to make some of our learning algorithms run much later as well, but we’ll get to that only in a later video.</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><ul>
<li>We may want to reduce the dimension of our features if we have a lot of redundant data. </li>
<li>To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line. </li>
</ul>
<p>Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm.</p>
<p>Note: in dimensionality reduction, we are reducing our features rather than our number of examples. Our variable m will stay the same size; n, the number of features each example from $x^{(1)}$ to $x^{(m)}$ carries, will be reduced. </p>
<h3 id="02-motivation-ii-visualization"><a href="#02-motivation-ii-visualization" class="headerlink" title="02_motivation-ii-visualization"></a>02_motivation-ii-visualization</h3><p>In the last video, we talked about dimensionality reduction for the purpose of compressing the data. In this video, I’d like to tell you about <strong>a second application of dimensionality reduction and that is to visualize the data</strong>. For a lot of machine learning applications, it really helps us to develop effective learning algorithms, if we can understand our data better. If there is some way of visualizing the data better, and so, dimensionality reduction offers us, often, another useful tool to do so. </p>
<p>Let’s start with an example. Let’s say we’ve collected a large data set of many statistics and facts about different countries around the world. So, maybe the first feature, X1 is the country’s GDP, or the Gross Domestic Product, and X2 is a per capita, meaning the per person GDP, X3 human development index, life expectancy, X5, X6 and so on. And we may have a huge data set like this, where, you know, maybe 50 features for every country, and we have a huge set of countries. So is there something we can do to try to understand our data better? I’ve given this huge table of numbers. How do you visualize this data? If you have 50 features, it’s very difficult to plot 50-dimensional data. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/6.png" alt="50-features_of_GDP_of_various_contries"></p>
<p>What is a good way to examine this data? Using dimensionality reduction, what we can do is, instead of having each country represented by this featured vector, xi, which is 50-dimensional, so instead of, say, having a country like Canada, instead of having 50 numbers to represent the features of Canada, let’s say we can come up with a different feature representation that is these z vectors, that is in R2. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/7.png" alt="50-features_tp_2-features_of_GDP_of_various_contries"></p>
<p>If that’s the case, if we can have just a pair of numbers, z1 and z2 that somehow, summarizes my 50 numbers, maybe what we can do  [xx] is to plot these countries in R2 and use that to try to understand the space in [xx] of features of different countries [xx]  the better and so, here, what you can do is reduce the data from 50 D, from 50 dimensions to 2D, so you can plot this as a 2 dimensional plot, and, when you do that, it turns out that, if you look at the output of the Dimensionality Reduction algorithms, It usually doesn’t astride a physical meaning to these new features you want $z_1,z_2$. It’s often up to us to figure out you know, roughly what these features means. But, And if you plot those features, here is what you might find. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/8.png" alt="a_plot_of_2-features_of_GDP_of_various_contries"></p>
<p>So, here, every country is represented by a point ZI, which is an R2 and so each of those. Dots, and this figure represents a country, and so, here’s Z1 and here’s Z2, and a  couple of these. So, you might find, for example, That the horizontial axis the Z1 axis corresponds roughly to the overall country size, or the overall economic activity of a country. So the overall GDP, overall economic size of a country. Whereas the vertical axis in our data might correspond to the per person GDP. Or the per person well being, or the per person economic activity, and, you might find that, given these 50 features, you know, these are really the 2 main dimensions of the deviation, and so, out here you may have a country like the U.S.A., which is a relatively large GDP, you know, is a very large GDP and a relatively high per-person GDP as well. Whereas here you might have a country like Singapore, which actually has a very high per person GDP as well, but because Singapore is a much smaller country the overall economy size of Singapore is much smaller than the US. And, over here, you would have countries where individuals are unfortunately some are less well off, maybe shorter life expectancy, less health care, less economic maturity that’s why smaller countries, whereas a point like this will correspond to a country that has a fair, has a substantial amount of economic activity, but where individuals tend to be somewhat less well off. So you might find that the axes Z1 and Z2 can help you to most succinctly capture really what are the two main dimensions of the variations amongst different countries. Such as the overall economic activity of the country projected by the size of the country’s overall economy as well as the per-person individual well-being, measured by per-person GDP, per-person healthcare, and things like that. So that’s how you can use dimensionality reduction, in order to reduce data from 50 dimensions or whatever, down to two dimensions, or maybe down to three dimensions, so that you can plot it and understand your data better. </p>
<p>In the next video, we’ll start to develop a specific algorithm, called PCA, or Principal Component Analysis, which will allow us to do this and also do the earlier application I talked about of compressing the data.</p>
<h4 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h4><p><strong>Motivation II: Visualization</strong><br>It is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.<br>We need to find new features, $z_1,z_2$ (and perhaps $z_3$ ) that can effectively <strong>summarize</strong> all the other features.<br>Example: hundreds of features related to a country’s economic system may all be combined into one feature that you call “Economic Activity.” </p>
<h2 id="02-principal-component-analysis"><a href="#02-principal-component-analysis" class="headerlink" title="02_principal-component-analysis"></a>02_principal-component-analysis</h2><h3 id="01-principal-component-analysis-problem-formulation"><a href="#01-principal-component-analysis-problem-formulation" class="headerlink" title="01_principal-component-analysis-problem-formulation"></a>01_principal-component-analysis-problem-formulation</h3><p>For the problem of dimensionality reduction, by far the most popular, by far the most commonly used algorithm is something called <strong>principle components analysis, or PCA</strong>. </p>
<p>In this video, I’d like to start talking about the problem formulation for PCA. In other words, let’s try to formulate, precisely, exactly what we would like PCA to do. Let’s say we have a data set like this. So, this is a data set of examples x and R2 and let’s say I want to reduce the dimension of the data from two-dimensional to one-dimensional. In other words, I would like to find a line onto which to project the data. So what seems like a good line onto which to project the data, it’s a line like this, might be a pretty good choice. And the reason we think this might be a good choice is that if you look at where the projected versions of the point scales, so I take this point and project it down here. Get that, this point gets projected here, to here, to here, to here. What we find is that the distance between each point and the projected version is pretty small. <strong>That is, these blue line segments are pretty short. So what PCA does formally is it tries to find a lower dimensional surface, really a line in this case, onto which to project the data so that the sum of squares of these little blue line segments is minimized. The length of those blue line segments, that’s sometimes also called the projection error. And so what PCA does is it tries to find a surface onto which to project the data so as to minimize that.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/9.png" alt=""></p>
<p><strong>As an aside, before applying PCA, it’s standard practice to first perform mean normalization at feature scaling so that the features x1 and x2 should have zero mean, and should have comparable ranges of values</strong>. I’ve already done this for this example, but I’ll come back to this later and talk more about feature scaling and the normalization in the context of PCA later. But coming back to this example, in contrast to the red line that I just drew, here’s a different line onto which I could project my data, which is this magenta line. And, as we’ll see, this magenta line is a much worse direction onto which to project my data, right? So if I were to project my data onto the magenta line, we’d get a set of points like that. And the projection errors, that is these blue line segments, will be huge. So these points have to move a huge distance in order to get projected onto the magenta line. And so that’s why PCA, principal components analysis, will choose something like the red line rather than the magenta line down here. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/10.png" alt="PCA_1-2"></p>
<p>Let’s write out the PCA problem a little more formally. The goal of PCA, if we want to reduce data from two-dimensional to one-dimensional is, we’re going to try find a vector that is a vector u1, which is going to be an Rn, so that would be an R2 in this case. I’m gonna find the direction onto which to project the data, so it’s to minimize the projection error. So, in this example I’m hoping that PCA will find this vector, which l wanna call u(1), so that when I project the data onto the line that I define by extending out this vector, I end up with pretty small reconstruction errors. And that reference of data that looks like this. And by the way, I should mention that where the PCA gives me u(1) or -u(1), doesn’t matter. So if it gives me a positive vector in this direction, that’s fine. If it gives me the opposite vector facing in the opposite direction, so that would be like minus u(1). Let’s draw that in blue instead, right? But it gives a positive u(1) or negative u(1), it doesn’t matter because each of these vectors defines the same red line onto which I’m projecting my data. So this is a case of reducing data from two-dimensional to one-dimensional. In the more general case we have n-dimensional data and we’ll want to reduce it to k-dimensions. In that case we want to find not just a single vector onto which to project the data but we want to find k-dimensions onto which to project the data. So as to minimize this projection error. So here’s the example. If I have a 3D point cloud like this, then maybe what I want to do is find vectors. So find a pair of vectors. And I’m gonna call these vectors. Let’s draw these in red. I’m going to find a pair of vectors, sustained from the origin. Here’s u(1), and plane, or they define a 2D surface, right? Like this with a 2D surface onto which I am going to project my data. For those of you that are familiar with linear algebra, for this year they’re really experts in linear algebra, the formal definition of this is that we are going to find the set of vectors u(1), u(2), maybe up to u(k). And what we’re going to do is project the data onto the linear subspace spanned by this set of k vectors. But if you’re not familiar with linear algebra, just think of it as finding k directions instead of just one direction onto which to project the data. So finding a k-dimensional surface is really finding a 2D plane in this case, shown in this figure, where we can define the position of the points in a plane using k directions. And that’s why for PCA we want to find k vectors onto which to project the data. And so more formally in PCA, what we want to do is find this way to project the data so as to minimize the sort of projection distance, which is the distance between the points and the projections. And so in this 3D example too. Given a point we would take the point and project it onto this 2D surface. We are done with that. And so the projection error would be, the distance between the point and where it gets projected down to my 2D surface. And so what PCA does is I try to find the line, or a plane, or whatever, onto which to project the data, to try to minimize that square projection, that 90 degree or that orthogonal projection error. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/11.png" alt=""></p>
<p>Finally, one question I sometimes get asked is how does PCA relate to linear regression? Because when explaining PCA, I sometimes end up drawing diagrams like these and that looks a little bit like linear regression. <strong>It turns out PCA is not linear regression and despite some cosmetic similarity, these are actually totally different algorithms.</strong> If we were doing linear regression, what we would do would be, <em>on the left we would be trying to predict the value of some variable y given some info features x</em>. <strong>And so linear regression, what we’re doing is we’re fitting a straight line so as to minimize the square error between point and this straight line.</strong> And so what we’re minimizing would be the squared magnitude of these blue lines. And notice that I’m drawing these blue lines vertically. That these blue lines are the vertical distance between the point and the value predicted by the hypothesis. <strong>Whereas in contrast, in PCA</strong>, what it does is it tries to minimize the magnitude of these blue lines, which are drawn at an angle. These are really the shortest <strong>orthogonal distances</strong>. The shortest distance between the point x and this red line. And this gives very different effects depending on the dataset. And more generally, when you’re doing linear regression, there is this distinguished variable y they we’re trying to predict. All that linear regression as well as taking all the values of x and try to use that to predict y. Whereas in PCA, there is no distinguish, or there is no special variable y that we’re trying to predict. And instead, we have a list of features, x1, x2, and so on, up to xn, and all of these features are treated equally,so no one of them is special. </p>
<p>As one last example, if I have three-dimensional data and I want to reduce data from 3D to 2D, so maybe I wanna find two directions, u(1) and u(2), onto which to project my data. Then what I have is I have three features, x1, x2, x3, and all of these are treated alike. All of these are treated symmetrically and there’s no special variable y that I’m trying to predict. And so PCA is not a linear regression, and even though at some cosmetic level they might look related, these are actually very different algorithms. So hopefully you now understand what PCA is doing. It’s trying to find a lower dimensional surface onto which to project the data, so as to minimize this squared projection error. To minimize the square distance between each point and the location of where it gets projected. In the next video, we’ll start to talk about how to actually find this lower dimensional surface onto which to project the data.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/12.png" alt="PCA_is_not_linear_regression"></p>
<h4 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h4><p><strong>Principal Component Analysis Problem Formulation</strong></p>
<p>The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)<br><strong><em>Problem formulation</em></strong><br>Given two features, $x_1$ and $x_2$ we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.<br>The same can be done with three features, where we map them to a plane.<br><strong>The goal of PCA</strong> is to reduce the average of all the distances of every feature to the projection line. This is the <strong>projection error</strong>.<br>Reduce from 2d to 1d: find a direction (a vector $u^{(1)} \in \mathbb{R}^n$) onto which to project the data so as to minimize the projection error.<br>The more general case is as follows:<br>Reduce from n-dimension to k-dimension: Find k vectors $u^{(1)}, u^{(2)}, \dots, u^{(k)}$ onto which to project the data so as to minimize the projection error.<br>If we are converting from 3d to 2d, we will project our data onto two directions (a plane), so k will be 2.<br><strong><em>PCA is not linear regression</em></strong><br>In linear regression, we are minimizing the <strong>squared error</strong> from every point to our predictor line. These are vertical distances.<br>In PCA, we are minimizing the <strong>shortest distance</strong> , or shortest orthogonal distances, to our data points.<br>More generally, in linear regression we are taking all our examples in x and applying the parameters in Θ to predict y.<br>In PCA, we are taking a number of features $x_1, x_2, \dots, x_n$, and finding a closest common dataset among them. We aren’t trying to predict any result and we aren’t applying any theta weights to the features. </p>
<h3 id="02-principal-component-analysis-algorithm"><a href="#02-principal-component-analysis-algorithm" class="headerlink" title="02_principal-component-analysis-algorithm"></a>02_principal-component-analysis-algorithm</h3><p>In this video I’d like to tell you about the <strong>principle components analysis algorithm</strong>. And by the end of this video you know to implement PCA for yourself.</p>
<p>And use it reduce the dimension of your data. Before applying PCA,there is a data pre-processing step which you should always do.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/13.png" alt="Data_preprocessing_of_PCA"></p>
<p>Given the trading sets of the examples is important to always perform mean normalization, and then depending on your data, maybe perform feature scaling as well. this is very similar to the mean normalization and feature scaling process that we have for supervised learning. In fact it’s exactly the same procedure except that we’re doing it now to our unlabeled data, X1 through Xm. So for mean normalization we first compute the mean of each feature and then we replace each feature, X, with X minus its mean, and so this makes each feature now have exactly zero mean The different features have very different scales. So for example, if x1 is the size of a house, and x2 is the number of bedrooms, to use our earlier example, we then also scale each feature to have a comparable range of values. And so, similar to what we had with supervised learning, we would take x, i substitute j, that’s the j feature and so we would subtract of the mean, now that’s what we have on top, and then divide by sj. Here, sj is some measure of the beta values of feature j.  So, it could be the max minus min value, or more commonly, it is the standard deviation of feature j. Having done this sort of data pre-processing, here’s what the PCA algorithm does.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/14.png" alt="an_example_of_PCA"></p>
<p>We saw from the previous video that what PCA does is, it tries to find a lower dimensional sub-space onto which to project the data, so as to minimize the squared projection errors, sum of the squared projection errors, as the square of the length of those blue lines that and so what we wanted to do specifically is find a vector, u1, which specifies that direction or in the 2D case we want to find two vectors, u1 and u2, to define this surface onto which to project the data. So, just as a quick reminder of what reducing the dimension of the data means, for this example on the left we were given the examples xI, which are in r2. And what we like to do is find a set of numbers zI in r push to represent our data. So that’s what from reduction from 2D to 1D means. So specifically by projecting data onto this red line there. We need only one number to specify the position of the points on the line. So i’m going to call that number z or z1. Z here  [xx] real number, so that’s like a one dimensional vector. So z1 just refers to the first component of this, you know, one by one matrix, or this one dimensional vector. And so we need only one number to specify the position of a point. So if this example here was my example X1, then maybe that gets mapped here. And if this example was X2 maybe that example gets mapped And so this point here will be Z1 and this point here will be Z2, and similarly we would have those other points for These, maybe X3, X4, X5 get mapped to Z1, Z2, Z3. So What PCA has to do is we need to come up with a way to compute two things. One is to compute these vectors, u1, and in this case u1 and u2. And the other is how do we compute these numbers, Z. So on the example on the left we’re reducing the data from 2D to 1D. In the example on the right, we would be reducing data from 3 dimensional as in r3, to zi, which is now two dimensional. So these z vectors would now be two dimensional. So it would be z1 z2 like so, and so we need to give away to compute these new representations, the z1 and z2 of the data as well. So how do you compute all of these quantities? It turns out that a mathematical derivation, also the mathematical proof, for what is the right value U1, U2, Z1, Z2, and so on. That mathematical proof is very complicated and beyond the scope of the course. But once you’ve done  [xx] it turns out that the procedure to actually find the value of u1 that you want is not that hard, even though so that the mathematical proof that this value is the correct value is someone more involved and more than i want to get into. But let me just describe the specific procedure that you have to implement in order to compute all of these things, the vectors, u1, u2, the vector z.  </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/15.png" alt="svd_of_PCA"></p>
<p>Here’s the procedure. Let’s say we want to reduce the data to n dimensions to k dimension What we’re going to do is first compute something called the covariance matrix, and the covariance matrix is commonly denoted by this Greek alphabet which is the capital Greek alphabet sigma. It’s a bit unfortunate that the Greek alphabet sigma looks exactly like the summation symbols. So this is the Greek alphabet Sigma is used to denote a matrix and this here is a summation symbol. So hopefully in these slides there won’t be ambiguity about which is Sigma Matrix, the matrix, which is a summation symbol, and hopefully it will be clear from context when I’m using each one. How do you compute this matrix let’s say we want to store it in an octave variable called sigma. What we need to do is compute something called the eigenvectors of the matrix sigma. And an octave, the way you do that is you use this command, u s v equals s v d of sigma. SVD, by the way, stands for singular value decomposition. This is a Much more advanced single value composition. It is much more advanced linear algebra than you actually need to know but now It turns out that when sigma is equal to matrix there is a few ways to compute these are high in vectors and If you are an expert in linear algebra and if you’ve heard of high in vectors before you may know that there is another octet function called I, which can also be used to compute the same thing. and It turns out that the SVD function and the I function it will give you the same vectors, although SVD is a little more numerically stable. So I tend to use SVD, although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix sigma it gives you the same thing. This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite You really don’t need to know what that means, but the SVD and I-functions are different functions but when they are applied to a covariance matrix which can be proved to always satisfy this mathematical property; they’ll always give you the same thing. Okay, that was probably much more linear algebra than you needed to know. In case none of that made sense, don’t worry about it. All you need to know is that this system command you should implement in Octave. And if you’re implementing this in a different language than Octave or MATLAB, what you should do is find the numerical linear algebra library that can compute the SVD or singular value decomposition, and there are many such libraries for probably all of the major programming languages. People can use that to compute the matrices u, s, and d of the covariance matrix sigma. So just to fill in some more details, this covariance matrix sigma will be an n by n matrix. And one way to see that is if you look at the definition this is an n by 1 vector and this here I transpose is 1 by N so the product of these two things is going to be an N by N matrix. 1xN transfers, 1xN, so there’s an NxN matrix and when we add up all of these you still have an NxN matrix. And what the SVD outputs three matrices, u, s, and v.  <strong>The thing you really need out of the SVD is the u matrix. The u matrix will also be a NxN matrix. And if we look at the columns of the U matrix it turns out that the columns of the U matrix will be exactly those vectors, u1, u2 and so on. So u, will be matrix. And if we want to reduce the data from n dimensions down to k dimensions, then what we need to do is take the first k vectors. that gives us u1 up to uK which gives us the K direction onto which we want to project the data.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/16.png" alt="Ureduce-matrix_of_PCA"></p>
<p>The rest of the procedure from this SVD numerical linear algebra routine we get this matrix u.  We’ll call these columns u1-uN. So, just to wrap up the description of the rest of the procedure, from the SVD numerical linear algebra routine we get these matrices u, s, and d.  we’re going to use the first K columns of this matrix to get u1-uK. Now the other thing we need to is take my original data set, X which is an RN And find a lower dimensional representation Z, which is a R K for this data. So the way we’re going to do that is take the first K Columns of the U matrix. Construct this matrix. Stack up U1, U2 and so on up to U K in columns. It’s really basically taking, you know, this part of the matrix, the first K columns of this matrix. And so this is going to be an N by K matrix. I’m going to give this matrix a name. I’m going to call this matrix U, subscript “reduce,” sort of a reduced version of the U matrix maybe. I’m going to use it to reduce the dimension of my data. And the way I’m going to compute Z is going to let Z be equal to this U reduce matrix transpose times X. Or alternatively, you know, to write down what this transpose means. When I take this transpose of this U matrix, what I’m going to end up with is these vectors now in rows. I have U1 transpose down to UK transpose. Then take that times X, and that’s how I get my vector Z. Just to make sure that these dimensions make sense, this matrix here is going to be k by n and x here is going to be n by 1 and so the product here will be k by 1. And so z is k dimensional, is a k dimensional vector, which is exactly what we wanted. And of course these x’s here right, can be Examples in our training set can be examples in our cross validation set, can be examples in our test set, and for example if you know, I wanted to take training example i, I can write this as xi XI and that’s what will give me ZI over there. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/19.png" alt="implementation_of_PCA_in_octave_code"></p>
<p>So, to summarize, here’s the PCA algorithm on one slide. After mean normalization, to ensure that every feature is zero mean and optional feature scaling whichYou really should do feature scaling if your features take on very different ranges of values. After this pre-processing we compute the carrier matrix Sigma like so by the way if your data is given as a matrix like hits if you have your data Given in rows like this. If you have a matrix X which is your time trading sets written in rows where x1 transpose down to x1 transpose, this covariance matrix sigma actually has a nice vectorizing implementation. You can implement in octave, you can even run sigma equals 1 over m, times x, which is this matrix up here, transpose times x and this simple expression, that’s the vectorize implementation of how to compute the matrix sigma. I’m not going to prove that today. This is the correct vectorization whether you want, you can either numerically test this on yourself by trying out an octave and making sure that both this and this implementations give the same answers or you Can try to prove it yourself mathematically. Either way but this is the correct vectorizing implementation, without compusingnext we can apply the SVD routine to get u, s, and d. And then we grab the first k columns of the u matrix you reduce and finally this defines how we go from a feature vector x to this reduce dimension representation z. And similar to k Means if you’re apply PCA, they way you’d apply this is with vectors X and RN. So, this is not done with X-0 1. So that was the PCA algorithm. One thing I didn’t do is give a mathematical proof that this There it actually give the projection of the data onto the K dimensional subspace onto the K dimensional surface that actually minimizes the square projection error Proof of that is beyond the scope of this course. Fortunately the PCA algorithm can be implemented in not too many lines of code. and if you implement this in octave or algorithm, you actually get a very effective dimensionality reduction algorithm. </p>
<p>So, that was the PCA algorithm. One thing I didn’t do was give a mathematical proof that the U1 and U2 and so on and the Z and so on you get out of this procedure is really the choices that would minimize these squared projection error. Right, remember we said What PCA tries to do is try to find a surface or line onto which to project the data so as to minimize to square projection error. So I didn’t prove that this that, and the mathematical proof of that is beyond the scope of this course. But fortunately the PCA algorithm can be implemented in not too many lines of octave code. And if you implement this, this is actually what will work, or this will work well, and if you implement this algorithm, you get a very effective dimensionality reduction algorithm. That does do the right thing of minimizing this square projection error</p>
<h4 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h4><p>Before we can apply PCA, there is a data pre-processing step we must perform:<br><strong>Data preprocessing</strong><br>Given training set: x(1),x(2),…,x(m)<br>Preprocess (feature scaling/mean normalization):<br>$\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}$<br>Replace each $x_j^{(i)}$ with $x_j^{(i)} - \mu_j$<br>If different features on different scales (e.g., $x_1$ = size of house, $x_2$ = number of bedrooms), scale features to have comparable range of values.<br>Above, we first subtract the mean of each feature from the original feature. Then we scale all the features $x_j^{(i)} = \dfrac{x_j^{(i)} - \mu_j}{s_j}$<br>We can define specifically what it means to reduce from 2d to 1d data as follows:<br>$$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$<br>The z values are all real numbers and are the projections of our features onto $u^{(1)}$.<br>So, PCA has two tasks: figure out $u^{(1)},\dots,u^{(k)}$ and also to find $z_1, z_2, \dots, z_m$.<br>The mathematical proof for the following procedure is complicated and beyond the scope of this course.<br><strong>1. Compute “covariance matrix”</strong><br>$$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$<br>This can be vectorized in Octave as: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sigma &#x3D; (1&#x2F;m) * X&#39; * X;</span><br></pre></td></tr></table></figure>
<p>We denote the covariance matrix with a capital sigma (which happens to be the same symbol for summation, confusingly—they represent entirely different things).<br>Note that $x^{(i)}$ is an n×1 vector, $(x^{(i)})^T$ is an 1×n vector and X is a m×n matrix (row-wise stored examples). The product of those will be an n×n matrix, which are the dimensions of Σ.<br><strong>2. Compute “eigenvectors” of covariance matrix Σ</strong><br>[U,S,V] = svd(Sigma);<br>svd() is the ‘singular value decomposition’, a built-in Octave function.<br>What we actually want out of svd() is the ‘U’ matrix of the Sigma covariance matrix: $U \in \mathbb{R}^{n \times n}$. U contains $u^{(1)},\dots,u^{(n)}$, which is exactly what we want. </p>
<p><strong>3. Take the first k columns of the U matrix and compute z</strong><br>We’ll assign the first k columns of U to a variable called ‘Ureduce’. This will be an n×k matrix. We compute z with:<br>$$z^{(i)} = Ureduce^T \cdot x^{(i)}$$<br>$UreduceZ^T$ will have dimensions k×n while x(i) will have dimensions n×1. The product $Ureduce^T \cdot x^{(i)}$ will have dimensions k×1.<br>To summarize, the whole algorithm in octave is roughly: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Sigma &#x3D; (1&#x2F;m) * X&#39; * X; % compute the covariance matrix</span><br><span class="line">[U,S,V] &#x3D; svd(Sigma);   % compute our projected directions</span><br><span class="line">Ureduce &#x3D; U(:,1:k);     % take the first k directions</span><br><span class="line">Z &#x3D; X * Ureduce;        % compute the projected data</span><br></pre></td></tr></table></figure>

<h2 id="03-applying-pca"><a href="#03-applying-pca" class="headerlink" title="03_applying-pca"></a>03_applying-pca</h2><h3 id="01-reconstruction-from-compressed-representation"><a href="#01-reconstruction-from-compressed-representation" class="headerlink" title="01_reconstruction-from-compressed-representation"></a>01_reconstruction-from-compressed-representation</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/20.png" alt="reconstruction_from_compressed_representation"></p>
<p>In some of the earlier videos, I was talking about PCA as a compression  algorithm where you may have say, 1,000-dimensional data and compres  it to 100-dimensional feature vector. Or have three-dimensional data and compress it to a two-dimensiona  representation. So, if this is a compression algorithm, there should be a way to go bac  from this compressed representation back to an approximation of you  original high-dimensional data. So given zi, which may  100-dimensional, how do you go back to your original representation, xi which was maybe a 1000-dimensional. In this video, I’d like to describe how to do that. In the PCA algorithm, we may have an example like this, so maybe that’s my example x1, and maybe that’s my example x2. And what we do is we take these examples, and we project them onto this one dimensional surface. And then now we need to use a real number, say z1, to specify the location of these points after they’ve been projected onto this one dimensional surface. So, given the point z1, how can we go back to this original two dimensional space? In particular, given the point z, which is R, can we map this back to some approximate representation x and R2 of whatever the original value of the data was? So whereas z equals U reduce transpose x, if you want to go in the opposite direction, the equation for that is, we’re going to write x approx equals U reduce, times z. And again, just to check the dimensions, here U reduce is going to be an n by k dimensional vector, z is going to be k by one dimensional vector. So you multiply these out that’s going to be n by one, so x approx is going to be an n dimensional vector. And so the intent of PCA, that is if the square projection error is not too big, is that this x approx will be close to whatever was the original value of x that you have used to derive z in the first place. To show a picture of what this looks like, this is what it looks like. What you get back of this procedure are points that lie on the projection of that, onto the green line. So to take our early example, if we started off with this value of x1, and we got this value of z1, if you plug z1 through this formula to get x1 approx, then this point here, that would be x1 approx, which is going to be in R2. And similarly, if you do the same procedure, this would be x2 approx. And that’s a pretty decent approximation to the original data. So that’s how you go back from your low dimensional representation z, back to an uncompressed representation of the data. We get back an approximation to your original data x. And we also call this process reconstruction of the original data where we think of trying to reconstruct the original value of x from the compressed representation. So, given an unlabeled data set, you now know how to apply PCA and take your high dimensional features x and map that to this lower-dimensional representation z. And from this video hopefully you now also know how to take these low-representation z and map it back up to an approximation of your original high-dimensional data</p>
<p>Now that you know how to implement and apply PCA, what I’d like to do next is talk about some of the mechanics of how to actually use PCA well. And in particular in the next video, I’d like to talk about how to choose k, which is how to choose the dimension of the reduced representation vector z.</p>
<h4 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h4><p>If we use PCA to compress our data, how can we uncompress our data, or go back to our original number of features?<br>To go from 1-dimension back to 2d we do: $z \in \mathbb{R} \rightarrow x \in \mathbb{R}^2$.<br>We can do this with the equation:<br>$$x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}$$.<br>Note that we can only get approximations of our original data.<br>Note: It turns out that the U matrix has the special property that it is a Unitary Matrix. One of the special properties of a Unitary Matrix is:<br>$U^{-1} = U^<em>$ where the “</em>“ means “conjugate transpose”.<br>Since we are dealing with real numbers here, this is equivalent to:<br>$U^{-1} = U^T$ So we could compute the inverse and use that, but it would be a waste of energy and compute cycles. </p>
<h3 id="02-choosing-the-number-of-principal-components"><a href="#02-choosing-the-number-of-principal-components" class="headerlink" title="02_choosing-the-number-of-principal-components"></a>02_choosing-the-number-of-principal-components</h3><p>In the PCA algorithm we take N dimensional features and reduce them to some K dimensional feature representation. <strong>This number K is a parameter of the PCA algorithm. This number K is also called the number of principle components or the number of principle components that we’ve retained.</strong> And in this video <strong>I’d like to give you some guidelines, tell you about how people tend to think about how to choose this parameter K for PCA.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/21.png" alt="choosing_k_of_PCA"></p>
<p>In order to choose k, that is to choose the number of principal components, here are a couple of useful concepts. What PCA tries to do is it tries to minimize the average squared projection error. So it tries to minimize this quantity, which I’m writing down, which is the difference between the original data X and the projected version, X-approx-i, which was defined last video, so it tries to minimize the squared distance between x and it’s projection onto that lower dimensional surface. So that’s the average square projection error. Also let me define the total variation in the data to be the average length squared of these examples Xi so the total variation in the data is the average of my training sets of the length of each of my training examples. And this one says, “On average, how far are my training examples from the vector, from just being all zeros?” How far is, how far on average are my training examples from the origin? When we’re trying to choose k, a pretty common rule of thumb for choosing k is to choose the smaller values so that the ratio between these is less than 0.01. So in other words, a pretty common way to think about how we choose k is we want the average squared projection error. That is the average distance between x and it’s projections divided by the total variation of the data. That is how much the data varies. We want this ratio to be less than, let’s say, 0.01. Or to be less than 1%, which is another way of thinking about it. And the way most people think about choosing K is rather than choosing K directly the way most people talk about it is as what this number is, whether it is 0.01 or some other number. And if it is 0.01, another way to say this to use the language of PCA is that 99% of the variance is retained. I don’t really want to, don’t worry about what this phrase really means technically but this phrase “99% of variance is retained” just means that this quantity on the left is less than 0.01. And so, if you are using PCA and if you want to tell someone, you know, how many principle components you’ve retained it would be more common to say well, I chose k so that 99% of the variance was retained. And that’s kind of a useful thing to know, it means that you know, the average squared projection error divided by the total variation that was at most 1%. That’s kind of an insightful thing to think about, whereas if you tell someone that, “Well I had to 100 principle components” or “k was equal to 100 in a thousand dimensional data” it’s a little hard for people to interpret that. So this number 0.01 is what people often use. Other common values is 0.05, and so this would be 5%, and if you do that then you go and say well 95% of the variance is retained and, you know other numbers maybe 90% of the variance is retained, maybe as low as 85%. So 90% would correspond to say 0.10, kinda 10%. And so range of values from, you know, 90, 95, 99, maybe as low as 85% of the variables contained would be a fairly typical range in values. Maybe 95 to 99 is really the most common range of values that people use. For many data sets you’d be surprised, in order to retain 99% of the variance, you can often reduce the dimension of the data significantly and still retain most of the variance. Because for most real life data says many features are just highly correlated, and so it turns out to be possible to compress the data a lot and still retain you know 99% of the variance or 95% of the variance. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/22.png" alt="choosing_k_of_PCA_by_counting_singular_values"></p>
<p>So how do you implement this? Well, here’s one algorithm that you might use. You may start off, if you want to choose the value of k, we might start off with k equals 1. And then we run through PCA. You know, so we compute, you reduce, compute z1, z2, up to zm. Compute all of those x1 approx and so on up to xm approx and then we check if 99% of the variance is retained. Then we’re good and we use k equals 1. But if it isn’t then what we’ll do we’ll next try K equals 2. And then we’ll again run through this entire procedure and check, you know is this expression satisfied. Is this less than 0.01. And if not then we do this again. Let’s try k equals 3, then try k equals 4, and so on until maybe we get up to k equals 17 and we find 99% of the data have is retained and then we use k equals 17, right? That is one way to choose the smallest value of k, so that and 99% of the variance is retained. But as you can imagine, this procedure seems horribly inefficient we’re trying k equals one, k equals two, we’re doing all these calculations. Fortunately when you implement PCA it actually, in this step, it actually gives us a quantity that makes it much easier to compute these things as well. Specifically when you’re calling SVD to get these matrices u, s, and d, when you’re calling usvd on the covariance matrix sigma, it also gives us back this matrix S and what S is, is going to be a square matrix an N by N matrix in fact, that is diagonal. So is diagonal entries s one one, s two two, s three three down to s n n are going to be the only non-zero elements of this matrix, and everything off the diagonals is going to be zero. Okay? So those big O’s that I’m drawing, by that what I mean is that everything off the diagonal of this matrix all of those entries there are going to be zeros. And so, what is possible to show, and I won’t prove this here, and it turns out that for a given value of k, this quantity over here can be computed much more simply. And that quantity can be computed as one minus sum from i equals 1 through K of Sii divided by sum from I equals 1 through N of Sii. So just to say that it words, or just to take another view of how to explain that, if K equals 3 let’s say. What we’re going to do to compute the numerator is sum from one–  I equals 1 through 3 of of Sii, so just compute the sum of these first three elements. So that’s the numerator. And then for the denominator, well that’s the sum of all of these diagonal entries. And one minus the ratio of that, that gives me this quantity over here, that I’ve circled in blue. And so, what we can do is just test if this is less than or equal to 0.01. Or equivalently, we can test if the sum from i equals 1 through k, s-i-i divided by sum from i equals 1 through n, s-i-i if this is greater than or equal to 4.99, if you want to be sure that 99% of the variance is retained. And so what you can do is just slowly increase k, set k equals one, set k equals two, set k equals three and so on, and just test this quantity to see what is the smallest value of k that ensures that 99% of the variance is retained. And if you do this, then you need to call the SVD function only once. Because that gives you the S matrix and once you have the S matrix, you can then just keep on doing this calculation by increasing the value of K in the numerator and so you don’t need keep to calling SVD over and over again to test out the different values of K. So this procedure is much more efficient, and this can allow you to select the value of K without needing to run PCA from scratch over and over. You just run SVD once, this gives you all of these diagonal numbers, all of these numbers S11, S22 down to SNN, and then you can just you know, vary K in this expression to find the smallest value of K, so that 99% of the variance is retained. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/23.png" alt="choosing_k_of_PCA_by_counting_singular_values2"></p>
<p>So to summarize, the way that I often use, the way that I often choose K when I am using PCA for compression is I would call SVD once in the covariance matrix, and then I would use this formula and pick the smallest value of K for which this expression is satisfied. And by the way, even if you were to pick some different value of K, even if you were to pick the value of K manually, you know maybe you have a thousand dimensional data and I just want to choose K equals one hundred. Then, if you want to explain to others what you just did, a good way to explain the performance of your implementation of PCA to them, is actually to take this quantity and compute what this is, and that will tell you what was the percentage of variance retained. And if you report that number, then, you know, people that are familiar with PCA, and people can use this to get a good understanding of how well your hundred dimensional representation is approximating your original data set, because there’s 99% of variance retained. That’s really a measure of your square of construction error, that ratio being 0.01, just gives people a good intuitive sense of whether your implementation of PCA is finding a good approximation of your original data set. </p>
<p>So hopefully, that gives you an efficient procedure for choosing the number K. For choosing what dimension to reduce your data to, and if you apply PCA to very high dimensional data sets, you know, to like a thousand dimensional data, very often, just because data sets tend to have highly correlated features, this is just a property of most of the data sets you see, you often find that PCA will be able to retain ninety nine percent of the variance or say, ninety five ninety nine, some high fraction of the variance, even while compressing the data by a very large factor.</p>
<h4 id="summary-5"><a href="#summary-5" class="headerlink" title="summary"></a>summary</h4><p>How do we choose k, also called <em>the number of principal components</em> ? Recall that k is the dimension we are reducing to.<br>One way to choose k is by using the following formula: </p>
<ul>
<li>Given the average squared projection error: $\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$ </li>
<li>Also given the total variation in the data: $\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2$ </li>
<li>Choose k to be the smallest value such that: $\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01$ </li>
</ul>
<p>In other words, the squared projection error divided by the total variation should be less than one percent, so that 99% of the variance is retained .<br><strong>Algorithm for choosing k</strong></p>
<ol>
<li>Try PCA with k=1,2,… </li>
<li>Compute $U_{reduce}, z, x$ </li>
<li>Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k. </li>
</ol>
<p>This procedure would actually be horribly inefficient. In Octave, we will call svd: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] &#x3D; svd(Sigma)</span><br></pre></td></tr></table></figure>
<p>Which gives us a matrix S. We can actually check for 99% of retained variance using the S matrix as follows: </p>
<p>$$\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99$$ </p>
<h3 id="03-advice-for-applying-pca"><a href="#03-advice-for-applying-pca" class="headerlink" title="03_advice-for-applying-pca"></a>03_advice-for-applying-pca</h3><p>In an earlier video, I had said that <strong>PCA can be sometimes used to speed up the running time of a learning algorithm</strong>. In this video, I’d like to explain <strong>how to actually do that</strong>, and also say some, just try to give <strong>some advice about how to apply PCA.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/24.png" alt="speeding_up_supervised_learning_by_PCA"></p>
<p>Here’s how you can use PCA to speed up a learning algorithm, and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA. Let’s say you have a supervised learning problem, note this is a supervised learning problem with inputs X and labels Y, and let’s say that your examples xi are very high dimensional. So, lets say that your examples, xi are 10,000 dimensional feature vectors. One example of that, would be, if you were doing some computer vision problem, where you have a 100x100 images, and so if you have 100x100, that’s 10000 pixels, and so if xi are, you know, feature vectors that contain your 10000 pixel intensity values, then you have 10000 dimensional feature vectors. So with very high-dimensional feature vectors like this, running a learning algorithm can be slow, right? Just, if you feed 10,000 dimensional feature vectors into logistic regression, or a new network, or support vector machine or what have you, just because that’s a lot of data, that’s 10,000 numbers, it can make your learning algorithm run more slowly. Fortunately with PCA we’ll be able to reduce the dimension of this data and so make our algorithms run more efficiently. Here’s how you do that. We are going first check our labeled training set and extract just the inputs, we’re just going to extract the X’s and temporarily put aside the Y’s. So this will now give us an unlabelled training set x1 through xm which are maybe there’s a ten thousand dimensional data, ten thousand dimensional examples we have. So just extract the input vectors x1 through xm. Then we’re going to apply PCA and this will give me a reduced dimension representation of the data, so instead of 10,000 dimensional feature vectors I now have maybe one thousand dimensional feature vectors. So that’s like a 10x savings. So this gives me, if you will, a new training set. So whereas previously I might have had an example x1, y1, my first training input, is now represented by z1. And so we’ll have a new sort of training example, which is Z1 paired with y1. And similarly Z2, Y2, and so on, up to ZM, YM. Because my training examples are now represented with this much lower dimensional representation Z1, Z2, up to ZM. Finally, I can take this reduced dimension training set and feed it to a learning algorithm maybe a neural network, maybe logistic regression, and I can learn the hypothesis H, that takes this input, these low-dimensional representations Z and tries to make predictions. So if I were using logistic regression for example, I would train a hypothesis that outputs, you know, one over one plus E to the negative-theta transpose Z, that takes this input to one these z vectors, and tries to make a prediction. And finally, if you have a new example, maybe a new test example X. What you do is you would take your test example x, map it through the same mapping that was found by PCA to get you your corresponding z. And that z then gets fed to this hypothesis, and this hypothesis then makes a prediction on your input x. One final note, what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running PCA only on the training sets. And in particular, this mapping that PCA is learning, right, this mapping, what that does is it computes the set of parameters. That’s the feature scaling and mean normalization. And there’s also computing this matrix U reduced. But all of these things that U reduce, that’s like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced so on, that should be obtained by running PCA only on your training set. And then having found U reduced, or having found the parameters for feature scaling where the mean normalization and scaling the scale that you divide the features by to get them on to comparable scales. Having found all those parameters on the training set, you can then apply the same mapping to other examples that may be In your cross-validation sets or in your test sets, OK? Just to summarize, when you’re running PCA, run your PCA only on the training set portion of the data not the cross-validation set or the test set portion of your data. And that defines the mapping from x to z and you can then apply that mapping to your cross-validation set and your test set and by the way in this example I talked about reducing the data from ten thousand dimensional to one thousand dimensional, this is actually not that unrealistic.<strong>For many problems we actually reduce the dimensional data. You know by 5x maybe by 10x and still retain most of the variance and we can do this barely effecting the performance, in terms of classification accuracy, let’s say, barely affecting the classification accuracy of the learning algorithm. And by working with lower dimensional data our learning algorithm can often run much much faster</strong>. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/25.png" alt="application_of_PCA"></p>
<p>To summarize, we’ve so far talked about the following applications of PCA. First is the compression application where we might do so to reduce the memory or the disk space needed to store data and we just talked about how to use this to speed up a learning algorithm. In these applications, in order to choose K, often we’ll do so according to, figuring out what is the percentage of variance retained, and so for this learning algorithm, speed up application often will retain 99%  of the variance. That would be a very typical choice for how to choose k. So that’s how you choose k for these compression applications. Whereas for visualization applications while usually we know how to plot only two dimensional data or three dimensional data, and so for visualization applications, we’ll usually choose k equals 2 or k equals 3, because we can plot only 2D and 3D data sets. So that summarizes the main applications of PCA, as well as how to choose the value of k for these different applications. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/26.png" alt="application_of_PCA"></p>
<p>I should mention that there is often <strong>one frequent misuse of PCA</strong> and you sometimes hear about others doing this hopefully not too often. I just want to mention this so that you know not to do it. And there is one bad use of PCA, which iss to try to use it to prevent over-fitting. Here’s the reasoning. This is not a great way to use PCA, but here’s the reasoning behind this method, which is,you know if we have Xi, then maybe we’ll have n features, but if we compress the data, and use Zi instead and that reduces the number of features to k, which could be much lower dimensional. And so if we have a much smaller number of features, if k is 1,000 and n is 10,000, then if we have only 1,000 dimensional data, maybe we’re less likely to over-fit than if we were using 10,000-dimensional data with like a thousand features. So some people think of PCA as a way to prevent over-fitting. But just to emphasize this is a bad application of PCA and I do not recommend doing this. And it’s not that this method works badly. If you want to use this method to reduce the dimensional data, to try to prevent over-fitting, it might actually work OK. But this just is not a good way to address over-fitting and instead, if you’re worried about over-fitting, there is a much better way to address it, to use regularization instead of using PCA to reduce the dimension of the data. <strong>And the reason is, if you think about how PCA works, it does not use the labels y. You are just looking at your inputs xi, and you’re using that to find a lower-dimensional approximation to your data. So what PCA does, is it throws away some information. It throws away or reduces the dimension of your data without knowing what the values of y is, so this is probably okay using PCA this way is probably okay if, say 99 percent of the variance is retained, if you’re keeping most of the variance, but it might also throw away some valuable information. And it turns out that if you’re retaining 99% of the variance or 95% of the variance or whatever, it turns out that just using regularization will often give you at least as good a method for preventing over-fitting and regularization will often just work better, because when you are applying linear regression or logistic regression or some other method with regularization, well, this minimization problem actually knows what the values of y are, and so is less likely to throw away some valuable information, whereas PCA doesn’t make use of the labels and is more likely to throw away valuable information</strong>.</p>
<p>So, to summarize, it is a good use of PCA, if your main motivation to speed up your learning algorithm, but using PCA to prevent over-fitting, that is not a good use of PCA, and using regularization instead is really what many people would recommend doing instead. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/14/27.png" alt="last_bad_use_of_PCA"></p>
<p>Finally, <strong>one last misuse of PCA.</strong> And so I should say PCA is a very useful algorithm, I often use it for the compression on the visualization purposes. But, what I sometimes see, is also people sometimes use PCA where it shouldn’t be. So, here’s a pretty common thing that I see, which is if someone is designing a machine-learning system, they may write down the plan like this: let’s design a learning system. Get a training set and then, you know, what I’m going to do is run PCA, then train logistic regression and then test on my test data. So often at the very start of a project, someone will just write out a project plan than says lets do these four steps with PCA inside. Before writing down a project plan the incorporates PCA like this, one very good question to ask is, well, what if we were to just do the whole without using PCA. And often people do not consider this step before coming up with a complicated project plan and implementing PCA and so on. And sometime, and so specifically, what I often advise people is, before you implement PCA, I would first suggest that, you know, do whatever it is, take whatever it is you want to do and first consider doing it with your original raw data xi, and only if that doesn’t do what you want, then implement PCA before using Zi. So, before using PCA you know, instead of reducing the dimension of the data, I would consider well, let’s ditch this PCA step, and I would consider, let’s just train my learning algorithm on my original data. <strong>Let’s just use my original raw inputs xi, and I would recommend, instead of putting PCA into the algorithm, just try doing whatever it is you’re doing with the xi first. And only if you have a reason to believe that doesn’t work, so that only if your learning algorithm ends up running too slowly, or only if the memory requirement or the disk space requirement is too large, so you want to compress your representation, but if only using the xi doesn’t work, only if you have evidence or strong reason to believe that using the xi won’t work, then implement PCA and consider using the compressed representation</strong>. Because what I do see, is sometimes people start off with a project plan that incorporates PCA inside, and sometimes they, whatever they’re doing will work just fine, even without using PCA instead. So, just consider that as an alternative as well, before you go to spend a lot of time to get PCA in, figure out what k is and so on. </p>
<p><strong>So, that’s it for PCA. Despite these last sets of comments, PCA is an incredibly useful algorithm, when you use it for the appropriate applications and I’ve actually used PCA pretty often and for me, I use it mostly to speed up the running time of my learning algorithms. But I think, just as common an application of PCA, is to use it to compress data, to reduce the memory or disk space requirements, or to use it to visualize data. And PCA is one of the most commonly used and one of the most powerful unsupervised learning algorithms.</strong> And with what you’ve learned in these videos, I think hopefully you’ll be able to implement PCA and use them through all of these purposes as well.</p>
<h4 id="summary-6"><a href="#summary-6" class="headerlink" title="summary"></a>summary</h4><p>The most common use of PCA is to speed up supervised learning.<br>Given a training set with a large number of features (e.g. $x^{(1)},\dots,x^{(m)} \in \mathbb{R}^{10000}$ ) we can use PCA to reduce the number of features in each example of the training set (e.g. $z^{(1)},\dots,z^{(m)} \in \mathbb{R}^{1000}$).<br>Note that we should define the PCA reduction from $x^{(i)}$ to $z^{(i)}$ only on the training set and not on the cross-validation or test sets. You can apply the mapping z(i) to your cross-validation and test sets after it is defined on the training set.<br>Applications </p>
<ul>
<li>Compressions </li>
</ul>
<p>Reduce space of data<br>Speed up algorithm </p>
<ul>
<li>Visualization of data </li>
</ul>
<p>Choose k = 2 or k = 3<br><strong>Bad use of PC A:</strong> trying to prevent overfitting. We might think that reducing the features with PCA would be an effective way to address overfitting. It might work, but is not recommended because it does not consider the values of our results y. Using just regularization will be at least as effective.<br>Don’t assume you need to do PCA. <strong>Try your full machine learning algorithm without PCA first.</strong> Then use PCA if you find that you need it.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/13/13_unsupervised-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/13/13_unsupervised-learning/" class="post-title-link" itemprop="url">13_unsupervised-learning note13</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-13 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-13T00:00:00+05:30">2018-01-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 04:37:42" itemprop="dateModified" datetime="2020-04-09T04:37:42+05:30">2020-04-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/English/" itemprop="url" rel="index"><span itemprop="name">English</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>36k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>33 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This personal note is written after studying the opening course on <a href="https://www.coursera.org" target="_blank" rel="noopener">the coursera website</a>, <a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">Machine Learning by Andrew NG</a> . And images, audios of this note all comes from the opening course. </p>
<h2 id="01-clustering"><a href="#01-clustering" class="headerlink" title="01_clustering"></a>01_clustering</h2><h3 id="01-unsupervised-learning-introduction"><a href="#01-unsupervised-learning-introduction" class="headerlink" title="01_unsupervised-learning-introduction"></a>01_unsupervised-learning-introduction</h3><p>In this video, I’d like to start to talk about <strong>clustering</strong>. This will be exciting, because this is our first unsupervised learning algorithm, where we learn from unlabeled data instead from labelled data. </p>
<p>So, what is unsupervised learning? I briefly talked about unsupervised learning at the beginning of the class but it’s useful to contrast it with supervised learning. So, here’s a typical supervised learning problem where we’re given a labeled training set and the goal is to find the decision boundary that separates the positive label examples and the negative label examples. So, the supervised learning problem in this case is given a set of labels to fit a hypothesis to it.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/1.png" alt="supervised_learning_vs_unsupervised_learning1"></p>
<p>In contrast, in the unsupervised learning problem we’re given data that does not have any labels associated with it. So, we’re given data that looks like this.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/2.png" alt="supervised_learning_vs_unsupervised_learning2"></p>
<p>Here’s a set of points add in no labels, and so, our training set is written just x1, x2, and so on up to xm and we don’t get any labels y. And that’s why the points plotted up on the figure don’t have any labels with them. <strong>So, in unsupervised learning what we do is we give this sort of unlabeled training set to an algorithm and we just ask the algorithm find some structure in the data for us.</strong> </p>
<p>Given this data set one type of structure we might have an algorithm find is that it looks like this data set has points grouped into two separate clusters and so an algorithm that finds clusters like the ones I’ve just circled is called <strong><em>a clustering algorithm</em></strong>. And <strong>this would be our first type of unsupervised learning, although there will be other types of unsupervised learning algorithms that we’ll talk about later that finds other types of structure or other types of patterns in the data other than clusters.</strong> We’ll talk about this after we’ve talked about clustering. </p>
<p>So, what is clustering good for? Early in this class I already mentioned a few applications. One is market segmentation where you may have a database of customers and want to group them into different marker segments so you can sell to them separately or serve your different market segments better. Social network analysis. There are actually groups have done this things like looking at a group of people’s social networks. So, things like Facebook, Google+, or maybe information about who other people that you email the most frequently and who are the people that they email the most frequently and to find coherence in groups of people. So, this would be another maybe clustering algorithm where you know want to find who are the coherent groups of friends in the social network? Here’s something that one of my friends actually worked on which is, use clustering to organize computer clusters or to organize data centers better. Because if you know which computers in the data center in the cluster tend to work together, you can use that to reorganize your resources and how you layout the network and how you design your data center communications. And lastly, something that actually another friend worked on using clustering algorithms to understand galaxy formation and using that to understand astronomical data. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/3.png" alt="Applications_of_clustering"></p>
<p>So, that’s clustering which is our first example of an unsupervised learning algorithm. In the next video we’ll start to talk about a specific clustering algorithm.</p>
<h4 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h4><p>Unsupervised learning is contrasted from supervised learning because it uses an <strong>unlabeled</strong> training set rather than a labeled one.<br>In other words, we don’t have the vector y of expected results, we only have a dataset of features where we can find structure.<br>Clustering is good for:</p>
<ul>
<li>Market segmentation </li>
<li>Social network analysis </li>
<li>Organizing computer clusters </li>
<li>Astronomical data analysis</li>
</ul>
<h3 id="02-k-means-algorithm"><a href="#02-k-means-algorithm" class="headerlink" title="02_k-means-algorithm"></a>02_k-means-algorithm</h3><p>In the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us. <strong>The K Means algorithm is by far the most popular, by far the most widely used clustering algorithm</strong>, and in this video I would like to tell you what the K Means Algorithm is and how it works. The K means clustering algorithm is best illustrated in pictures. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/4.png" alt=""></p>
<p>Let’s say I want to take an unlabeled data set like the one shown here, and I want to group the data into two clusters. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/5.png" alt=""></p>
<p>If I run the K Means clustering algorithm, here is what I’m going to do. The first step is to randomly initialize two points, called the cluster centroids. So, these two crosses here, these are called the <strong>Cluster Centroids</strong> and I have two of them because I want to group my data into two clusters. <strong>K Means is an iterative algorithm and it does two things. First is a cluster assignment step, and second is a move centroid step.</strong> </p>
<p>So, let me tell you what those things mean. <strong>The first of the two steps</strong> in the loop of K means, is this <strong><em>cluster assignment step</em></strong>. What that means is that, it’s going through each of the examples, each of these green dots shown here and depending on whether it’s closer to the red cluster centroid or the blue cluster centroid, it is going to assign each of the data points to one of the two cluster centroids. Specifically, what I mean by that, is to go through your data set and color each of the points either red or blue, depending on whether it is closer to the red cluster centroid or the blue cluster centroid, and I’ve done that in this diagram here. So, that was the cluster assignment step. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/6.png" alt=""></p>
<p>The other part of K means, in the loop of K means, is the <strong>move centroid step</strong>, and what we are going to do is, we are going to take the two cluster centroids, that is, the red cross and the blue cross, and we are going to move them to the average of the points colored the same colour. So what we are going to do is look at all the red points and compute the average, really the mean of the location of all the red points, and we are going to move the red cluster centroid there. And the same things for the blue cluster centroid, look at all the blue dots and compute their mean, and then move the blue cluster centroid there. So, let me do that now. We’re going to move the cluster centroids as follows and I’ve now moved them to their new means. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/7.png" alt=""></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/8.png" alt=""></p>
<p>The red one moved like that and the blue one moved like that and the red one moved like that. And then we go back to another cluster assignment step, so we’re again going to look at all of my unlabeled examples and depending on whether it’s closer the red or the blue cluster centroid, I’m going to color them either red or blue. I’m going to assign each point to one of the two cluster centroids, so let me do that now. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/9.png" alt=""></p>
<p>And so the colors of some of the points just changed. And then I’m going to do another move centroid step. So I’m going to compute the average of all the blue points, compute the average of all the red points and move my cluster centroids like this, and so, let’s do that again. Let me do one more cluster assignment step. So colour each point red or blue, based on what it’s closer to and then do another move centroid step and we’re done.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/10.png" alt=""></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/11.png" alt=""></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/12.png" alt=""></p>
<p>And in fact if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further. And so, this is the, at this point, K means has converged and it’s done a pretty good job finding the two clusters in this data. </p>
<p>Let’s write out the K means algorithm more formally. <strong>The K means algorithm takes two inputs. One is a parameter K,</strong> which is the number of clusters you want to find in the data. I’ll later say how we might go about trying to choose k, but for now let’s just say that we’ve decided we want a certain number of clusters and we’re going to tell the algorithm how many clusters we think there are in the data set. <strong>And then K means also takes as input this sort of unlabeled training set of just the Xs and because this is unsupervised learning, we don’t have the labels Y anymore.</strong> </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/13.png" alt=""></p>
<p>And <strong>for unsupervised learning of the K means I’m going to use the <em>* *convention</em> **that $X^{(i)}$ is an $R^N$ dimensional vector. And that’s why my training examples are now N dimensional rather N plus one dimensional vectors.</strong> This is what the K means algorithm does. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/14.png" alt=""></p>
<p><strong>The first step is that it randomly initializes k cluster centroids which we will call mu 1, mu 2, up to mu k.</strong> And so in the earlier diagram, the cluster centroids corresponded to the location of the red cross and the location of the blue cross. So there we had two cluster centroids, so maybe the red cross was mu 1 and the blue cross was mu 2, and more generally we would have k cluster centroids rather than just 2. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/15.png" alt=""></p>
<p><strong>Then the inner loop of k means does the following</strong>, we’re going to repeatedly do the following. First for each of my training examples, I’m going to set this variable $c^{(i)}$ to be the index 1 through K of the cluster centroid closest to $x^{(i)}$. So this was my <strong>cluster assignment step</strong>, where we took each of my examples and coloured it either red or blue, depending on which cluster centroid it was closest to. So $c^{(i)}$ is going to be a number from 1 to K that tells us, you know, is it closer to the red cross or is it closer to the blue cross, and another way of writing this is I’m going to, to compute $c^{(i)}$, I’m going to take my $i_{th}$ example $x^{(i)}$ and and I’m going to <strong>measure it’s distance to each of my cluster centroids</strong>, this is mu and then lower-case k, right, so capital K is the total number centroids and I’m going to use lower case k here to index into the different centroids. But so, <strong>$c^{(i)}$ is going to, I’m going to minimize over my values of k and find the value of K that minimizes this distance between Xi and the cluster centroid, and then, you know, the value of k that minimizes this, that’s what gets set in $c^{(i)}$.</strong> So, here’s another way of writing out what Ci is. If I write the norm between Xi minus Mu-k, then this is the distance between my ith training example Xi and the cluster centroid Mu subscript K, this is–this here, that’s a lowercase K. So uppercase K is going to be used to denote the total number of cluster centroids, and this lowercase K’s a number between one and capital K. I’m just using lower case K to index into my different cluster centroids. Next is lower case k. So that’s the distance between the example and the cluster centroid and so what I’m going to do is find the value of K, of lower case k that minimizes this, and so the value of k that minimizes you know, that’s what I’m going to set as Ci, and by convention here I’ve written the distance between Xi and the cluster centroid, by convention people actually tend to write this as the squared distance. So we think of Ci as picking the cluster centroid with the smallest squared distance to my training example Xi. But of course minimizing squared distance, and minimizing distance that should give you the same value of Ci, but we usually put in the square there, just as the convention that people use for K means. So that was the cluster assignment step.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/16.png" alt=""></p>
<p>The other in the loop of K means does the <strong>move centroid step</strong>. And what that does is for each of my cluster centroids, so for lower case k equals 1 through K, <strong>it sets Mu-k equals to the average of the points assigned to the cluster</strong>. So as a concrete example, let’s say that one of my cluster centroids, let’s say cluster centroid two, has training examples, you know, 1, 5, 6, and 10 assigned to it. And what this means is, really this means that C1 equals to C5 equals to C6 equals to and similarly well c10 equals, too, right? If we got that from the cluster assignment step, then that means examples 1,5,6 and 10 were assigned to the cluster centroid two. Then in this move centroid step, what I’m going to do is just compute the average of these four things. So X1 plus X5 plus X6 plus X10. And now I’m going to average them so here I have four points assigned to this cluster centroid, just take one quarter of that. And now Mu2 is going to be an n-dimensional vector. Because each of these example x1, x5, x6, x10 each of them were an n-dimensional vector, and I’m going to add up these things and, you know, divide by four because I have four points assigned to this cluster centroid, I end up with my move centroid step, for my cluster centroid mu-2. This has the effect of moving mu-2 to the average of the four points listed here. One thing that I’ve asked is, well here we said, let’s let mu-k be the average of the points assigned to the cluster. </p>
<p><strong>But what if there is a cluster centroid no points with zero points assigned to it. In that case the more common thing to do is to just eliminate that cluster centroid. And if you do that, you end up with K minus one clusters instead of k clusters. Sometimes if you really need k clusters, then the other thing you can do if you have a cluster centroid with no points assigned to it is you can just randomly reinitialize that cluster centroid, but it’s more common to just eliminate a cluster if somewhere during K means it with no points assigned to that cluster centroid, and that can happen, altthough in practice it happens not that often.</strong> So that’s the K means Algorithm. </p>
<p>Before wrapping up this video I just want to tell you about one other common application of K Means and that’s to the problems with non well separated clusters. Here’s what I mean. So far we’ve been picturing K Means and applying it to data sets like that shown here where we have three pretty well separated clusters, and we’d like an algorithm to find maybe the 3 clusters for us. <strong>But it turns out that very often K Means is also applied to data sets that look like this where there may not be several very well separated clusters.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/17.png" alt=""></p>
<p>Here is an example application, to t-shirt sizing. Let’s say you are a t-shirt manufacturer you’ve done is you’ve gone to the population that you want to sell t-shirts to, and you’ve collected a number of examples of the height and weight of these people in your population and so, well I guess height and weight tend to be positively highlighted so maybe you end up with a data set like this, you know, with a sample or set of examples of different peoples heights and weight. Let’s say you want to size your t shirts. Let’s say I want to design and sell t shirts of three sizes, small, medium and large. So how big should I make my small one? How big should I my medium? And how big should I make my large t-shirts. One way to do that would to be to run my k means clustering logarithm on this data set that I have shown on the right and maybe what K Means will do is group all of these points into one cluster and group all of these points into a second cluster and group all of those points into a third cluster. So, even though the data, you know, before hand it didn’t seem like we had 3 well separated clusters, K Means will kind of separate out the data into multiple pluses for you. And what you can do is then look at this first population of people and look at them and, you know, look at the height and weight, and try to design a small t-shirt so that it kind of fits this first population of people well and then design a medium t-shirt and design a large t-shirt. And this is in fact kind of an example of market segmentation where you’re using K Means to separate your market into 3 different segments. So you can design a product separately that is a small, medium, and large t-shirts, that tries to suit the needs of each of your 3 separate sub-populations well. So that’s the K Means algorithm. And by now you should know how to implement the K Means Algorithm and kind of get it to work for some problems. </p>
<p>But in the next few videos what I want to do is really get more deeply into the nuts and bolts of K means and to talk a bit about how to actually get this to work really well.</p>
<h4 id="summary-1"><a href="#summary-1" class="headerlink" title="summary"></a>summary</h4><p>The K-Means Algorithm is the most popular and widely used algorithm for automatically grouping data into coherent subsets. </p>
<ol>
<li>Randomly initialize two points in the dataset called the cluster centroids . </li>
<li>Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to. </li>
<li>Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages. </li>
<li>Re-run (2) and (3) until we have found our clusters. </li>
</ol>
<p>Our main variables are: </p>
<ul>
<li>K (number of clusters) </li>
<li>Training set ${x^{(1)}, x^{(2)}, \dots,x^{(m)}}$ </li>
<li>Where $x^{(i)} \in \mathbb{R}^n$ </li>
<li>Note that we will not use the $x_0=1$ convention. </li>
</ul>
<p><strong>The algorithm:</strong></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)</span><br><span class="line">Repeat:</span><br><span class="line">   <span class="keyword">for</span> i = <span class="number">1</span> to m:</span><br><span class="line">      c(i):= index (from <span class="number">1</span> to K) of cluster centroid closest to x(i)</span><br><span class="line">   <span class="keyword">for</span> k = <span class="number">1</span> to K:</span><br><span class="line">      mu(k):= average (mean) of points assigned to cluster k</span><br></pre></td></tr></table></figure>

<p>The <strong>first for-loop</strong> is the ‘Cluster Assignment’ step. We make a vector c where $c^{(i)}$ represents the centroid assigned to example $x^{(i)}$ .<br>We can write the operation of the Cluster Assignment step more mathematically as follows:<br>$c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2$<br>That is, each $c^{(i)}$ contains the index of the centroid that has minimal distance to $x^{(i)}$.<br>By convention, we square the right-hand-side, which makes the function we are trying to minimize more sharply increasing. It is mostly just a convention. But a convention that helps reduce the computation load because the Euclidean distance requires a square root but it is canceled.<br>Without the square:<br>$$||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + …}\quad||$$<br>With the square:<br>$$||x^{(i)} - \mu_k||^2 = ||\quad(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + …\quad||$$<br>so the square convention serves two purposes, minimize more sharply and less computation.<br>The <strong>second for-loop</strong> is the ‘Move Centroid’ step where we move each centroid to the average of its group.<br>More formally, the equation for this loop is as follows:<br>$$\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n$$<br>Where each of $x^{(k_1)}, x^{(k_2)}, \dots, x^{(k_n)}$ are the training examples assigned to group $mμ_k$.<br>If you have a cluster centroid with 0 points assigned to it, you can randomly <strong>re-initialize</strong> that centroid to a new point. You can also simply <strong>eliminate</strong> that cluster group.<br>After a number of iterations the algorithm will <strong>converge</strong> , where new iterations do not affect the clusters.<br>Note on non-separated clusters: some datasets have no real inner separation or natural structure. K-means can still evenly segment your data into K subsets, so can still be useful in this case. </p>
<h3 id="03-optimization-objective"><a href="#03-optimization-objective" class="headerlink" title="03_optimization-objective"></a>03_optimization-objective</h3><p>Most of the supervised learning algorithms we’ve seen, things like linear regression, logistic regression, and so on, all of those algorithms have an optimization objective or some cost function that the algorithm was trying to minimize. <strong>It turns out that k-means also has an optimization objective or a cost function that it’s trying to minimize</strong>. </p>
<p>And in this video I’d like to tell you what that optimization objective is. And the reason I want to do so is because this will be useful to us for two purposes. <strong>First, knowing what is the optimization objective of k-means will help us to debug the learning algorithm and just make sure that k-means is running correctly. And second, and perhaps more importantly, in a later video we’ll talk about how we can use this to help k-means find better costs for this and avoid the local optima.</strong> </p>
<p>But we do that in a later video that follows this one. <strong>Just as a quick reminder while k-means is running we’re going to be keeping track of two sets of variables. First is the $c^{(i)}$’s and that keeps track of the index or the number of the cluster, to which an example $x^{(i)}$ is currently assigned. And then the other set of variables we use is $/mu_k$, which is the location of cluster centroid k.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/18.png" alt=""></p>
<p>Again, for k-means we use capital K to denote the total number of clusters. And here lower case k is going to be an index into the cluster centroids and so, lower case k is going to be a number between one and capital K. Now here’s one more bit of notation, which is gonna use mu subscript ci ($/mu_{c^{(i)}}$) to denote the cluster centroid of the cluster to which example $x^{(i)}$ has been assigned, right? And to explain that notation a little bit more, lets say that xi has been assigned to cluster number five. What that means is that ci, that is the index of xi, that that is equal to five. Right? Because having ci equals five, if that’s what it means for the example xi to be assigned to cluster number five. And so mu subscript ci is going to be equal to mu subscript 5. Because ci is equal to five. And so this mu subscript ci is the cluster centroid of cluster number five, which is the cluster to which my example xi has been assigned. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/19.png" alt="K-means_optimization_objective"></p>
<p>Out with this notation, we’re now ready to write out what is the optimization objective of the k-means clustering algorithm and here it is. The cost function that k-means is minimizing is a function J of all of these parameters, $c^{(1)}$ through $c^{(m)}$ and $/mu_1$ through $/gmu_K$. That k-means is varying as the algorithm runs. And the optimization objective is shown to the right, is the average of 1 over m of sum from i equals 1 through m of this term here. That I’ve just drawn the red box around, right? The square distance between each example xi and the location of the cluster centroid to which xi has been assigned. So let’s draw this and just let me explain this. Right, so here’s the location of training example xi and here’s the location of the cluster centroid to which example xi has been assigned. So to explain this in pictures, if here’s x1, x2, and if a point here is my example xi, so if that is equal to my example xi, and if xi has been assigned to some cluster centroid, I’m gonna denote my cluster centroid with a cross, so if that’s the location of mu 5, let’s say. If x i has been assigned cluster centroid five as in my example up there, then this square distance, that’s the square of the distance between the point xi and this cluster centroid to which xi has been assigned. And what k-means can be shown to be doing is that it is trying to define parameters ci and mu i. Trying to find c and mu to try to minimize this cost function J. This cost function is sometimes also called the <strong>distortion cost function</strong>, or the distortion of the k-means algorithm. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/20.png" alt="K-means-algorithm_in_optimization_view"></p>
<p>And just to provide a little bit more detail, here’s the k-means algorithm. Here’s exactly the algorithm as we have written it out on the earlier slide.  <strong>And what this first step of this algorithm is, this was the cluster assignment step where we assigned each point to the closest centroid. And it’s possible to show mathematically that what the cluster assignment step is doing is exactly Minimizing J, with respect to the variables c1, c2 and so on, up to cm, while holding the cluster centroids mu 1 up to mu K, fixed.</strong> So what the cluster assignment step does is it doesn’t change the cluster centroids, but what it’s doing is this is exactly picking the values of c1, c2, up to cm. That minimizes the cost function, or the distortion function J. And it’s possible to prove that mathematically, but I won’t do so here. But it has a pretty intuitive meaning of just well, let’s assign each point to a cluster centroid that is closest to it, because that’s what minimizes the square of distance between the points in the cluster centroid. <strong>And then the second step of k-means, this second step over here. The second step was the move centroid step. And once again I won’t prove it, but it can be shown mathematically that what the move centroid step does is it chooses the values of mu that minimizes J, so it minimizes the cost function J with respect to, wrt is my abbreviation for, with respect to, when it minimizes J with respect to the locations of the cluster centroids mu 1 through mu K.</strong> So if is really is doing is this taking the two sets of variables and partitioning them into two halves right here. First the c sets of variables and then you have the mu sets of variables. <strong><em>And what it does is it first minimizes J with respect to the variable c and then it minimizes J with respect to the variables mu and then it keeps on. And, so all that’s all that k-means does.</em></strong> And now that we understand k-means as trying to minimize this cost function J, we can also use this to try to debug other any algorithm and just kind of make sure that our implementation of k-means is running correctly. </p>
<p>So, we now understand the k-means algorithm as trying to optimize this cost function J, which is also called the distortion function. We can use that to debug k means and help make sure that k-means is converging and is running properly. And in the next video we’ll also see how we can use this to help k-means find better clusters and to help k-means to avoid local optima.</p>
<h4 id="summary-2"><a href="#summary-2" class="headerlink" title="summary"></a>summary</h4><p>Recall some of the parameters we used in our algorithm:<br>$c^{(i)}$ = index of cluster (1,2,…,K) to which example $x^{(i)}$ is currently assigned<br>$\mu_k $= cluster centroid k (μk∈ℝn)<br>$\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned<br>Using these variables we can define our <strong>cost function</strong> :<br>$$J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2$$<br>Our <strong>optimization objective</strong> is to minimize all our parameters using the above cost function:<br>$$min_{c,\mu}\ J(c,\mu)$$<br>That is, we are finding all the values in sets c, representing all our clusters, and μ, representing all our centroids, that will minimize the average of the distances of every training example to its corresponding cluster centroid.<br>The above cost function is often called the <strong>distortion</strong> of the training examples.<br>In the <strong>cluster assignment step</strong> , our goal is to:<br>Minimize J(…) with $c^{(1)},\dots,c^{(m)}$ (holding $\mu_1,\dots,\mu_K$ fixed)<br>In the <strong>move centroid step</strong>, our goal is to:<br>Minimize J(…) with $\mu_1,\dots,\mu_K$<br>With k-means, <strong>it is not possible for the cost function to sometimes increase</strong> . It should always descend. </p>
<h3 id="04-random-initialization"><a href="#04-random-initialization" class="headerlink" title="04_random-initialization"></a>04_random-initialization</h3><p>In this video, I’d like to talk about <strong>how to initialize K-means and more importantly, this will lead into a discussion of how to make K-means avoid local optima as well.</strong> Here’s the K-means clustering algorithm that we talked about earlier. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/21.png" alt=""></p>
<p><strong>One step</strong> that we never really talked much about was this step of how you randomly initialize the cluster centroids. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/22.png" alt="random_initialization"></p>
<p>There are few different ways that one can imagine using to randomly initialize the cluster centroids. But, it turns out that there is one method that is much more recommended than most of the other options one might think about. So, let me tell you about that option since it’s what often seems to work best. Here’s how I usually initialize my cluster centroids. <strong>When running K-means, you should have the number of cluster centroids, K, set to be less than the number of training examples M.</strong> It would be really weird to run K-means with a number of cluster centroids that’s, you know, equal or greater than the number of examples you have, right? <strong>So the way I usually initialize K-means is, I would randomly pick k training examples. So, and, what I do is then set $\mu_1$ of $\mu_K$ equal to these k examples</strong>.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/23.png" alt=""></p>
<p> Let me show you a concrete example. Lets say that k is equal to 2 and so on this example on the right let’s say I want to find two clusters. So, what I’m going to do in order to initialize my cluster centroids is, I’m going to randomly pick a couple examples. And let’s say, I pick this one and I pick that one. And the way I’m going to initialize my cluster centroids is, I’m just going to initialize my cluster centroids to be right on top of those examples. So that’s my first cluster centroid and that’s my second cluster centroid, and that’s one random initialization of K-means. The one I drew looks like a particularly good one. And sometimes I might get less lucky and maybe I’ll end up picking that as my first random initial example, and that as my second one. And here I’m picking two examples because k equals 2. Some we have randomly picked two training examples and if I chose those two then I’ll end up with, may be this as my first cluster centroid and that as my second initial location of the cluster centroid. So, that’s how you can randomly initialize the cluster centroids. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/24.png" alt=""></p>
<p> And so at initialization, your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on, if you have more clusters and more cluster centroid. And sort of the side common. I should say that in the earlier video where I first illustrated K-means with the animation. In that set of slides. Only for the purpose of illustration. I actually used a different method of initialization for my cluster centroids.But the method described on this slide, this is really the recommended way. And the way that you should probably use, when you implement K-means. So, as they suggested perhaps by these two illustrations on the right. You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized, and so, depending on the random initialization. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/25.png" alt=""></p>
<p><strong>K-means can end up at different solutions. And, in particular, K-means can actually end up at local optima.</strong> If you’re given the data sale like this. Well, it looks like, you know, there are three clusters, and so, if you run K-means and if it ends up at a good local optima this might be really the global optima, you might end up with that cluster ring. But if you had a particularly unlucky, random initialization, K-means can also get stuck at different local optima. So, in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points. And so, this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore, has split the second cluster into two separate sub-clusters like so, and it has also taken the second cluster and split it into two separate sub-clusters like so, and so, both of these examples on the lower right correspond to different local optima of K-means and in fact, in this example here, the cluster, the red cluster has captured only a single optima example. And the term local optima, by the way, refers to local optima of this distortion function J, and what these solutions on the lower left, what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it’s not doing a very good job minimizing this distortion function J. So, if you’re worried about K-means getting stuck in local optima, if you want to increase the odds of K-means finding the best possible clustering, like that shown on top here, what we can do, is try multiple, random initializations. <strong>So, instead of just initializing K-means once and hopping that that works, what we can do is, initialize K-means lots of times and run K-means lots of times, and use that to try to make sure we get as good a solution, as good a local or global optima as possible.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/26.png" alt=""></p>
<p> Concretely, here’s how you could go about doing that. Let’s say, I decide to run K-meanss a hundred times so I’ll execute this loop a hundred times and it’s fairly typical a number of times when came to will be something from 50 up to may be 1000. So, let’s say you decide to say K-means one hundred times. So what that means is that we would randomnly initialize K-means. And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings, and a set of cluster centroids, and then we would then compute the distortion J, that is compute this cause function on the set of cluster assignments and cluster centroids that we got. Finally, having done this whole procedure a hundred times. <strong>You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data, just pick one, that gives us the lowest cost. That gives us the lowest distortion. And it turns out that if you are running K-means with a fairly small number of clusters , so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often, can sometimes make sure that you find a better local optima. Make sure you find the better clustering data. But if K is very large, so, if K is much greater than 10, certainly if K were, you know, if you were trying to find hundreds of clusters, then, having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing, doing multiple random initializations will probably give you a slightly better solution but, but maybe not that much.</strong> But it’s really in the regime of where you have a relatively small number of clusters, especially if you have, maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering. So, that’s K-means with random initialization. If you’re trying to learn a clustering with a relatively small number of clusters, 2, 3, 4, 5, maybe, 6, 7, using multiple random initializations can sometimes, help you find much better clustering of the data. But, even if you are learning a large number of clusters, the initialization, the random initialization method that I describe here. That should give K-means a reasonable starting point to start from for finding a good set of clusters.</p>
<h4 id="summary-3"><a href="#summary-3" class="headerlink" title="summary"></a>summary</h4><p>There’s one particular recommended method for randomly initializing your cluster centroids. </p>
<ol>
<li>Have K&lt;m. That is, make sure the number of your clusters is less than the number of your training examples. </li>
<li>Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique). </li>
<li>Set $\mu_1,\dots,\mu_K$ equal to these K examples. </li>
</ol>
<p><strong>K-means can get stuck in local optima</strong>. To decrease the chance of this happening, you can run the algorithm on many different random initializations. In cases where K&lt;10 it is strongly recommended to run a loop of random initializations. </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to <span class="number">100</span>:</span><br><span class="line">   randomly initialize k-means</span><br><span class="line">   <span class="built_in">run</span> k-means to <span class="built_in">get</span> <span class="string">'c'</span> <span class="keyword">and</span> <span class="string">'m'</span></span><br><span class="line">   compute the cost function (distortion) J(c,m)</span><br><span class="line">pick the clustering that gave us the lowest cost</span><br></pre></td></tr></table></figure>
<h3 id="05-choosing-the-number-of-clusters"><a href="#05-choosing-the-number-of-clusters" class="headerlink" title="05_choosing-the-number-of-clusters"></a>05_choosing-the-number-of-clusters</h3><p>In this video I’d like to talk about <strong>one last detail of K-means clustering which is how to choose the number of clusters, or how to choose the value of the parameter capsule K.</strong> </p>
<p>To be honest, there actually isn’t a great way of answering this or doing this automatically and by far the most common way of choosing the number of clusters, is still choosing it manually by looking at visualizations or by looking at the output of the clustering algorithm or something else. But I do get asked this question quite a lot of how do you choose the number of clusters, and so I just want to tell you know what are peoples’ current thinking on it although, the most common thing is actually to choose the number of clusters by hand. A large part of why it might not always be easy to choose the number of clusters is that it is often generally ambiguous how many clusters there are in the data. Looking at this data set some of you may see four clusters and that would suggest using K equals 4. Or some of you may see two clusters and that will suggest K equals 2 and now this may see three clusters. <strong>And so, looking at the data set like this, the true number of clusters, it actually seems genuinely ambiguous to me, and I don’t think there is one right answer. And this is part of our supervised learning. We are aren’t given labels, and so there isn’t always a clear cut answer.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/27.png" alt="what_is_the_right_of_K"></p>
<p> And this is one of the things that makes it more difficult to say, have an automatic algorithm for choosing how many clusters to have. When people talk about ways of choosing the number of clusters, one method that people sometimes talk about is something called the <strong>Elbow Method</strong>. Let me just tell you a little bit about that, and then mention some of its advantages but also shortcomings.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/28.png" alt="elbow_method"></p>
<p> So the Elbow Method, what we’re going to do is vary K, which is the total number of clusters. So, we’re going to run K-means with one cluster, that means really, everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here. And then we’re going to run K means with two clusters, maybe with multiple random initial agents, maybe not. But then, you know, with two clusters we should get, hopefully, a smaller distortion, and so plot that there. And then run K-means with three clusters, hopefully, you get even smaller distortion and plot that there. I’m gonna run K-means with four, five and so on. And so we end up with a curve showing how the distortion, you know, goes down as we increase the number of clusters. And so we get a curve that maybe looks like this. And if you look at this curve, what the Elbow Method does it says “Well, let’s look at this plot. Looks like <strong>there’s a clear elbow there</strong>“. <strong>Right, this is, would be by analogy to the human arm where, you know, if you imagine that you reach out your arm, then, this is your shoulder joint, this is your elbow joint and I guess, your hand is at the end over here. And so this is the Elbow Method.</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/29.png" alt="cost_j_with_elbow"></p>
<p>Then you find this sort of pattern where the distortion goes down rapidly from 1 to 2, and 2 to 3, and then you reach an elbow at 3, and then the distortion goes down very slowly after that. And then it looks like, you know what, maybe using three clusters is the right number of clusters, because that’s the elbow of this curve, right? That it goes down, distortion goes down rapidly until K equals 3, really goes down very slowly after that. So let’s pick K equals 3. If you apply the Elbow Method, and if you get a plot that actually looks like this, then, that’s pretty good, and this would be a reasonable way of choosing the number of clusters. </p>
<p>It turns out the Elbow Method isn’t used that often, and one reason is that, if you actually use this on a clustering problem, it turns out that fairly often, you know, you end up with a curve that looks much more ambiguous, maybe something like this.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/30.png" alt="cost_j_without_elbow"></p>
<p> And if you look at this, I don’t know, maybe there’s no clear elbow, but it looks like distortion continuously goes down, maybe 3 is a good number, maybe 4 is a good number, maybe 5 is also not bad. And so, if you actually do this in a practice, you know, if your plot looks like the one on the left and that’s great. It gives you a clear answer, but just as often, you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is. It  makes it harder to choose a number of clusters using this method. So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn’t necessarily, you know, have a very high expectation of it working for any particular problem.</p>
<p> Finally, here’s one other way of how, thinking about how you choose the value of K, very often people are running K-means in order you get clusters for some later purpose, or for some sort of downstream purpose. Maybe you want to use K-means in order to do market segmentation, like in the T-shirt sizing example that we talked about. Maybe you want K-means to organize a computer cluster better, or maybe a learning cluster for some different purpose, and so, if that later, downstream purpose, such as market segmentation. If that gives you an evaluation metric, then often, a better way to determine the number of clusters, is to see how well different numbers of clusters serve that later downstream purpose. Let me step through a specific example. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/13/31.png" alt="choosing_the_value_of_K_for_downstream_purpose"></p>
<p> Let me go through the T-shirt size example again, and I’m trying to decide, do I want three T-shirt sizes? So, I choose K equals 3, then I might have small, medium and large T-shirts. Or maybe, I want to choose K equals 5, and then I might have, you know, extra small, small, medium, large and extra large T-shirt sizes. So, you can have like 3 T-shirt sizes or four or five T-shirt sizes. We could also have four T-shirt sizes, but I’m just showing three and five here, just to simplify this slide for now. So, if I run K-means with K equals 3, maybe I end up with, that’s my small and that’s my medium and that’s my large. Whereas, if I run K-means with 5 clusters, maybe I end up with, those are my extra small T-shirts, these are my small, these are my medium, these are my large and these are my extra large. And the nice thing about this example is that, this then maybe gives us another way to choose whether we want 3 or 4 or 5 clusters, and in particular, what you can do is, you know, think about this from the perspective of the T-shirt business and ask: “Well if I have five segments, then how well will my T-shirts fit my customers and so, how many T-shirts can I sell? How happy will my customers be?” What really makes sense, from the perspective of the T-shirt business, in terms of whether, I want to have Goer T-shirt sizes so that my T-shirts fit my customers better. Or do I want to have fewer T-shirt sizes so that I make fewer sizes of T-shirts. And I can sell them to the customers more cheaply. And so, the t-shirt selling business, that might give you a way to decide, between three clusters versus five clusters. So, that gives you an example of how a later downstream purpose like the problem of deciding what T-shirts to manufacture, how that can give you an evaluation metric for choosing the number of clusters. </p>
<p> For those of you that are doing the program exercises, if you look at this week’s program exercise associative K-means, that’s an example there of using K-means for image compression. And so if you were trying to choose how many clusters to use for that problem, you could also, again use the evaluation metric of image compression to choose the number of clusters, K? So, how good do you want the image to look versus, how much do you want to compress the file size of the image, and, you know, if you do the programming exercise, what I’ve just said will make more sense at that time. </p>
<p> <strong>So, just summarize, for the most part, the number of customers K is still chosen by hand by human input or human insight. One way to try to do so is to use the Elbow Method, but I wouldn’t always expect that to work well, but I think the better way to think about how to choose the number of clusters is to ask, for what purpose are you running K-means? And then to think, what is the number of clusters K that serves that, you know, whatever later purpose that you actually run the K-means for.</strong></p>
<h4 id="summary-4"><a href="#summary-4" class="headerlink" title="summary"></a>summary</h4><p>Choosing K can be quite arbitrary and ambiguous.<br>The <strong>elbow method</strong> : plot the cost J and the number of clusters K. The cost function should reduce as we increase the number of clusters, and then flatten out. Choose K at the point where the cost function starts to flatten out.<br>However, fairly often, the curve is very <strong>gradual</strong> , so there’s no clear elbow.<br><strong>Note</strong>: J will always decrease as K is increased. The one exception is if k-means gets stuck at a bad local optimum.<br>Another way to choose K is to observe how well k-means performs on <strong>a downstream purpose</strong> . In other words, you choose K that proves to be most useful for some goal you’re trying to achieve from using these clusters. </p>
<h2 id="Bonus-Discussion-of-the-drawbacks-of-K-Means"><a href="#Bonus-Discussion-of-the-drawbacks-of-K-Means" class="headerlink" title="Bonus: Discussion of the drawbacks of K-Means"></a>Bonus: Discussion of the drawbacks of K-Means</h2><p>This links to a discussion that shows various situations in which K-means gives totally correct but unexpected results: <a href="http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means" target="_blank" rel="noopener">http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Karan"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Karan</p>
  <div class="site-description" itemprop="description">Refuse to Fall</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">91</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/yourname" title="YouTube → https:&#x2F;&#x2F;youtube.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/yourname" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Karan</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">2.2m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">33:59</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


    </div>
</body>
</html>
