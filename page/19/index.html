<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">

<script>
    (function(){
        if(''){
                         If (prompt('Please enter the article password') !== ''){
                                 Alert('Password error!');
                history.back();
            }
        }
    })();
</script>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"snakecoding.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":20,"offset":15,"onmobile":false},"copycode":{"enable":true,"show_result":"flat","style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":false},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Refuse to Fall">
<meta property="og:type" content="website">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://snakecoding.com/page/19/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="Refuse to Fall">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Karan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://snakecoding.com/page/19/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Machine Learning</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Machine Learning" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Machine Learning</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Data Science</p>
      <a>
        <img class="custom-logo-image" src="/images/custom-logo.jpg" alt="Machine Learning">
      </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">87</span></a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="#" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2017/08/08/graph_and_network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/08/08/graph_and_network/" class="post-title-link" itemprop="url">Tsinghua linear-algebra-2 8th-lecture graph-and-network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-08-08 20:16:00" itemprop="dateCreated datePublished" datetime="2017-08-08T20:16:00+05:30">2017-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 16:34:45" itemprop="dateModified" datetime="2020-04-09T16:34:45+05:30">2020-04-09</time>
              </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>8.4k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>8 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Notes from: Tsinghua University Open Class: Linear Algebra 2-Lecture 8: Graphs and Networks</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><h4 id="The-vector-form-of-Ohm’s-law"><a href="#The-vector-form-of-Ohm’s-law" class="headerlink" title="The vector form of Ohm’s law"></a>The vector form of Ohm’s law</h4><p>! [matrix_of_Ohm’s_law.png] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/1.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Graphs-and-Matrices"><a href="#Graphs-and-Matrices" class="headerlink" title="Graphs and Matrices"></a>Graphs and Matrices</h3><p>! [directed_graphs.png] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/2.png<i class="fa fa-external-link-alt"></i></span>)<br>! [circute_graph] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/3.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="incidence-matrix"><a href="#incidence-matrix" class="headerlink" title="incidence matrix"></a>incidence matrix</h4><p>! [incidence_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvNC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/4.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="adjacency-matrix"><a href="#adjacency-matrix" class="headerlink" title="adjacency matrix"></a>adjacency matrix</h4><p>! [adjacency_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvNS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/5.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="Laplacian-matrix"><a href="#Laplacian-matrix" class="headerlink" title="Laplacian matrix"></a>Laplacian matrix</h4><p>! [laplacian_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvNi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/6.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>** Note: The positive semidefinite proof is similar to the stiffness matrix **</p>
<h3 id="Network-and-weighted-Laplacian-matrix"><a href="#Network-and-weighted-Laplacian-matrix" class="headerlink" title="Network and weighted Laplacian matrix"></a>Network and weighted Laplacian matrix</h3><p>! [network] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvNy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/7.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="Circuit-related-laws-of-physics"><a href="#Circuit-related-laws-of-physics" class="headerlink" title="Circuit-related laws of physics"></a>Circuit-related laws of physics</h4><p>! [typical_circuit_laws] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvOC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/8.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><h5 id="No-external-source"><a href="#No-external-source" class="headerlink" title="No external source"></a>No external source</h5><p>! [1st_example_of_circuit_network_and_laplacian_matrix_without_external_sources] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvOS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/9.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="Connect-to-external-source"><a href="#Connect-to-external-source" class="headerlink" title="Connect to external source"></a>Connect to external source</h4><p>! [2nd_example_of_circuit_network_and_laplacian_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMTAucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/10.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="With-right-K-A-TCA"><a href="#With-right-K-A-TCA" class="headerlink" title="With right $ K = A ^ TCA $"></a>With right $ K = A ^ TCA $</h4><p>! [K = ATCA_with_weights] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMTEucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/11.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Four-basic-subspaces-of-the-correlation-matrix"><a href="#Four-basic-subspaces-of-the-correlation-matrix" class="headerlink" title="Four basic subspaces of the correlation matrix"></a>Four basic subspaces of the correlation matrix</h3><h4 id="N-A"><a href="#N-A" class="headerlink" title="N (A)"></a>N (A)</h4><p>! [N (A) _of_incidence_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMTIucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/12.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="C-A"><a href="#C-A" class="headerlink" title="C (A)"></a>C (A)</h4><p>According to the definition of $ C (A) $: $ C (A) = \ {Ax | x \ in R ^ n } $. Continue to use the previous letters: $ u $ is the potential at each point, $ e $ is the potential difference between the sides, $ Au = e $, when $ Au = e $ has a solution $ \ Leftrightarrow e \ in C (A) $</p>
<ol>
<li><p>To prove: $ dim (C (A)) = n-1 $, that is, any $ n-1 $ column vector of $ A $ is linearly independent. Let $ A = (a_1, a_2, , … \ ,, a_n) $, assuming that $ a_1, a_2, , … \ ,, a_ {n-1} $ is linearly related, then there is $ c_1 , c_2, , … \ ,, c_ {n-1} \ in R $ and not all 0 satisfy: $ c_1a_1 + c_2a_2 + … + c_ {n-1} a_ {n-1} + 0a_n = 0 \ Rightarrow A \ begin {pmatrix} c_1 \ c_2 \\ vdots \ c_ {n-1} \ 0 \ end {pmatrix} = {0} \ Rightarrow \ begin {pmatrix} c_1 \ c_2 \ \ vdots \ c_ {n-1} \ 0 \ end {pmatrix} \ in N (A), $ but and $ N (A) = \ left \ {c \ begin {pmatrix} 1 \\ vdots \ \ 1 \ end {pmatrix} \ Bigg | c \ in R \ right } $ contradiction, and so on, it can be proved that the dimension of $ C (A) $ is $ n-1 $, that is, any $ A $ n-1 $ column vectors can be used as a set of bases for $ C (A) $.</p>
</li>
<li><p>Find the corresponding circuit in the matrix: $ e \ in C (A) $ The following equation has a solution $ Au = e \ Rightarrow \ begin {pmatrix} -1 &amp; 1 &amp; 0 &amp; 0 \ -1 &amp; 0 &amp; 1 &amp; 0 \ 0 &amp; -1 &amp; 1 &amp; 0 \ 0 &amp; -1 &amp; 0 &amp; 1 \ 0 &amp; 0 &amp; -1 &amp; 1 \ end {pmatrix} \ begin {pmatrix} u_1 \ u_2 \ u_3 \ u_4 \ u_5 \ end {pmatrix} = \ begin {pmatrix} e_1 \ e_2 \ e_3 \ e_4 \ \ e_5 \ end {pmatrix} \ Rightarrow \ begin {cases} -u_1 + u_2 = e_1 \ -u_1 + u_3 = e_2 \ -u_2 + u_3 = e_3 \ -u_2 + u_4 = e_4 \ -u_3 + u_4 = e_5 \ end {cases} \ Rightarrow \ begin {cases} e_1-e_2 + e_3 = 0 \ e_3-e_4 + e_5 = 0 \ end {cases} $, which is the difference between the potentials of the three edges 1,2,3 The sum is 0. From the figure, it can be seen that edges 1, 2, and 3 exactly form a loop, as are edges 3, 4, and 5. This happens to be ** Kirchholff Voltage Law (KVL) <strong>. Write these two loop equations in matrix form $ \ begin {pmatrix} 1 &amp; -1 &amp; 1 &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 1 \ end {pmatrix} \ begin {pmatrix} e_1 \ e_2 \ e_3 \ e_4 \ e_5 \ end {pmatrix} = 0 $. This is called the matrix $ B = \ begin {pmatrix} 1 &amp; -1 &amp; 1 &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 1 \ end {pmatrix} $ is the ** loop matrix *<em>, you can see that each row of it represents a loop and is called *</em> minimal loop **, and each column represents an edge. If the direction of the side is counterclockwise, the positive sign is taken, otherwise, the negative sign is taken. *</strong> Note that at this time $ e \ in N (B) $ ***.</p>
</li>
<li><p>Additionally, $ BA = \ begin {pmatrix} 1 &amp; -1 &amp; 1 &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; 1 &amp; -1 &amp; 1 \ end {pmatrix} \ begin {pmatrix} -1 &amp; 1 &amp; 0 &amp; 0 \ -1 &amp; 0 &amp; 1 &amp; 0 \ 0 &amp; -1 &amp; 1 &amp; 0 \ 0 &amp; -1 &amp; 0 &amp; 1 \ 0 &amp; 0 &amp;- 1 &amp; 1 \ end {pmatrix} = \ begin {pmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \ 0 &amp; 0 &amp; 0 &amp; 0 \ end {pmatrix} $ is $ C (A) \ subseteq N (B) $. $ dim (N (B)) = 3, dim (C (A)) = 3 $, so $ C (A) $ constitutes the base of $ N (B) $. From the point of view of rational meaning: The operation performed by the $ A $ matrix means solving the difference of the potentials on each side. The rows of $ B $ are just loops, and the result must be 0 by the law of $ KVL $.</p>
</li>
</ol>
<h4 id="N-A-T"><a href="#N-A-T" class="headerlink" title="$ N (A ^ T) $"></a>$ N (A ^ T) $</h4><ol>
<li>By definition: $ N (A ^ T) = \ {y \ in R ^ m | A ^ Ty = 0 } $. In the example, each row of the correlation matrix $ A $ represents an edge, and each column represents a vertex. Then the rows of $ A ^ T $ represent vertices, and the columns represent edges.<br> $ A ^ Ty = 0 \ Rightarrow \ begin {pmatrix} -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \ 1 &amp; 0 &amp; -1 &amp; -1 &amp; 0 \ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -1 \ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \ end {pmatrix} \ begin {pmatrix} y_1 \ y_2 \ y_3 \ \ y_4 \ y_5 \ end {pmatrix} = \ begin {pmatrix} 0 \ 0 \ 0 \ 0 \ 0 \ end {pmatrix} \ Rightarrow \ begin {cases} -y_1-y_2 = 0 \ y_1 -y_3-y_4 = 0 \ y_2 + y_3-y_5 = 0 \ y_4 + y_5 = 0 \ end {cases} $<br> Interpretation of physical meaning: $ y_i $ is the current on the edge of each $ i $. The above equation shows that the sum of the input and output current of each vertex is 0, which is ** Kichhoff Current Law (KCL) **.</li>
</ol>
<ol>
<li>$ A ^ Ty = 0 $, obtained from the previous article:<br> $ BA = 0 \ Rightarrow A ^ TB ^ T = 0 \ Rightarrow A ^ TB ^ T = \ begin {pmatrix} -1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 \ 1 &amp; 0 &amp; -1 &amp; -1 &amp; 0 \ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -1 \ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \ end {pmatrix} \ begin {pmatrix} 1 &amp; 0 \ -1 &amp; 0 \ 1 &amp; 1 \ 0 &amp; -1 \ 0 &amp; 1 \ end {pmatrix} = \ begin {pmatrix} 0 &amp; 0 \ 0 &amp; 0 \ 0 &amp; 0 \ 0 &amp; 0 \ end {pmatrix} $<br> Therefore, $ C (B ^ T) \ subseteq N (A ^ T) $. Because $ r (A) = C (A) = r = n-1, N (A ^ T) + C (A) = m, N (A ^ T) = mr = 5-3 = 2 $, because $ The column vector of B ^ T $ is linearly independent, that is, the row vector of $ B $ represents the loop, then the loop vector is a set of bases of $ N (A ^ T) $.</li>
</ol>
<h4 id="C-A-T"><a href="#C-A-T" class="headerlink" title="$ C (A ^ T) $"></a>$ C (A ^ T) $</h4><p>! [C (A ^ T) _of_incidence_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMTMucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/13.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="to-sum-up"><a href="#to-sum-up" class="headerlink" title="to sum up"></a>to sum up</h4><p>! [summary_of_4_subspaces_of_incidence_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMTQucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/14.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>-$ N (A_ {m \ times n}) $ zero space $ Au = 0 $, $ N (A) = c {(1,1, , … \ ,, 1) ^ T} _ {n \ times 1} $; physical meaning: the electric potential at each point is equal, and the electric potential difference is 0.<br>-$ C (A_ {m \ times n}) $ column space $ Au = e $ (x, b used above), any $ n-1 $ column in $ A $ constitutes $ C (A) $ A set of bases; physical meaning Each minimum loop potential is conserved, and the maximum loop potential formed by each minimum loop is still conserved, interpreting the KVL law.<br>-$ N (A ^ T) $ left zero space $ A ^ Ty = 0 $, the loop vector constitutes a set of bases of $ N (A ^ T) $; interprets the KCL law without external current source.<br>-$ C (A ^ T) $ row space, $ A ^ Ty = f $, each maximum tree subgraph corresponds to the row vector (ie edge) of the association matrix to form a group of $ C (A ^ T) $ Based on the interpretation of the KCL law with an external current source.</p>
<h4 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h4><h5 id="N-B-C-A"><a href="#N-B-C-A" class="headerlink" title="N (B) = C (A)"></a>N (B) = C (A)</h5><p>! [N (B) = C (A)] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMTUucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/15.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>** Any vector in the zero space of B must belong to the column space of A *<em>, the characteristics of each vector in the column space of $ A $, for example, $ A $ multiplied by a $ x_1 $ to $ x_n $, $ x_1 $ to $ x_n $ are the electric potential of $ n $ vertices. Multiply this vector by $ A $ to get the electric potential difference on each side, then the corresponding $ x_j-x_k $ is the electric potential difference at the two vertices of $ j $ and $ k $, the connection of the vertices, $ j $ and $ k $ The potential difference on the edge of the wire. *</em> So if we want to show that the vector in N (B) belongs to C (A), then we only need to show that any vector belongs to the zero space of B, and it can be written in such a form in the end. ** Then suppose $ e $ belongs to $ N (B) $, then we can take a maximal tree subgraph of the connected graph, and then take a vertex on the maximal tree subgraph $ T $ as the base point, then arbitrary There is only one such path on $ T $ between the other vertex $ K $ and this base point. Because $ T $ is a tree, it cannot have a loop, so there is only one in $ T $ A road connecting K to the base point. ** Define the potential of K: the sum of the potentials on each side of this road, and the sum of the potentials on each side **, our $ e_1 $ to $ e_m $, we can characterize the potential on each side, then we can Seeing that $ e $ belongs to $ N (B) $, we can actually check that the potential difference on any side is actually $ e_j $, etc. $ u_k $ minus $ u_1 $, then this $ k $ is the starting point of j , $ L $ is the end point of $ j $, and finally we can get $ e = -Au $, so $ e $ belongs to $ C (A) $ is this place, we want to use $ e $ to belong to $ N ( B) $, we can check out: this potential difference on any edge is equal to $ u_k $ minus $ u_l $, which is to satisfy the Korhoff voltage law.</p>
<h4 id="Euler’s-formula"><a href="#Euler’s-formula" class="headerlink" title="Euler’s formula"></a>Euler’s formula</h4><p>! [Euler’s_formula_of_2_dimensions] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTgvMTYucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-8/16.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>For $ B_ {x \ times m} \ Rightarrow C (B ^ T) + dim (N (B)) = r_B + dim (N (B)) = m \ Rightarrow m-r_B = dim (N (B)) = dim (C (A)) = n-1 $</p>
<p>And because of the Euler formula: $ ml = n-1 $, we get: $ r_B = l $, that is, $ B $ is full-rank, in fact, the minimum loop group corresponds to the maximum linear independent group. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2017/08/07/engineering_matrices/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/08/07/engineering_matrices/" class="post-title-link" itemprop="url">Tsinghua linear-algebra-2 7th-lecture engineering-matrix</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-08-07 20:16:00" itemprop="dateCreated datePublished" datetime="2017-08-07T20:16:00+05:30">2017-08-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 16:34:47" itemprop="dateModified" datetime="2020-04-09T16:34:47+05:30">2020-04-09</time>
              </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>4.8k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>4 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Notes from: Tsinghua University Open Class: Linear Algebra 2-Lecture 7: Matrix in Engineering</p>
<h2 id="Several-principles-of-applied-mathematics"><a href="#Several-principles-of-applied-mathematics" class="headerlink" title="Several principles of applied mathematics"></a>Several principles of applied mathematics</h2><ol>
<li>Turn a nonlinear problem into a linear problem (Nonlinear becomes linear)</li>
<li>Convert continuous problems into discrete (Continuous becomes discrete)</li>
</ol>
<h2 id="Matrix-in-the-project"><a href="#Matrix-in-the-project" class="headerlink" title="Matrix in the project"></a>Matrix in the project</h2><p>Many physical theorems are as approximations of reality, such as Hook’s law, Ohm’s law, Newton’s second law (F = ma), and discuss the vector forms of these laws. The vector form of the linear relationship is discussed in the following ** methods and frameworks:</p>
<p>$ (1) \ e = Au $<br>$ (2) \ y = Ce $<br>$ (3) \ f = A ^ Tw $</p>
<p>Where $ u $ is the starting unknown $ (primary \ unknown) $, and $ f $ is the external input $ (input) $:</p>
<p>! [framework_of_linear_ralation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/1.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>The linear problem is usually: Enter $ f $ and find $ u $?</p>
<p>E.g:<br>Hook’s law: Displacement is proportional to force f = ku<br>Ohm’s law: Current is proportional to voltage difference generalized to vector form: $ f = Ku $<br>Another example: least squares $ A ^ TAx = A ^ Tb $, find $ x $</p>
<p>! [framework_of_least_squares_approximations] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/2.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h2 id="Linear-spring-model"><a href="#Linear-spring-model" class="headerlink" title="Linear spring model"></a>Linear spring model</h2><p>! [Line_spring_model] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/3.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Situation-1"><a href="#Situation-1" class="headerlink" title="Situation (1)"></a>Situation (1)</h3><p>! [1st_example_of_line_spring_model] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvNC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/4.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Situation-2"><a href="#Situation-2" class="headerlink" title="Situation (2)"></a>Situation (2)</h3><p>! [2nd_example_of_line_spring_model] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvNS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/5.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Situation-3"><a href="#Situation-3" class="headerlink" title="Situation (3)"></a>Situation (3)</h3><p>! [3rd_example_of_line_spring_model] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvNi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/6.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Situation-4"><a href="#Situation-4" class="headerlink" title="Situation (4)"></a>Situation (4)</h3><p>! [4th_example_of_line_spring_model] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvNy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/7.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="to-sum-up"><a href="#to-sum-up" class="headerlink" title="to sum up"></a>to sum up</h3><p>! [summary_of_example_of_line_spring_model] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvOC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/8.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>The vector form of Hooke’s law applies it to elasticity mechanics. This $ u $ represents the up and down displacement of the mass. $ E $ is the spring and the amount of elongation or contraction. What about the relationship between them? It can be calculated by the matrix of $ A $, then A is very similar to a difference matrix, so that the elongation and contraction of the spring and the elastic force of the spring can be described by Hooke’s law, then the elastic forces generated by these several springs we Raised to a vector form such as Hooke’s law: each diagonal component of C represents an elastic coefficient ($ y = Ce $). Finally, between the elastic force and the external force: When the balance is reached, the relationship between them can be described by a matrix. This matrix and the previous matrix are just transposed with each other. Finally, the whole process is combined into this matrix. The stiffness matrix is ​​called the stiffness matrix and describes the degree of deformation of the system under external force.</p>
<h3 id="Stiffness-matrix"><a href="#Stiffness-matrix" class="headerlink" title="Stiffness matrix"></a>Stiffness matrix</h3><p>! [4_kinds_of_stiffness_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvOS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/9.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>** Note: The teacher here has a small jump **, the slag notes are as follows:</p>
<p>-$ K, T $ is positive definite: The $ C $ matrix indicates that the elastic coefficient is positive definite, $ K = A ^ TCA $, and when $ A $ is reversible, $ K $ and $ C $ are contracted.</p>
<blockquote>
<p>The symmetric matrix contracted with the positive definite matrix is ​​also positive definite</p>
<p>The first method of judging whether a real symmetric matrix is ​​positive definite: whether the eigenvalues ​​are all positive, if so, the real symmetric matrix is ​​positive definite. According to the inertia theorem, since the matrix (denoted as $ B $) contracted with the positive definite matrix (denoted as $ A $) whose eigenvalue sign is consistent with $ A $ and maintains symmetry, then the eigenvalue of $ B $ is also all positive , So $ B $ is also positive definite.</p>
</blockquote>
<p>-$ B, C $ are positive semidefinite</p>
<blockquote>
<p>Because the elastic coefficient matrix $ C $ is a positive definite diagonal matrix $ \ Rightarrow x ^ TKx = x ^ TA ^ TCAx = x ^ TA ^ T ({\ sqrt {C}} ^ T \ sqrt {C}) Ax = x ^ TA ^ T {\ sqrt {C}} ^ T \ sqrt {C} Ax = || \ sqrt {C} Ax || ^ 2 $, because $ A $ is strange, $ x ^ TKx \ ge 0 $, So K is positive semidefinite.</p>
</blockquote>
<h4 id="Nature-1"><a href="#Nature-1" class="headerlink" title="Nature 1"></a>Nature 1</h4><p>! [1st_property_of_4_kinds_stiffness_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMTAucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/10.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>Note: $ f_i $ is the external force suffered by $ i $ questions, for example: gravity, $ f_i = m_ig, \ m_i $ is the mass of $ i $ plastids.</p>
<h4 id="Nature-2"><a href="#Nature-2" class="headerlink" title="Nature 2"></a>Nature 2</h4><p>! [2nd_property_of_4_kinds_stiffness_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMTEucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/11.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>Note: The teacher here directly said the general conclusion, $ A, \ B $ are all positive definite, then $ AB $ may be asymmetric, but $ AB $ has a positive eigenvalue. The disc theorem is also directly quoted (!!! Slag Engineering Dog said unheard of !!!).</p>
<h4 id="Nature-3"><a href="#Nature-3" class="headerlink" title="Nature 3"></a>Nature 3</h4><p>! [3rd_property_of_4_kinds_stiffness_matrix] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMTIucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/12.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="From-discrete-to-continuous"><a href="#From-discrete-to-continuous" class="headerlink" title="From discrete to continuous"></a>From discrete to continuous</h4><p>** $ f = A ^ TCAu $ **</p>
<p>! [discrete becomes continuous] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMTMucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/13.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>to sum up:</p>
<p>$ (1) \ e = u_i-u_ {i-1} = \ Delta u = {du \ over dx} = Au \ (2) y = Ce = c (x) e (x) \ (3) \ f =-(y_i-y_ {i-1}) =-\ Delta y =-{dy \ over dx} = A ^ Ty $</p>
<p>! [discrete becomes continuous] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTcvMTQucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-7/14.png<i class="fa fa-external-link-alt"></i></span>)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2017/08/06/pseudo_inverse/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/08/06/pseudo_inverse/" class="post-title-link" itemprop="url">Tsinghua linear-algebra-2 6th-lecture pseudo-inverse</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-08-06 20:30:00" itemprop="dateCreated datePublished" datetime="2017-08-06T20:30:00+05:30">2017-08-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 16:34:38" itemprop="dateModified" datetime="2020-04-09T16:34:38+05:30">2020-04-09</time>
              </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>6.9k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>6 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Notes from: Tsinghua University Open Class: Linear Algebra 2-Lecture 6: Pseudo-inverse</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>! [introductory_content_of_pseudo-inverse] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvMS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/1.png<i class="fa fa-external-link-alt"></i></span>)<br>The singular value decomposition of a matrix can be understood as a linear transformation from $ R ^ n $ to $ R ^ m $ expressed in a matrix on different bases, and then using the singular value decomposition of the matrix<br>To define the pseudo-inverse of the matrix, and then use the pseudo-inverse of the matrix to discuss the least squares solution when the linear equations Ax = b has no solution, the central problem of linear algebra is<br>Solve the linear equation system $ Ax = b $, the simplest case is if the coefficient matrix A is an invertible matrix of order n, then for any n-dimensional vector $ b $, the linear equation system $ Ax = b $ has a unique Solution, this solution is $ A ^ {-1} b $, then this is inspired to the matrix that is not invertible or for the matrix of $ A_ {m \ times n} $, let ’s define an inverse matrix of it, then this When the inverse matrix is ​​called ** pseudo-inverse ** or ** generalized inverse **.</p>
<h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><p>The definition of pseudo-inverse comes from [singular value decomposition] (/ 2017/08/03 / singular_values_decomposition /) (need to understand the content of singular value decomposition):<br>! [definition_of_pseudo_inverse] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvMi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/2.png<i class="fa fa-external-link-alt"></i></span>)<br>(1) If $ A $ is reversible, that is, $ r = m = n $, then: $ A ^ {-1} = (U \ Sigma V ^ T) ^ {-1} = V \ Sigma ^ {-1} U ^ T = A ^ + $, Note: Decomposition formula by singular value $ AV = U \ Sigma, \ (v_1 , … , v_r) \ in C (A ^ T), \ (v_ {r + 1) , … , v_n) \ in N (A), \ (u_1 , … , u_r) \ in C (A), \ (u_ (r + 1) , … , u_m) \ in N (A ^ T) $ got: $ AV = U \ Sigma: C (A ^ T) \ rightarrow C (A) $, the same can be got: $ A ^ + U ^ T = V \ Sigma ^ {+}: C (A) \ rightarrow C (A ^ T) $</p>
<p>(2) $ AA ^ + = (U \ Sigma_ {m \ times n} V ^ T) (V \ Sigma ^ + _ {n \ times m} U ^ T) = U \ Sigma_ {m \ times n} \ Sigma ^ + _ {n \ times m} U ^ T = U \ begin {pmatrix} I_r &amp; 0 \ 0 &amp; 0 \ end {pmatrix} _ {m \ times m} U ^ T $ yields the following three properties:</p>
<p>-Symmetry: $ (AA ^ +) ^ T = AA ^ + $<br>-$ AA ^ + = u_1u_1 ^ T + , … , + u_ru_r ^ T, U = (u_1, , … , u_r, , u_ {r + 1} , … , , u_n) $<br>-Orthogonal projection matrix from $ AA ^ + = R ^ m $ to $ C (A) $, $ AA ^ + | _ {C (A)} = id, AA ^ + | _ {N (A ^ T) } = 0 $<br>    * Proof 1: $ AA ^ + x = (u_1u_1 ^ T + , … , + u_ru_r ^ T) x = (u_1 ^ Tx) u_1 + , … , + (u_r ^ Tx) u_r $ , Decomposed by singular value svd to get $ V = (v_1, , … \ ,, v_r) $ is the unit orthogonal of $ A ^ T $ column space (ie $ C (A ^ T) $) Eigenvector base, and $ U = (u_1, , … \ ,, u_r) $ is the unit orthogonal eigenvector base of $ C (A) $, so $ AA ^ + $ is projected to $ The orthogonal projection matrix of C (A) $ (that is, the part of $ C (A) $ is reserved), so the transformation of $ AA ^ + $ limited to $ C (A) $ becomes the identity Transform. And $ U $ 中 $ (u_ {r + 1} , … , u_m) $ and $ U ^ T $ 中 $ (u_ {r + 1} , … , u_m) ^ T $ is the base multiplier of $ N (A ^ T) $ multiplied by the matrix $ \ begin {pmatrix} I_r &amp; 0 \ 0 &amp; 0 \ end {pmatrix} _ {m \ times m} $$ 0 in the lower right corner of the $ $ Is equivalent to performing zero transformation on the part belonging to $ N (A ^ T) $.<br>    * Proof 2: $ A ^ + u_j = {1 \ over \ sigma_j} v_j \ Rightarrow AA ^ + u_j = A ({1 \ over \ sigma_j} v_j) = {1 \ over \ sigma_j} Av_j $ according to the singular value During decomposition, $ Av_j = \ sigma u_j, (1 \ le j \ le r) $ gets $ AA ^ + u_j = u_j (1 \ le j \ le r), \ AA ^ + u_j = 0 (r + 1 \ le j \ le m) $<br>    * Verification: $ (AA ^ +) (AA ^ +) = U \ begin {pmatrix} I_r &amp; 0 \ 0 &amp; 0 \ end {pmatrix} _ {m \ times m} U ^ TU \ begin {pmatrix} I_r &amp; 0 \ 0 &amp; 0 \ end {pmatrix} _ {m \ times m} U ^ T $, because $ U $ is the unit orthogonal feature vector basis from svd decomposition, so: $ U ^ T = U ^ {-1} \ Rightarrow (AA ^ +) (AA ^ +) = U \ begin {pmatrix} I_r &amp; 0 \ 0 &amp; 0 \ end {pmatrix} _ {m \ times m} U ^ T = AA ^ + $, which is the nature of projection: multiple projection results This is the first projection result.<br>    * Result: $ \ forall \ p \ in R ^ m, b = p + e, p \ in C (A), e \ in N (A ^ T), AA ^ + b = p $</p>
<p>(3) $ A ^ + A = (V \ Sigma ^ + _ {n \ times m} U ^ T) (U \ Sigma_ {m \ times n} V ^ T) = V \ begin {pmatrix} I_r &amp; 0 \ 0 &amp; 0 \ end {pmatrix} _ {n \ times n} V ^ T $ gets the following three properties (proof as above):</p>
<p>-$ (A ^ + A) ^ T = A ^ + A $<br>-$ A ^ + A = v_1v_1 ^ T + , … , + v_rv_r ^ T $<br>-Orthogonal projection matrix from $ A ^ + A = R ^ n $ to $ C (A ^ T) $ ($ A ^ + A | _ {C (A ^ T)} = id, \ quad A ^ + A | _ {N (A)} = 0 $):<br>    -$ \ forall \ x \ in R ^ n = C (A ^ T) \ bigoplus N (A)), \ x = x_ {1, r} + x_ {r + 1, n}, \ x_ {1, r} \ in C (A ^ T), \ x_ {r + 1, n} \ in N (A ^ T), \ A ^ + Ax = A ^ + A (x_1, , … , x_r, x_ {r + 1}, , … , x_n) = x_ {1, r} $</p>
<h2 id="Why-is-it-called-pseudo-inverse-left-inverse-right-inverse"><a href="#Why-is-it-called-pseudo-inverse-left-inverse-right-inverse" class="headerlink" title="Why is it called pseudo inverse, left inverse, right inverse"></a>Why is it called pseudo inverse, left inverse, right inverse</h2><p>! [why_call_it_as_pseudo-inverse] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvMy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/3.png<i class="fa fa-external-link-alt"></i></span>) </p>
<h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>! [example_of_pseudo-inverse] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvNC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/4.png<i class="fa fa-external-link-alt"></i></span>)<br>** Note: $ u_1, u_2, u_3 $ is a set of bases for $ R ^ m $. Then it is $ {Av_1 \ over \ sigma_1} $, then it ’s easy to calculate, it is $ {1 \ over \ sqrt {2 }} \ begin {pmatrix} 1 \ 1 \ 0 \ end {pmatrix} $ that $ u_2 $ and $ u_3 $ are the feature vectors corresponding to 0, and $ u_2 $ and $ u_3 $ can be regarded as three-dimensional Inside, the unit orthogonal vector given by the orthogonal complement of $ u_1 $ **.</p>
<h2 id="Special-cases"><a href="#Special-cases" class="headerlink" title="Special cases"></a>Special cases</h2><p>! [a_special_case_of_pseudo_inverse] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvNS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/5.png<i class="fa fa-external-link-alt"></i></span>)  </p>
<h2 id="Jordan-standard-pseudo-inverse"><a href="#Jordan-standard-pseudo-inverse" class="headerlink" title="Jordan standard pseudo-inverse"></a>Jordan standard pseudo-inverse</h2><p>! [pseudo-inverse_of_normal_Jordan_form] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvNi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/6.png<i class="fa fa-external-link-alt"></i></span>)<br>** Derivation conclusion: $ J_n ^ + = J_n ^ T $, the pseudo-inverse of Jordan standard form is its own transposition. **</p>
<h2 id="Moore-Penrose-pseudo-inverse"><a href="#Moore-Penrose-pseudo-inverse" class="headerlink" title="Moore-Penrose pseudo-inverse"></a>Moore-Penrose pseudo-inverse</h2><h3 id="EHMoore-pseudo-inverse"><a href="#EHMoore-pseudo-inverse" class="headerlink" title="EHMoore pseudo-inverse"></a>EHMoore pseudo-inverse</h3><p>! [pseudo-inverse_of_E.H.Moore] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvNy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/7.png<i class="fa fa-external-link-alt"></i></span>) </p>
<h3 id="Penrose-pseudo-inverse"><a href="#Penrose-pseudo-inverse" class="headerlink" title="Penrose pseudo-inverse"></a>Penrose pseudo-inverse</h3><p>! [pseudo-inverse_of_Penrose] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvOC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/8.png<i class="fa fa-external-link-alt"></i></span>)<br><strong>Note:</strong> </p>
<ol>
<li>** A can be a complex matrix of mxn, in which case (3) (4) becomes a conjugate transpose. **</li>
<li>** Penrose pseudo-inverse is equivalent to EHMoore pseudo-inverse. **</li>
</ol>
<p>$ (1) AXA = A \ Rightarrow AXAX = AX \ Rightarrow (AX) ^ N = AX \ Rightarrow AX $ is an idempotent matrix, projection matrix<br>$ (2) XAX = X \ Rightarrow XAXA = XA \ Rightarrow (XA) ^ N = XA \ Rightarrow XA $ is an idempotent matrix, projection matrix<br>$ (3) (AX) ^ T = AX \ Rightarrow AX $ is a symmetric matrix<br>$ (4) (XA) ^ T = XA \ Rightarrow XA $ is a symmetric matrix</p>
<p>Pseudo-inverse matrix obtained by singular value decomposition $ A ^ + $, $ AA ^ +: R ^ m \ rightarrow C (A) $, $ A ^ + A: R ^ n \ rightarrow C (A ^ T) = C (A ^ +) $, the above has proved that both are symmetrical, so it meets Penrose’s definition of pseudo-inverse matrix. For the proof of the uniqueness of the pseudo-inverse, the above picture is too small to enlarge.</p>
<h2 id="Application-of-pseudo-inverse-least-squares"><a href="#Application-of-pseudo-inverse-least-squares" class="headerlink" title="Application of pseudo-inverse least squares"></a>Application of pseudo-inverse least squares</h2><h3 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h3><p>! [introductory_content_of_least_squares_approximations_by_pseudo-inverse] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvOS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/9.png<i class="fa fa-external-link-alt"></i></span>)<br>** But we need to request $ e $, which is the solution with the smallest error! ** But at this time, $ A_ {m \ times n} $ is not a column full rank and there is no inverse matrix, so it is natural to think of using pseudo-inverse solution.</p>
<h3 id="Pseudo-inverse-solution-to-normal-equations-best-least-squares-solution"><a href="#Pseudo-inverse-solution-to-normal-equations-best-least-squares-solution" class="headerlink" title="Pseudo-inverse solution to normal equations-best least squares solution"></a>Pseudo-inverse solution to normal equations-best least squares solution</h3><p>! [the_best_solution_of_least_squares_approximations_by_pseudo-inverse] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvMTAucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/10.png<i class="fa fa-external-link-alt"></i></span>)<br>** Note: Since $ A ^ + $ comes from: $ A ^ + U ^ T = V \ Sigma ^ {+}, \ (v_1 , … , v_r) \ in C (A ^ T), \ (v_ {r + 1} , … , v_n) \ in N (A), \ (u_1 , … , u_r) \ in C (A), \ (u_ {r + 1 } , … , u_m) \ in N (A ^ T), \\ Sigma ^ + = \ begin {pmatrix} {1 \ over \ sigma_1} \ &amp; {1 \ over \ sigma_2} \ &amp;&amp;. \ &amp;&amp;&amp;. \ &amp;&amp;&amp;&amp; {1 \ over \ sigma_r} \ &amp;&amp;&amp;&amp;&amp; 0 \ end {pmatrix} _ {n \ times m} \ Rightarrow A ^ +: C (A) \ rightarrow C (A ^ T) $ In addition, because $ A ^ TAx = 0, Ax = 0 $ have the same solution, the zero space is the same. **</p>
<h3 id="Four-basic-subspaces-of-the-best-least-squares-solution"><a href="#Four-basic-subspaces-of-the-best-least-squares-solution" class="headerlink" title="Four basic subspaces of the best least squares solution"></a>Four basic subspaces of the best least squares solution</h3><p>! [4_subspaces_of_best_solution_of_least_squares_approximations] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTYvMTEucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-6/11.png<i class="fa fa-external-link-alt"></i></span>) </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2017/08/05/linear_transformation_2nd_part/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2017/08/05/linear_transformation_2nd_part/" class="post-title-link" itemprop="url">Tsinghua linear-algebra-2 5th-lecture linear-transformation-2nd-part</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2017-08-05 20:16:00" itemprop="dateCreated datePublished" datetime="2017-08-05T20:16:00+05:30">2017-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-09 16:34:43" itemprop="dateModified" datetime="2020-04-09T16:34:43+05:30">2020-04-09</time>
              </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>9.3k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>8 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Notes from: Tsinghua University Open Class: Linear Algebra 2-Lecture 5: Linear Transformation 2</p>
<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><p>For a given linear transformation, choose an appropriate basis to make its matrix representation as simple as possible, we introduce the matrix representation of linear transformation<br>For the linear transformation $ \ sigma $ from the $ n $ -dimensional vector space $ V $ to the $ m $ -dimensional vector space $ W $, we take a set of bases from $ V $ $ v_1 $ to $ v_n $ A set of bases W from $ w_1 $ to $ w_m $, then the linear transformation $ σ $ acting on $ v_1 $ to $ v_n $ can be linearly represented by $ w_1, …, w_m $, and the coefficients we express are represented by a The matrix $ A $ of n $ is described, so the linear transformation $ σ $ corresponds to the matrix $ A $ of $ m × n $. The matrix representation of the linear transformation depends on the choice of our basis. Generally speaking, if the basis is changed, it will have a different matrix representation for the same linear transformation. Then we hope to find the properties of the linear transformation that are independent of the basis selection. When we use the matrix to study these properties of the linear transformation, we can use the matrix representation as simple as possible under the base.</p>
<h2 id="Identity-transformation-and-basis-transformation"><a href="#Identity-transformation-and-basis-transformation" class="headerlink" title="Identity transformation and basis transformation"></a>Identity transformation and basis transformation</h2><p>The identity transformation is invariant, so the invariant linear transformation corresponds to the identity matrix.</p>
<p>! [identical_linear_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/1.png<i class="fa fa-external-link-alt"></i></span>) </p>
<blockquote>
<p>the 9th property of determinant: the determinant of $ AB $ is det $ A $ times det $ B $: $ | AB | = | A || B | $ </p>
</blockquote>
<p>Therefore: because $ (\ sigma_1 , …. , \ sigma_n) $ and $ (\ beta_1 , … , \ beta_n) $ are both base vectors, so they are both full rank and $ n $ dimension, so reversible, then introduce $ P $ reversible. Otherwise $ | \ alpha_1 , … , \ alpha_n | \ ne | \ beta_1 , … , \ beta_n || P | $<br>! [example_identical_linear_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/2.png<i class="fa fa-external-link-alt"></i></span>)  </p>
<h2 id="Application-of-base-transformation"><a href="#Application-of-base-transformation" class="headerlink" title="Application of base transformation"></a>Application of base transformation</h2><h3 id="A-256x256-grayscale-image"><a href="#A-256x256-grayscale-image" class="headerlink" title="A 256x256 grayscale image"></a>A 256x256 grayscale image</h3><p>! [256x256_application_of_change_of_basis] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/3.png<i class="fa fa-external-link-alt"></i></span>) </p>
<p>** Note: $ C ^ N $ is the base of the complex number of $ n $ dimension elements **</p>
<h3 id="Three-of-the-bases-of-the-image"><a href="#Three-of-the-bases-of-the-image" class="headerlink" title="Three of the bases of the image"></a>Three of the bases of the image</h3><p>! [img_basis] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvNC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/4.png<i class="fa fa-external-link-alt"></i></span>) </p>
<p>The wavelet base is good for its inverse, and the Fourier base is good for its inverse. If it is a solid color image of $ 4 \ times4 $, the first components of wavelet base or Fourier base $ w_1 $ and $ \ xi_1 $ are used as the base, which is expressed as $ c_1w_1 = W \ begin {pmatrix} c_1 \ 0 \ 0 \ 0 \ end {pmatrix} $ and $ c_1 \ xi_1 = \ xi \ begin {pmatrix} c_1 \ 0 \ 0 \ 0 \ end {pmatrix} $. For images with more drastic transformations between pixels, you can use $ c (w_3 + w_4) $ in wavelet basis and $ c \ xi_3 $ in Fourier basis.</p>
<h3 id="jpeg"><a href="#jpeg" class="headerlink" title="jpeg"></a>jpeg</h3><p>! [jpeg_process] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvNS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/5.png<i class="fa fa-external-link-alt"></i></span>) </p>
<p>The image itself is represented by the coefficient matrix $ c $, so the so-called compressed and transmitted image is also compressed and transmitted this matrix $ c $. What compression does is use as little information (data) as possible to represent the original information (data). This process will lose some unimportant information (data). Corresponding to the non-zero element comparison of $ c $ on the matrix Less (this requires that a smaller number of basis vectors can be used to approximate the original matrix, the less the better). Because $ c = W ^ {-1} x $, it is also important to be able to quickly calculate the inverse of the base, and wavelet bases and Fourier bases fit this feature.</p>
<h2 id="Linear-transformation-of-matrix-under-different-bases"><a href="#Linear-transformation-of-matrix-under-different-bases" class="headerlink" title="Linear transformation of matrix under different bases"></a>Linear transformation of matrix under different bases</h2><p>! [the_same_linear_transform_of_different_bases] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvNi5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/6.png<i class="fa fa-external-link-alt"></i></span>)  </p>
<p>** Theorem: The linear transformation of the $ n $ vector space $ V $$ \ sigma $ under different bases of $ V $ is a similar matrix. **</p>
<p>! [the_same_linear_transform_of_different_bases_at_different_aspects] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvNy5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/7.png<i class="fa fa-external-link-alt"></i></span>) </p>
<p>From the above picture:<br>$ I_1 $ and $ I_2 $ are identity transformations<br>$ (\ beta_1 , … , \ beta_n) = I_1 (\ beta_1 , … , \ beta_n) = (\ alpha_1 , … , \ alpha_n) P $<br>$ (\ alpha_1 , … , \ alpha_n) = I_2 (\ alpha_1 , … , \ alpha_n) = (\ beta_1 , … , \ beta_n) P ^ {-1} $<br>Linear transformation compound angle: $ \ sigma = I_2 , \ sigma , I_1 , \ rightarrow , B = P ^ {-1} AP $</p>
<h2 id="The-invariance-of-the-same-linear-transformation-under-different-basis"><a href="#The-invariance-of-the-same-linear-transformation-under-different-basis" class="headerlink" title="The invariance of the same linear transformation under different basis"></a>The invariance of the same linear transformation under different basis</h2><p>When we use the matrix to study the linear transformation, we hope to study the properties of the linear transformation independent of the basis selection. From the above discussion, we know that the linear transformation of this vector space $ V $ to itself under different bases is similar to each other. Therefore, the so-called unrelated property is the invariant property under similar transformation, so it is very important to study similar invariants naturally in linear algebra. We know that for a matrix, the characteristic polynomials, eigenvalues, traces, determinants, the rank of the matrix, etc. are all similar invariants of the matrix. In this way, we call an n-dimensional vector space $ V $ linearly transformed in $ V $ The matrix $ A $ under a set of bases, the characteristic polynomial, eigenvalue, trace determinant of the matrix representing $ A $ is called the linear transformation of the characteristic polynomial, eigenvalue, trace, determinant.<br>! [properties_of_the_same_linear_transformation_of_different_bases] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvOC5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/8.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h2 id="Matrix-factorization-and-basis-transformation"><a href="#Matrix-factorization-and-basis-transformation" class="headerlink" title="Matrix factorization and basis transformation"></a>Matrix factorization and basis transformation</h2><p>Given a linear transformation $ σ $ from $ R ^ n $ to $ R ^ m $, its standard basis in $ R ^ n $ $ e_1 $ to $ e_n $ and the standard basis of $ R ^ m $ $ ẽ_1 , …, ẽ_m $ the matrix is ​​$ A $,<br>$ σ $ acting on $ e_1… e_n $ is equal to $ \ tilde {e} _1, …, \ tilde {e} _m $ multiplied by the matrix $ A $, which means that $ σ $ acts on $ e_j $ , Which is equal to the jth column of $ A $, which is $ A $ multiplied by $ e_j $, so this linear transformation can be expressed as any pair of $ n $ -dimensional vector $ v $, then $ σ $ acts on $ v $ is the matrix $ A $ multiplied by $ V $:<br>$$ \ sigma (e_1 , … , e_n) = (\ tilde {e} _1 , … , \ tilde {e} _m) A \ rightarrow \ sigma (e_j) = Ae_j $$</p>
<p>! [input_space_form_of_linear_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvOS5wbmc=">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/9.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>Then do the base transformation, the first changes the input base, the second changes the output base, and the third input and output base changes.</p>
<p>! [matrix_decomposition_and_basis_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTAucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/10.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Diagonalizing-the-matrix-as-a-linear-transformation"><a href="#Diagonalizing-the-matrix-as-a-linear-transformation" class="headerlink" title="Diagonalizing the matrix as a linear transformation"></a>Diagonalizing the matrix as a linear transformation</h3><p>! [vector_basis_of_linear_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTEucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/11.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>From the above, we can get $ \ sigma (x_1 , … , x_n) = (x_1 , … , x_n) \ Lambda = S \ Lambda $, $ x $ is the eigenvector basis, and the other base transformation {id} _1 (S) = S = \ {e } S $<br>The linear transformation of $ σ $ is under the new basis of the eigenvector of A, and its matrix representation is the diagonal matrix of $ \ Lambda $. The matrix of $ σ $ from $ R ^ n $ to $ R ^ n $ under the standard basis is $ A $, and the matrix of $ σ $ under the eigenvector basis is the diagonal matrix $ \ Lambda $. Then input the group of bases of $ x $ and output the group of bases of $ e $, the identity transformation, its matrix representation is $ S $. If you input the base group $ e $ and output the identity transformation of the base group $ x $, its matrix representation is $ S ^ {-1} $.</p>
<h3 id="Singular-value-decomposition-as-a-linear-transformation"><a href="#Singular-value-decomposition-as-a-linear-transformation" class="headerlink" title="Singular value decomposition as a linear transformation"></a>Singular value decomposition as a linear transformation</h3><p>! [SVD_decomposition_as_linear_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTIucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/12.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h2 id="Linear-transformation-kernel-and-image"><a href="#Linear-transformation-kernel-and-image" class="headerlink" title="Linear transformation kernel and image"></a>Linear transformation kernel and image</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>! [definition_kernel_and_image] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTMucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/13.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="Zero-degree-and-rank-of-linear-transformation"><a href="#Zero-degree-and-rank-of-linear-transformation" class="headerlink" title="Zero degree and rank of linear transformation"></a>Zero degree and rank of linear transformation</h3><p>! [nullity_and_rank_of_linear_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTQucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/14.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h4 id="Proof-of-rank-of-linear-transformation"><a href="#Proof-of-rank-of-linear-transformation" class="headerlink" title="Proof of rank of linear transformation"></a>Proof of rank of linear transformation</h4><p>! [proof_nullity_and_rank_of_linear_transformation] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTUucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/15.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>Note: $ L (\ sigma (v_1), , … \ ,, \ sigma (v_n)) $ symbol meaning: by $ \ sigma (v_1), , … \ ,, \ sigma (v_n) $ Linear Zhang Cheng.</p>
<h4 id="Dimensional-formula-of-linear-transformation"><a href="#Dimensional-formula-of-linear-transformation" class="headerlink" title="Dimensional formula of linear transformation"></a>Dimensional formula of linear transformation</h4><p>! [dim (kernel) + dim (image) = dimV.png] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTYucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/16.png<i class="fa fa-external-link-alt"></i></span>)<br>! [kernel + image! = V.png] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTcucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/17.png<i class="fa fa-external-link-alt"></i></span>) </p>
<h4 id="Single-shot-full-shot-reversible"><a href="#Single-shot-full-shot-reversible" class="headerlink" title="Single shot full shot reversible"></a>Single shot full shot reversible</h4><p>** Single shot double shot full shot learned in middle school **</p>
<p>! [injective_surjective_bijective] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTgucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/18.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>** Injective, surjective and inverse under linear transformation **</p>
<p>! [injective_surjective_inverse_of_linear_transformation_are_equivalent] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMTkucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/19.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>The first equivalent symbol proof (counter-proof method): If the single shot cannot launch the kernel only $ \ {0 } $, then assume $ \ exists , \ alpha (\ ne0) \ in {ker , \ sigma} $ then $ \ sigma (\ alpha) = 0 $, and because $ \ sigma (0) = 0 $, that is, $ \ sigma (\ alpha , or , 0) = 0 $ contradicts injective. Conversely, if $ \ sigma (v_1) = 0, \ sigma (v_2) = 0 $, according to the definition or nature of the linear transformation: $ \ sigma (v_1-v_2) = 0 \ rightarrow v_1-v_2 \ in ker , \ sigma = \ {0 } \ rightarrow v_1 = v_2 \ rightarrow \ sigma $ is a single shot. Therefore: $ \ sigma $ is a single shot $ \ Leftarrow \ Rightarrow ker , \ sigma = \ {0 } $</p>
<p><strong>example:</strong></p>
<p>! [example_of_injective_surjective_inverse_of_linear_transformation_are_equivalent] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMjAucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/20.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h2 id="invariant-subspace"><a href="#invariant-subspace" class="headerlink" title="invariant subspace"></a>invariant subspace</h2><h3 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h3><p>! [definition_of_invariant_subspace] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMjEucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/21.png<i class="fa fa-external-link-alt"></i></span>)</p>
<h3 id="The-meaning-of-invariant-subspace"><a href="#The-meaning-of-invariant-subspace" class="headerlink" title="The meaning of invariant subspace"></a>The meaning of invariant subspace</h3><p>! [candy_of_invariant_subspace] (<span class="exturl" data-url="aHR0cDovL3E2Z204Zm9tdy5ia3QuY2xvdWRkbi5jb20vZ2l0cGFnZS90c2luZ2h1YV9saW5lYXJfYWxnZWJyYS8yLTUvMjIucG5n">http://q6gm8fomw.bkt.clouddn.com/gitpage/tsinghua_linear_algebra/2-5/22.png<i class="fa fa-external-link-alt"></i></span>)</p>
<p>From here, we can see that we want to decompose the large space into the direct sum of invariant subspaces, so that we can take out the appropriate base, so that the linear transformation<br>The matrix representation under this set of bases can become the shape of a diagonal block, then the study of linear transformation is transformed into the study that it is limited to an invariant subspace<br>On this basis, look at the structure of the nilpotent transformation.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/18/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><span class="page-number current">19</span><a class="page-number" href="/page/20/">20</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><a class="extend next" rel="next" href="/page/20/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Karan"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Karan</p>
  <div class="site-description" itemprop="description">Refuse to Fall</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">87</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOnNuYWtlY29kaW5nLnB5QGdtYWlsLmNvbQ==" title="Get In Touch → mailto:snakecoding.py@gmail.com"><i class="fa fa-envelope fa-fw"></i>Get In Touch</span>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Karan</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">2.4m</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">35:43</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  
  <script data-pjax>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


    </div>
</body>
</html>
