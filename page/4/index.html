<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"snakecoding.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":"flat","style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":false},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Refuse to Fall">
<meta property="og:type" content="website">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="https://snakecoding.com/page/4/index.html">
<meta property="og:site_name" content="Machine Learning">
<meta property="og:description" content="Refuse to Fall">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Karan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://snakecoding.com/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Machine Learning</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Machine Learning</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">60</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/yourname" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/03/01/practical-aspects-of-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/03/01/practical-aspects-of-deep-learning/" class="post-title-link" itemprop="url">practical-aspects-of-deep-learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-03-01 00:00:00" itemprop="dateCreated datePublished" datetime="2018-03-01T00:00:00+05:30">2018-03-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:38" itemprop="dateModified" datetime="2020-04-06T20:25:38+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>These are my personal programming assignments at the 1th week after studying the course <a href="https://www.coursera.org/learn/deep-neural-network/" target="_blank" rel="noopener">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h1 id="Part-1：Initialization"><a href="#Part-1：Initialization" class="headerlink" title="Part 1：Initialization"></a>Part 1：Initialization</h1><p>A well chosen initialization can: </p>
<ul>
<li>Speed up the convergence of gradient descent </li>
<li>Increase the odds of gradient descent converging to a lower training (and generalization) error</li>
</ul>
<p>To get started, run the following cell to load the packages and the planar dataset you will try to classify.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> sigmoid, relu, compute_loss, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>); <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span>;</span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># load image dataset: blue/red dots in circles</span></span><br><span class="line">train_X, train_Y, test_X, test_Y = load_dataset();</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Initialization/output_1_0.png" alt="png"></p>
<p>There are some import function：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the relu of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- relu(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    s = np.maximum(<span class="number">0</span>,x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_loss</span><span class="params">(a3, Y)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the loss function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a3 -- post-activation, output of forward propagation</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, same shape as a3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss - value of the loss function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(<span class="number">1</span> - a3), <span class="number">1</span> - Y)</span><br><span class="line">    loss = <span class="number">1.</span>/m * np.nansum(logprobs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation (and computes the loss) presented in Figure 2.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape ()</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape ()</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape ()</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape ()</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape ()</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape ()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the loss function (vanilla logistic loss)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    z1 = np.dot(W1, X) + b1</span><br><span class="line">    a1 = relu(z1)</span><br><span class="line">    z2 = np.dot(W2, a1) + b2</span><br><span class="line">    a2 = relu(z2)</span><br><span class="line">    z3 = np.dot(W3, a2) + b3</span><br><span class="line">    a3 = sigmoid(z3)</span><br><span class="line"></span><br><span class="line">    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a3, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(X, Y, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in figure 2.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache</span><br><span class="line"></span><br><span class="line">    dz3 = <span class="number">1.</span>/m * (a3 - Y)</span><br><span class="line">    dW3 = np.dot(dz3, a2.T)</span><br><span class="line">    db3 = np.sum(dz3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    da2 = np.dot(W3.T, dz3)</span><br><span class="line">    dz2 = np.multiply(da2, np.int64(a2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = np.dot(dz2, a1.T)</span><br><span class="line">    db2 = np.sum(dz2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    da1 = np.dot(W2.T, dz2)</span><br><span class="line">    dz1 = np.multiply(da1, np.int64(a1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = np.dot(dz1, X.T)</span><br><span class="line">    db1 = np.sum(dz1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    gradients = &#123;<span class="string">"dz3"</span>: dz3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,</span><br><span class="line">                 <span class="string">"da2"</span>: da2, <span class="string">"dz2"</span>: dz2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2,</span><br><span class="line">                 <span class="string">"da1"</span>: da1, <span class="string">"dz1"</span>: dz1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of n_model_backward</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters['W' + str(i)] = ... </span></span><br><span class="line"><span class="string">                  parameters['b' + str(i)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(k+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(k+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span> + str(k+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(k+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(k+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span> + str(k+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function is used to predict the results of a  n-layer neural network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data set of examples you would like to label</span></span><br><span class="line"><span class="string">    parameters -- parameters of the trained model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    p -- predictions for the given dataset X</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    p = np.zeros((<span class="number">1</span>,m), dtype = np.int)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a3, caches = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert probas to 0/1 predictions</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, a3.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> a3[<span class="number">0</span>,i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            p[<span class="number">0</span>,i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p[<span class="number">0</span>,i] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># print results</span></span><br><span class="line">    print(<span class="string">"Accuracy: "</span>  + str(np.mean((p[<span class="number">0</span>,:] == y[<span class="number">0</span>,:]))))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    train_X, train_Y = sklearn.datasets.make_circles(n_samples=<span class="number">300</span>, noise=<span class="number">.05</span>)</span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    test_X, test_Y = sklearn.datasets.make_circles(n_samples=<span class="number">100</span>, noise=<span class="number">.05</span>)</span><br><span class="line">    <span class="comment"># Visualize the data</span></span><br><span class="line">    plt.scatter(train_X[:, <span class="number">0</span>], train_X[:, <span class="number">1</span>], c=train_Y, s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">    train_X = train_X.T</span><br><span class="line">    train_Y = train_Y.reshape((<span class="number">1</span>, train_Y.shape[<span class="number">0</span>]))</span><br><span class="line">    test_X = test_X.T</span><br><span class="line">    test_Y = test_Y.reshape((<span class="number">1</span>, test_Y.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">return</span> train_X, train_Y, test_X, test_Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, X, y)</span>:</span></span><br><span class="line">    <span class="comment"># Set min and max values and give it some padding</span></span><br><span class="line">    x_min, x_max = X[<span class="number">0</span>, :].min() - <span class="number">1</span>, X[<span class="number">0</span>, :].max() + <span class="number">1</span></span><br><span class="line">    y_min, y_max = X[<span class="number">1</span>, :].min() - <span class="number">1</span>, X[<span class="number">1</span>, :].max() + <span class="number">1</span></span><br><span class="line">    h = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># Generate a grid of points with distance h between them</span></span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">    <span class="comment"># Predict the function value for the whole grid</span></span><br><span class="line">    Z = model(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    <span class="comment"># Plot the contour and training examples</span></span><br><span class="line">    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)</span><br><span class="line">    plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(y), cmap=plt.cm.Spectral)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_dec</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Used for plotting decision boundary.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (m, K)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict using forward propagation and a classification threshold of 0.5</span></span><br><span class="line">    a3, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (a3 &gt; <span class="number">0.5</span>)</span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure>

<p>You would like a classifier to separate the blue dots from the red dots.</p>
<h2 id="1-Neural-Network-model"><a href="#1-Neural-Network-model" class="headerlink" title="1. Neural Network model"></a>1. Neural Network model</h2><p>You will use a 3-layer neural network (already implemented for you). Here are the initialization methods you will experiment with:</p>
<ul>
<li>Zeros initialization – setting initialization = “zeros” in the input argument. </li>
<li>Random initialization – setting initialization = “random” in the input argument. This initializes the weights to large random values. </li>
<li>He initialization – setting initialization = “he” in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015.</li>
</ul>
<p><strong>Instructions</strong>: Please quickly read over the code below, and run it. In the next part you will implement the three initialization methods that this <code>model()</code> calls.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">15000</span>, print_cost = True, initialization = <span class="string">"he"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for gradient descent </span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    initialization -- flag to choose which initialization to use ("zeros","random" or "he")</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = [] <span class="comment"># to keep track of the loss</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    <span class="keyword">if</span> initialization == <span class="string">"zeros"</span>:</span><br><span class="line">        parameters = initialize_parameters_zeros(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"random"</span>:</span><br><span class="line">        parameters = initialize_parameters_random(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"he"</span>:</span><br><span class="line">        parameters = initialize_parameters_he(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        a3, cache = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        cost = compute_loss(a3, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        grads = backward_propagation(X, Y, cache)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the loss every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the loss</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<h2 id="2-Zero-initialization"><a href="#2-Zero-initialization" class="headerlink" title="2. Zero initialization"></a>2. Zero initialization</h2><p>There are two types of parameters to initialize in a neural network: </p>
<ul>
<li>the weight matrices $(W^{[1]},W^{[2]},W^{[3]},…,W^{[L−1]},W^{[L]})$ </li>
<li>the bias vectors $(b^{[1]},b^{[2]},b^{[3]},…,b^{[L−1]},b^{[L]})$</li>
</ul>
<p><strong>Exercise</strong>: Implement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry”, but lets try it anyway and see what happens. Use <code>np.zeros((..,..))</code> with the correct shapes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_zeros </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;&#125;;</span><br><span class="line">    L = len(layers_dims);            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l],layers_dims[l<span class="number">-1</span>]));</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>));</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_zeros([<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>]);</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]));</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]));</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]));</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[0. 0. 0.]
 [0. 0. 0.]]
b1 = [[0.]
 [0.]]
W2 = [[0. 0.]]
b2 = [[0.]]</code></pre><p>Run the following code to train your model on 15,000 iterations using zeros initialization.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"zeros"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>);</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>);</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.6931471805599453
Cost after iteration 1000: 0.6931471805599453
Cost after iteration 2000: 0.6931471805599453
Cost after iteration 3000: 0.6931471805599453
Cost after iteration 4000: 0.6931471805599453
Cost after iteration 5000: 0.6931471805599453
Cost after iteration 6000: 0.6931471805599453
Cost after iteration 7000: 0.6931471805599453
Cost after iteration 8000: 0.6931471805599453
Cost after iteration 9000: 0.6931471805599453
Cost after iteration 10000: 0.6931471805599455
Cost after iteration 11000: 0.6931471805599453
Cost after iteration 12000: 0.6931471805599453
Cost after iteration 13000: 0.6931471805599453
Cost after iteration 14000: 0.6931471805599453</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Initialization/output_10_1.png" alt="png"></p>
<pre><code>On the train set:
Accuracy: 0.5
On the test set:
Accuracy: 0.5</code></pre><p>The performance is really bad, and the cost does not really decrease, and the algorithm performs no better than random guessing. Why? Lets look at the details of the predictions and the decision boundary:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"predictions_train = "</span> + str(predictions_train));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"predictions_test = "</span> + str(predictions_test));</span><br></pre></td></tr></table></figure>

<pre><code>predictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0]]
predictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with Zeros initialization"</span>);</span><br><span class="line">axes = plt.gca();</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>]);</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>]);</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y);</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Initialization/output_13_0.png" alt="png"></p>
<p>The model is predicting 0 for every example.</p>
<p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with $n^{[l]} = 1$ for every layer, and the network is no more powerful than a linear classifier such as logistic regression.</p>
<p>What you should remember: </p>
<ul>
<li>The weights $W^{[l]}$ should be initialized randomly to break symmetry. </li>
<li>It is however okay to initialize the biases $b^{[l]}$ to zeros. Symmetry is still broken so long as $W^{[l]}$ is initialized randomly.</li>
</ul>
<h2 id="3-Random-initialization"><a href="#3-Random-initialization" class="headerlink" title="3. Random initialization"></a>3. Random initialization</h2><p>To break symmetry, lets intialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you will see what happens if the weights are intialized randomly, but to very large values.</p>
<p><strong>Exercise</strong>: </p>
<p>Implement the following function to initialize your weights to large random values (scaled by * 10) and your biases to zeros. Use <code>np.random.randn(...) * 10</code> for weights and <code>np.zeros((...))</code> for biases. We are using a fixed <code>np.random.seed(..)</code> to make sure your “random” weights match ours, so don’t worry if running several times your code gives you always the same initial values for the parameters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_random</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>);               <span class="comment"># This seed makes sure your "random" numbers will be the as ours</span></span><br><span class="line">    parameters = &#123;&#125;;</span><br><span class="line">    L = len(layers_dims);            <span class="comment"># integer representing the number of layers</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - <span class="number">1</span>]) * <span class="number">10</span>;</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>));</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_random([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]);</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]));</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]));</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]));</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 17.88628473   4.36509851   0.96497468]
 [-18.63492703  -2.77388203  -3.54758979]]
b1 = [[0.]
 [0.]]
W2 = [[-0.82741481 -6.27000677]]
b2 = [[0.]]</code></pre><p>Run the following code to train your model on 15,000 iterations using random initialization.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"random"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>);</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>);</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: inf


C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:44: RuntimeWarning: divide by zero encountered in log
C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:44: RuntimeWarning: invalid value encountered in multiply


Cost after iteration 1000: 0.6243339944795463
Cost after iteration 2000: 0.5983698376976234
Cost after iteration 3000: 0.5640713641303857
Cost after iteration 4000: 0.5502225777263651
Cost after iteration 5000: 0.5445189912897229
Cost after iteration 6000: 0.5374939942050982
Cost after iteration 7000: 0.47927872911735586
Cost after iteration 8000: 0.39787508336662053
Cost after iteration 9000: 0.3934925383461005
Cost after iteration 10000: 0.3920373161708829
Cost after iteration 11000: 0.38930570830972355
Cost after iteration 12000: 0.3861562072516527
Cost after iteration 13000: 0.38499595295812233
Cost after iteration 14000: 0.38280923039736164</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Initialization/output_18_3.png" alt="png"></p>
<pre><code>On the train set:
Accuracy: 0.83
On the test set:
Accuracy: 0.86</code></pre><p>If you see “inf” as the cost after the iteration 0, this is because of numerical roundoff; a more numerically sophisticated implementation would fix this. But this isn’t worth worrying about for our purposes.</p>
<p>Anyway, it looks like you have broken symmetry, and this gives better results. than before. The model is no longer outputting all 0s.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (predictions_train);</span><br><span class="line"><span class="keyword">print</span> (predictions_test);</span><br></pre></td></tr></table></figure>

<pre><code>[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1
  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0
  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0
  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0
  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1
  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1
  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1
  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1
  1 1 1 1 0 0 0 1 1 1 1 0]]
[[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1
  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0
  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with large random initialization"</span>);</span><br><span class="line">axes = plt.gca();</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>]);</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>]);</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y);</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Initialization/output_21_0.png" alt="png"></p>
<p><strong>Observations</strong>:</p>
<ul>
<li>The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when $log(a^{[3]})=log(0)$, the loss goes to infinity. </li>
<li>Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. </li>
<li>If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.</li>
</ul>
<p><strong>In summary</strong>: </p>
<ul>
<li>Initializing weights to very large random values does not work well. </li>
<li>Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part!</li>
</ul>
<h2 id="4-He-initialization"><a href="#4-He-initialization" class="headerlink" title="4. He initialization"></a>4. He initialization</h2><p>Finally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights $W^{[l]}$ of <code>sqrt(1./layers_dims[l-1])</code> where He initialization would use <code>sqrt(2./layers_dims[l-1])</code>.)</p>
<p><strong>Exercise</strong>: </p>
<p>Implement the following function to initialize your parameters with He initialization.</p>
<p><strong>Hint</strong>: </p>
<p>This function is similar to the previous <code>initialize_parameters_random()</code>. The only difference is that instead of multiplying <code>np.random.randn(..,..)</code> by 10, you will multiply it by 2dimension of the previous layer $\sqrt{\frac{2}{\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_he</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>);</span><br><span class="line">    parameters = &#123;&#125;;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span>; <span class="comment"># integer representing the number of layers</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - <span class="number">1</span>]) * np.sqrt(<span class="number">2</span> / layers_dims[l - <span class="number">1</span>]);</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>));</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_he([<span class="number">2</span>, <span class="number">4</span>, <span class="number">1</span>]);</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]));</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]));</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]));</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 1.78862847  0.43650985]
 [ 0.09649747 -1.8634927 ]
 [-0.2773882  -0.35475898]
 [-0.08274148 -0.62700068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]
b2 = [[0.]]</code></pre><p>Run the following code to train your model on 15,000 iterations using He initialization.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"he"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>);</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>);</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.8830537463419761
Cost after iteration 1000: 0.6879825919728063
Cost after iteration 2000: 0.6751286264523371
Cost after iteration 3000: 0.6526117768893807
Cost after iteration 4000: 0.6082958970572938
Cost after iteration 5000: 0.5304944491717495
Cost after iteration 6000: 0.4138645817071795
Cost after iteration 7000: 0.31178034648444414
Cost after iteration 8000: 0.23696215330322562
Cost after iteration 9000: 0.18597287209206836
Cost after iteration 10000: 0.15015556280371808
Cost after iteration 11000: 0.12325079292273551
Cost after iteration 12000: 0.09917746546525934
Cost after iteration 13000: 0.08457055954024277
Cost after iteration 14000: 0.07357895962677363</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Initialization/output_27_1.png" alt="png"></p>
<pre><code>On the train set:
Accuracy: 0.9933333333333333
On the test set:
Accuracy: 0.96</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with He initialization"</span>);</span><br><span class="line">axes = plt.gca();</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>]);</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>]);</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y);</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Initialization/output_28_0.png" alt="png"></p>
<p><strong>Observations</strong>: </p>
<ul>
<li>The model with He initialization separates the blue and the red dots very well in a small number of iterations.</li>
</ul>
<h2 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5. Conclusions"></a>5. Conclusions</h2><p>You have seen three different types of initializations. For the same number of iterations and same hyperparameters the comparison is:</p>
<p>comparison is:</p>
<table>
<thead>
<tr>
<th align="center"><strong>Model</strong></th>
<th align="center"><strong>Train accuracy</strong></th>
<th align="center"><strong>Problem/Comment</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">3-layer NN with zeros initialization</td>
<td align="center">50%</td>
<td align="center">fails to break symmetry</td>
</tr>
<tr>
<td align="center">3-layer NN with large random initialization</td>
<td align="center">83%</td>
<td align="center">too large weights</td>
</tr>
<tr>
<td align="center">3-layer NN with He initialization</td>
<td align="center">99%</td>
<td align="center">recommended method</td>
</tr>
</tbody></table>
<p>What you should remember from this notebook: </p>
<ul>
<li>Different initializations lead to different results </li>
<li>Random initialization is used to break symmetry and make sure different hidden units can learn different things </li>
<li>Don’t intialize to values that are too large </li>
<li>He initialization works well for networks with <code>ReLU</code> activations.</li>
</ul>
<h1 id="Part-2：Regularization"><a href="#Part-2：Regularization" class="headerlink" title="Part 2：Regularization"></a>Part 2：Regularization</h1><p>Let’s first import the packages you are going to use.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure>

<pre><code>C:\Anaconda3\lib\site-packages\h5py\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters</code></pre><p>There are some function imported：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (1, layer_dims[l])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - For example: the layer_dims for the "Planar Data classification model" would have been [2,2,1]. </span></span><br><span class="line"><span class="string">    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!</span></span><br><span class="line"><span class="string">    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims) <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>]) / np.sqrt(layer_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == layer_dims[l], layer_dims[l<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == layer_dims[l], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(a3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a3 -- post-activation, output of forward propagation</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, same shape as a3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(<span class="number">1</span> - a3), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">1.</span>/m * np.nansum(logprobs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_2D_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    data = scipy.io.loadmat(<span class="string">'datasets/data.mat'</span>)</span><br><span class="line">    train_X = data[<span class="string">'X'</span>].T</span><br><span class="line">    train_Y = data[<span class="string">'y'</span>].T</span><br><span class="line">    test_X = data[<span class="string">'Xval'</span>].T</span><br><span class="line">    test_Y = data[<span class="string">'yval'</span>].T</span><br><span class="line"></span><br><span class="line">    plt.scatter(train_X[<span class="number">0</span>, :], train_X[<span class="number">1</span>, :], c=np.squeeze(train_Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_X, train_Y, test_X, test_Y</span><br></pre></td></tr></table></figure>

<pre><code>&lt;ipython-input-2-41dc022e1c22&gt;:27: SyntaxWarning: assertion is always true, perhaps remove parentheses?
  assert(parameters[&apos;W&apos; + str(l)].shape == layer_dims[l], layer_dims[l-1])
&lt;ipython-input-2-41dc022e1c22&gt;:28: SyntaxWarning: assertion is always true, perhaps remove parentheses?
  assert(parameters[&apos;W&apos; + str(l)].shape == layer_dims[l], 1)</code></pre><p><strong>Problem Statement</strong>: You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France’s goal keeper should kick the ball so that the French team’s players can then hit it with their head.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/1.png" alt=""></p>
<p>$$\text{Figure 1 : Football field}$$<br>$$\text{The goal keeper kicks the ball in the air, the players of each team are fighting to hit the ball with their head}$$</p>
<p>They give you the following 2D dataset from France’s past 10 games.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = load_2D_dataset();</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/output_6_0.png" alt="png"></p>
<p>Each dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field. </p>
<ul>
<li>If the dot is blue, it means the French player managed to hit the ball with his/her head </li>
<li>If the dot is red, it means the other team’s player hit the ball with their head</li>
</ul>
<p><strong>Your goal</strong>: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.</p>
<p><strong>Analysis of the dataset</strong>: This dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well.</p>
<p>You will first try a non-regularized model. Then you’ll learn how to regularize it and decide which model you will choose to solve the French Football Corporation’s problem.</p>
<h2 id="1-Non-regularized-model"><a href="#1-Non-regularized-model" class="headerlink" title="1. Non-regularized model"></a>1. Non-regularized model</h2><p>You will use the following neural network (already implemented for you below). This model can be used: </p>
<ul>
<li>in regularization mode – by setting the lambd input to a non-zero value. We use “lambd” instead of “lambda” because “lambda” is a reserved keyword in Python. </li>
<li>in dropout mode – by setting the keep_prob to a value less than one</li>
</ul>
<p>You will first try the model without any regularization. Then, you will implement: </p>
<ul>
<li>L2 regularization – functions: “<code>compute_cost_with_regularization()</code>” and “<code>backward_propagation_with_regularization()</code>” </li>
<li>Dropout – functions: “<code>forward_propagation_with_dropout()</code>” and “<code>backward_propagation_with_dropout()</code>”</li>
</ul>
<p>In each part, you will run this model with the correct inputs so that it calls the functions you’ve implemented. Take a look at the code below to familiarize yourself with the model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- If True, print the cost every 10000 iterations</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learned by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost function</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># it is possible to use both L2 regularization and dropout, </span></span><br><span class="line">                                            <span class="comment"># but this assignment will only explore one at a time</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the loss every 10000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>

<p>Let’s train the model without any regularization, and observe the accuracy on the train/test sets.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the training set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.6557412523481002
Cost after iteration 10000: 0.16329987525724218
Cost after iteration 20000: 0.13851642423267105</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/output_10_1.png" alt="png"></p>
<pre><code>On the training set:
Accuracy: 0.9478672985781991
On the test set:
Accuracy: 0.915</code></pre><p>The train accuracy is 94.8% while the test accuracy is 91.5%. This is the <strong>baseline model</strong> (you will observe the impact of regularization on this model). Run the following code to plot the decision boundary of your model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model without regularization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/output_12_0.png" alt="png"></p>
<p>The non-regularized model is obviously overfitting the training set. <strong>It is fitting the noisy points</strong>! Lets now look at two techniques to reduce overfitting.</p>
<h2 id="2-L2-Regularization"><a href="#2-L2-Regularization" class="headerlink" title="2. L2 Regularization"></a>2. L2 Regularization</h2><p>The standard way to avoid overfitting is called <strong>L2 regularization</strong>. It consists of appropriately modifying your cost function, from:<br>$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{<a href="i">L</a>}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right) \large{)} \tag{1}$$<br>to:<br>$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{<a href="i">L</a>}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right) \large{)} }<em>\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W</em>{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$</p>
<p>Let’s modify your cost and observe the consequences.</p>
<p><strong>Exercise</strong>: Implement <code>compute_cost_with_regularization()</code> which computes the cost given by formula (2). To calculate $\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}$, use : <code>np.sum(np.square(Wl))</code></p>
<p><strong>Note</strong> that you have to do this for $W^{[1]}$, $W^{[2]}$ and $W^{[3]}$, then sum the three terms and multiply by $\frac{1}{m}\frac{\lambda}{2}$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost_with_regularization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function with L2 regularization. See formula (2) above.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing parameters of the model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the regularized loss function (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>];</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>];</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>];</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>];</span><br><span class="line"></span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y); <span class="comment"># This gives you the cross-entropy part of the cost</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    L2_regularization_cost = lambd / m / <span class="number">2</span> * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3)));</span><br><span class="line">    <span class="comment">### END CODER HERE ###</span></span><br><span class="line"></span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A3, Y_assess, parameters = compute_cost_with_regularization_test_case();</span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = <span class="number">0.1</span>)));</span><br></pre></td></tr></table></figure>

<pre><code>cost = 1.7864859451590758</code></pre><p>Of course, because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost.</p>
<p><strong>Exercise</strong>: Implement the changes needed in backward propagation to take into account regularization. The changes only concern dW1, dW2 and dW3. For each, you have to add the regularization term’s gradient $(\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation_with_regularization</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation()</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>];</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache;</span><br><span class="line"></span><br><span class="line">    dZ3 = A3 - Y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW3 = <span class="number">1</span> / m * np.dot(dZ3, A2.T) + lambd / m * W3;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line"></span><br><span class="line">    dA2 = np.dot(W3.T, dZ3);</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>));</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW2 = <span class="number">1</span> / m * np.dot(dZ2, A1.T) + lambd / m * W2;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line"></span><br><span class="line">    dA1 = np.dot(W2.T, dZ2);</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>));</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW1 = <span class="number">1</span> / m * np.dot(dZ1, X.T) + lambd / m * W1;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line"></span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW3 = "</span>+ str(grads[<span class="string">"dW3"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>dW1 = [[-0.25604646  0.12298827 -0.28297129]
 [-0.17706303  0.34536094 -0.4410571 ]]
dW2 = [[ 0.79276486  0.85133918]
 [-0.0957219  -0.01720463]
 [-0.13100772 -0.03750433]]
dW3 = [[-1.77691347 -0.11832879 -0.09397446]]</code></pre><p>Let’s now run the model with L2 regularization $(λ=0.7)$. The <code>model()</code> function will call: </p>
<ul>
<li><code>compute_cost_with_regularization</code> instead of <code>compute_cost</code> </li>
<li><code>backward_propagation_with_regularization</code> instead of <code>backward_propagation</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>);</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>);</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.6974484493131264
Cost after iteration 10000: 0.2684918873282239
Cost after iteration 20000: 0.26809163371273004</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/output_20_1.png" alt="png"></p>
<pre><code>On the train set:
Accuracy: 0.9383886255924171
On the test set:
Accuracy: 0.93</code></pre><p>Congrats, the test set accuracy increased to 93%. You have saved the French football team!</p>
<p>You are not overfitting the training data anymore. Let’s plot the decision boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with L2-regularization"</span>);</span><br><span class="line">axes = plt.gca();</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>, <span class="number">0.40</span>]);</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>, <span class="number">0.65</span>]);</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y);</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/output_22_0.png" alt="png"></p>
<p><strong>Observations</strong>: </p>
<ul>
<li>The value of $λ$ is a hyperparameter that you can tune using a dev set. </li>
<li>L2 regularization makes your decision boundary smoother. If $λ$ is too large, it is also possible to “oversmooth”, resulting in a model with high bias.</li>
</ul>
<p>What is L2-regularization actually doing?:</p>
<p>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.</p>
<p>What you should remember – the implications of L2-regularization on: </p>
<ul>
<li>The cost computation: <ul>
<li>A regularization term is added to the cost </li>
</ul>
</li>
<li>The backpropagation function: <ul>
<li>There are extra terms in the gradients with respect to weight matrices </li>
</ul>
</li>
<li>Weights end up smaller (“weight decay”): <ul>
<li>Weights are pushed to smaller values.</li>
</ul>
</li>
</ul>
<h2 id="3-Dropout"><a href="#3-Dropout" class="headerlink" title="3. Dropout"></a>3. Dropout</h2><p>Finally, dropout is a widely used regularization technique that is specific to deep learning.<br>It randomly shuts down some neurons in each iteration.</p>
<p>When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p>
<h3 id="3-1-Forward-propagation-with-dropout"><a href="#3-1-Forward-propagation-with-dropout" class="headerlink" title="3.1 Forward propagation with dropout"></a>3.1 Forward propagation with dropout</h3><p><strong>Exercise</strong>: Implement the forward propagation with dropout. You are using a 3 layer neural network, and will add dropout to the first and second hidden layers. We will not apply dropout to the input layer or output layer.</p>
<p><strong>Instructions</strong>: </p>
<p>You would like to shut down some neurons in the first and second layers. To do that, you are going to carry out 4 Steps: </p>
<ol>
<li><p>In lecture, we dicussed creating a variable $d^{[1]}$ with the same shape as $a^{[1]}$ using <code>np.random.rand()</code> to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix $D^{[1]}=[d^{[1]}<em>{(1)}d^{[1]}</em>{(2)}…d^{[1]}_{(m)}]$ of the same dimension as $A^{[1]}$.</p>
</li>
<li><p>Set each entry of $D^{[1]}$ to be 0 with probability (<code>1 - keep_prob</code>) or 1 with probability (<code>keep_prob</code>), by thresholding values in $D^{[1]}$ appropriately. <strong>Hint</strong>: to set all the entries of a matrix X to 0 (if entry is less than 0.5) or 1 (if entry is more than 0.5) you would do: <code>X = (X &lt; 0.5)</code>. Note that 0 and 1 are respectively equivalent to False and True. </p>
</li>
<li><p>Set $A^{[1]}$ to $A^{[1]}∗ D^{[1]}$. (You are shutting down some neurons). You can think of $D^{[1]}$ as a mask, so that when it is multiplied with another matrix, it shuts down some of the values. </p>
</li>
<li><p>Divide $A^{[1]}$ by <code>keep_prob</code>. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation_with_dropout</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>];</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>];</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>];</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>];</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>];</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1;</span><br><span class="line">    A1 = relu(Z1);</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>]);</span><br><span class="line">    D1 = D1 &lt; keep_prob;</span><br><span class="line">    A1 = np.multiply(D1, A1);</span><br><span class="line">    A1 /= keep_prob;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2;</span><br><span class="line">    A2 = relu(Z2);</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>], A2.shape[<span class="number">1</span>]);</span><br><span class="line">    D2 = D2 &lt; keep_prob;</span><br><span class="line">    A2 = np.multiply(D2, A2);</span><br><span class="line">    A2 /= keep_prob;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3;</span><br><span class="line">    A3 = sigmoid(Z3);</span><br><span class="line"></span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A3, cache;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters = forward_propagation_with_dropout_test_case();</span><br><span class="line">A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = <span class="number">0.7</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"A3 = "</span> + str(A3));</span><br></pre></td></tr></table></figure>

<pre><code>A3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]</code></pre><h2 id="3-2-Backward-propagation-with-dropout"><a href="#3-2-Backward-propagation-with-dropout" class="headerlink" title="3.2 Backward propagation with dropout"></a>3.2 Backward propagation with dropout</h2><p><strong>Exercise</strong>: Implement the backward propagation with dropout. As before, you are training a 3 layer network. Add dropout to the first and second hidden layers, using the masks $D^{[1]}$ and $D^{[2]}$ stored in the cache.</p>
<p><strong>Instruction</strong>: Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps: </p>
<ol>
<li>You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to A1. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to dA1. </li>
<li>During forward propagation, you had divided A1 by <code>keep_prob</code>. In backpropagation, you’ll therefore have to divide dA1 by <code>keep_prob</code> again (the calculus interpretation is that if $A^{[1]}$ is scaled by <code>keep_prob</code>, then its derivative $dA^{[1]}$ is also scaled by the same <code>keep_prob</code>).</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation_with_dropout</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_with_dropout()</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>];</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache;</span><br><span class="line"></span><br><span class="line">    dZ3 = A3 - Y;</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T);</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3);</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA2 = np.multiply(D2, dA2);             <span class="comment"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA2 /= keep_prob;                       <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>));</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T);</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line"></span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA1 = np.multiply(D1, dA1);             <span class="comment"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA1 /= keep_prob;             <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>));</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T);</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line"></span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess, cache = backward_propagation_with_dropout_test_case();</span><br><span class="line"></span><br><span class="line">gradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob = <span class="number">0.8</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA1 = "</span> + str(gradients[<span class="string">"dA1"</span>]));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA2 = "</span> + str(gradients[<span class="string">"dA2"</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>dA1 = [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]
 [ 0.65515713  0.         -0.00337459  0.         -0.        ]]
dA2 = [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]
 [ 0.          0.53159854 -0.          0.53159854 -0.34089673]
 [ 0.          0.         -0.00292733  0.         -0.        ]]</code></pre><p>Let’s now run the model with dropout (<code>keep_prob = 0.86</code>). It means at every iteration you shut down each neurons of layer 1 and 2 with 24% probability. The function <code>model()</code> will now call: </p>
<ul>
<li><code>forward_propagation_with_dropout</code> instead of <code>forward_propagation</code>. </li>
<li><code>backward_propagation_with_dropout</code> instead of <code>backward_propagation</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>);</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>);</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.6543912405149825


C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:47: RuntimeWarning: divide by zero encountered in log
C:\Anaconda3\lib\site-packages\ipykernel\__main__.py:47: RuntimeWarning: invalid value encountered in multiply


Cost after iteration 10000: 0.061016986574905584
Cost after iteration 20000: 0.060582435798513114</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/output_31_3.png" alt="png"></p>
<pre><code>On the train set:
Accuracy: 0.9289099526066351
On the test set:
Accuracy: 0.95</code></pre><p>Dropout works great! The test accuracy has increased again (to 95%)! Your model is not overfitting the training set and does a great job on the test set. The French football team will be forever grateful to you!</p>
<p>Run the code below to plot the decision boundary.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with dropout"</span>);</span><br><span class="line">axes = plt.gca();</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>]);</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>]);</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y);</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/Regularization/output_33_0.png" alt="png"></p>
<p><strong>Note</strong>: </p>
<ul>
<li>A common mistake when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training. </li>
<li>Deep learning frameworks like tensorflow, PaddlePaddle, keras or caffe come with a dropout layer implementation. Don’t stress - you will soon learn some of these frameworks.</li>
</ul>
<p><strong>What you should remember about dropout</strong>: </p>
<ul>
<li>Dropout is a regularization technique. </li>
<li>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time. </li>
<li>Apply dropout both during forward and backward propagation. </li>
<li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when <code>keep_prob</code> is other values than 0.5.</li>
</ul>
<h2 id="4-Conclusions"><a href="#4-Conclusions" class="headerlink" title="4. Conclusions"></a>4. Conclusions</h2><p>Here are the results of our three models:</p>
<table>
<thead>
<tr>
<th><strong>model</strong></th>
<th><strong>train accuracy</strong></th>
<th><strong>test accuracy</strong></th>
</tr>
</thead>
<tbody><tr>
<td>3-layer NN without regularization</td>
<td>95%</td>
<td>91.5%</td>
</tr>
<tr>
<td>3-layer NN with L2-regularization</td>
<td>94%</td>
<td>93%</td>
</tr>
<tr>
<td>3-layer NN with dropout</td>
<td>93%</td>
<td>95%</td>
</tr>
</tbody></table>
<p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.</p>
<p>Congratulations for finishing this assignment! And also for revolutionizing French football. :-)</p>
<p>What we want you to remember from this notebook: </p>
<ul>
<li>Regularization will help you reduce overfitting. </li>
<li>Regularization will drive your weights to lower values. </li>
<li>L2 regularization and Dropout are two very effective regularization techniques.</li>
</ul>
<h1 id="Part-3：Gradient-Checking"><a href="#Part-3：Gradient-Checking" class="headerlink" title="Part 3：Gradient Checking"></a>Part 3：Gradient Checking</h1><p>Welcome to the final assignment for this week! In this assignment you will learn to implement and use gradient checking. </p>
<p>You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud–whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user’s account has been taken over by a hacker. </p>
<p>But backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company’s CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, “Give me a proof that your backpropagation is actually working!” To give this reassurance, you are going to use “gradient checking”.</p>
<p>Let’s do it!</p>
<p>First import the libs which you will need.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Packages</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> gc_utils <span class="keyword">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector</span><br></pre></td></tr></table></figure>

<h2 id="1-How-does-gradient-checking-work"><a href="#1-How-does-gradient-checking-work" class="headerlink" title="1. How does gradient checking work?"></a>1. How does gradient checking work?</h2><p>Backpropagation computes the gradients $\frac{∂J}{∂θ}$ , where $θ$ denotes the parameters of the model. $J$ is computed using forward propagation and your loss function.</p>
<p>Because forward propagation is relatively easy to implement, you’re confident you got that right, and so you’re almost 100% sure that you’re computing the cost $J$ correctly. Thus, you can use your code for computing $J$ to verify the code for computing $\frac{∂J}{∂θ}$.</p>
<p>Let’s look back at the definition of a derivative (or gradient):<br>$$\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}$$</p>
<p>If you’re not familiar with the “$limε→0$” notation, it’s just a way of saying “when $ε$ is really really small.”</p>
<p>We know the following:</p>
<p>$\frac{∂J}{∂θ}$ is what you want to make sure you’re computing correctly.<br>You can compute $J(θ+ε)$ and $J(θ−ε)$ (in the case that $θ$ is a real number), since you’re confident your implementation for $J$ is correct.<br>Lets use equation (1) and a small value for $ε$ to convince your CEO that your code for computing $\frac{∂J}{∂θ}$ is correct!</p>
<h2 id="2-1-dimensional-gradient-checking"><a href="#2-1-dimensional-gradient-checking" class="headerlink" title="2. 1-dimensional gradient checking"></a>2. 1-dimensional gradient checking</h2><p>Consider a 1D linear function $J(θ)=θx$. The model contains only a single real-valued parameter $θ$, and takes $x$ as input.</p>
<p>You will implement code to compute $J(.)$ and its derivative $\frac{∂J}{∂θ}$. You will then use gradient checking to make sure your derivative computation for $J$ is correct.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/GradientChecking/1.png" alt=""><br>$$\text{Figure 1 : 1D linear model}$$</p>
<p>The diagram above shows the key computation steps: First start with $x$, then evaluate the function $J(x)$ (“forward propagation”). Then compute the derivative $\frac{∂J}{∂θ}$ (“backward propagation”).</p>
<p><strong>Exercise</strong>: implement “<code>forward propagation</code>” and “<code>backward propagation</code>” for this simple function. I.e., compute both $J(.)$ (“<code>forward propagation</code>”) and its derivative with respect to $θ$ (“<code>backward propagation</code>”), in two separate functions.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- the value of function J, computed using the formula J(theta) = theta * x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    J = theta*x</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x, theta = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">J = forward_propagation(x, theta)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"J = "</span> + str(J))</span><br></pre></td></tr></table></figure>

<pre><code>J = 8</code></pre><p><strong>Exercise</strong>: Now, implement the backward propagation step (derivative computation) of Figure 1. That is, compute the derivative of $J(θ)=θx$ with respect to $θ$. To save you from doing the calculus, you should get $d\theta=\frac{∂J}{∂θ}=x$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the derivative of J with respect to theta (see Figure 1).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dtheta -- the gradient of the cost with respect to theta</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dtheta = x;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dtheta</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x, theta = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">dtheta = backward_propagation(x, theta)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dtheta = "</span> + str(dtheta))</span><br></pre></td></tr></table></figure>

<pre><code>dtheta = 2</code></pre><p><strong>Exercise</strong>: To show that the <code>backward_propagation()</code> function is correctly computing the gradient $\frac{∂J}{∂θ}$, let’s implement gradient checking.</p>
<p><strong>Instructions</strong>:</p>
<ul>
<li>First compute “<code>gradapprox</code>” using the formula above (1) and a small value of $ε$. Here are the Steps to follow: <ol>
<li>$\theta^+ = \theta + \epsilon$</li>
<li>$\theta^- = \theta - \epsilon$</li>
<li>$J^+ = J(\theta^+)$</li>
<li>$J^- = J(\theta^-)$</li>
<li>$gradapprox=\frac{J^+-J^-}{2\epsilon}$</li>
</ol>
</li>
<li>Then compute the gradient using backward propagation, and store the result in a variable “grad” </li>
<li>Finally, compute the relative difference between “gradapprox” and the “grad” using the following formula:<br>  $$difference = \frac {\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2} \tag{2}$$</li>
</ul>
<p>You will need 3 Steps to compute this formula: </p>
<ul>
<li>1’. compute the numerator using <code>np.linalg.norm(…)</code> </li>
<li>2’. compute the denominator. You will need to call <code>np.linalg.norm(…)</code> twice. </li>
<li>3’. divide them. </li>
</ul>
<p>If this difference is small (say less than 10−7), you can be quite confident that you have computed your gradient correctly. Otherwise, there may be a mistake in the gradient computation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: gradient_check</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(x, theta, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></span><br><span class="line">    thetaplus = theta + epsilon</span><br><span class="line">    thetaminus = theta - epsilon</span><br><span class="line">    J_plus = forward_propagation(x, thetaplus)</span><br><span class="line">    J_minus = forward_propagation(x, thetaminus)</span><br><span class="line">    gradapprox = (J_plus-J_minus)/(<span class="number">2.</span>*epsilon)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check if gradapprox is close enough to the output of backward_propagation()</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    grad = backward_propagation(x, theta)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    difference = np.linalg.norm(grad-gradapprox)/(np.linalg.norm(grad) + np.linalg.norm(gradapprox))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &lt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is correct!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is wrong!"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x, theta = <span class="number">2</span>, <span class="number">4</span></span><br><span class="line">difference = gradient_check(x, theta)</span><br><span class="line">print(<span class="string">"difference = "</span> + str(difference))</span><br></pre></td></tr></table></figure>

<pre><code>The gradient is correct!
difference = 2.919335883291695e-10</code></pre><p>Congrats, the difference is smaller than the $10^{−7}$ threshold. So you can have high confidence that you’ve correctly computed the gradient in <code>backward_propagation()</code>.</p>
<p>Now, in the more general case, your cost function $J$ has more than a single 1D input. When you are training a neural network, $θ$ actually consists of multiple matrices $W^{[l]}$ and biases $b^{[l]}$! It is important to know how to do a gradient check with higher-dimensional inputs. Let’s do it!</p>
<h2 id="3-N-dimensional-gradient-checking"><a href="#3-N-dimensional-gradient-checking" class="headerlink" title="3. N-dimensional gradient checking"></a>3. N-dimensional gradient checking</h2><p>The following figure describes the forward and backward propagation of your fraud detection model.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/GradientChecking/2.png" alt=""><br>$$\text{Figure 2 : deep neural network} \ LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID$$</p>
<p>Let’s look at your implementations for forward propagation and backward propagation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_n</span><span class="params">(X, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation (and computes the cost) presented in Figure 3.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- training set for m examples</span></span><br><span class="line"><span class="string">    Y -- labels for m examples </span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (5, 4)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (5, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 5)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- the cost function (logistic cost for one example)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Cost</span></span><br><span class="line">    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(<span class="number">1</span> - A3), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">1.</span>/m * np.sum(logprobs)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, cache</span><br></pre></td></tr></table></figure>

<p>Now, run backward propagation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_n</span><span class="params">(X, Y, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in figure 2.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    Y -- true "label"</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_n()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line"></span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment"># 这里是故意使用一个错误的形式来验证gradient_check是否正常工作</span></span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) * <span class="number">2</span></span><br><span class="line">    <span class="comment"># 正确的形式，最后再修改的</span></span><br><span class="line">    <span class="comment"># dW2 = 1./m * np.dot(dZ2, A1.T)</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    <span class="comment"># 这里是故意使用一个错误的形式来验证gradient_check是否正常工作</span></span><br><span class="line">    db1 = <span class="number">4.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 正确的形式，最后再修改的</span></span><br><span class="line">    <span class="comment"># db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)</span></span><br><span class="line"></span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,</span><br><span class="line">                 <span class="string">"dA2"</span>: dA2, <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2,</span><br><span class="line">                 <span class="string">"dA1"</span>: dA1, <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>

<p>You obtained some results on the fraud detection test set but you are not 100% sure of your model. Nobody’s perfect! Let’s implement gradient checking to verify if your gradients are correct.</p>
<p><strong>How does gradient checking work?</strong></p>
<p>As in 1) and 2), you want to compare “<code>gradapprox</code>” to the gradient computed by backpropagation. The formula is still:<br>$$\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}$$</p>
<p>However, $θ$ is not a scalar anymore. It is a dictionary called “<code>parameters</code>”. We implemented a function “<code>dictionary_to_vector()</code>” for you. It converts the “<code>parameters</code>” dictionary into a vector called “<code>values</code>”, obtained by reshaping all parameters <code>(W1, b1, W2, b2, W3, b3)</code> into vectors and concatenating them.</p>
<p>The inverse function is “<code>vector_to_dictionary</code>” which outputs back the “<code>parameters</code>” dictionary.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/deep-neural-network/week1/GradientChecking/3.png" alt="Figure 2 : `dictionary_to_vector()` and `vector_to_dictionary()`"></p>
<p>$$\text{Figure 2 : dictionary_to_vector() and vector_to_dictionary()} \ \text{ You will need these functions in gradient_check_n()}$$</p>
<p>We have also converted the “gradients” dictionary into a vector “grad” using <code>gradients_to_vector()</code>. You don’t need to worry about that.</p>
<p><strong>Exercise</strong>: Implement<code>gradient_check_n()</code>.</p>
<p><strong>Instructions</strong>: Here is pseudo-code that will help you implement the gradient check.</p>
<p>For each i in num_parameters: </p>
<ul>
<li>To compute <code>J_plus[i]</code>: <ol>
<li>Set $θ^+$ to <code>np.copy(parameters_values)</code> </li>
<li>Set $θ^+_i$ to $θ^+_i+ε$ </li>
<li>Calculate $J^+_i$ using to <code>forward_propagation_n(x, y, vector_to_dictionary(theta_plus))</code>. </li>
</ol>
</li>
<li>To compute <code>J_minus[i]</code>: do the same thing with $θ^−$ </li>
<li>Compute <code>gradapprox[i]</code>=$\frac{J^+_i−J^-_i}{2ε}$</li>
</ul>
<p>Thus, you get a vector gradapprox, where <code>gradapprox[i]</code> is an approximation of the gradient with respect to <code>parameter_values[i]</code>. You can now compare this gradapprox vector to the gradients vector from backpropagation. Just like for the 1D case (Steps 1’, 2’, 3’), compute:<br>$$difference = \frac {| grad - gradapprox |_2}{| grad |_2 + | gradapprox |_2 } \tag{2}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: gradient_check_n</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. </span></span><br><span class="line"><span class="string">    x -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    y -- true "label"</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set-up variables</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute gradapprox</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".</span></span><br><span class="line">        <span class="comment"># "_" is used because the function you have to outputs two parameters but we only care about the first one</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaplus = np.copy(parameters_values)                                      <span class="comment"># Step 1</span></span><br><span class="line">        thetaplus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] + epsilon                                <span class="comment"># Step 2</span></span><br><span class="line">        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                   <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaminus = np.copy(parameters_values)                                     <span class="comment"># Step 1</span></span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaminus[i][<span class="number">0</span>] - epsilon                               <span class="comment"># Step 2        </span></span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                  <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute gradapprox[i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">        gradapprox[i] = (J_plus[i] - J_minus[i]) / (<span class="number">2</span> * epsilon)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compare gradapprox to backward propagation gradients by computing difference.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line"><span class="comment">#     print("grad: &#123;&#125;".format(grad))</span></span><br><span class="line"><span class="comment">#     print("gradapprox: &#123;&#125;".format(gradapprox))</span></span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox, ord=<span class="number">2</span>)                                           <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(grad, ord=<span class="number">2</span>) + np.linalg.norm(gradapprox, ord=<span class="number">2</span>)                                         <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                                        <span class="comment"># Step 3'</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">2e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, Y, parameters = gradient_check_n_test_case()</span><br><span class="line"></span><br><span class="line">cost, cache = forward_propagation_n(X, Y, parameters)</span><br><span class="line">gradients = backward_propagation_n(X, Y, cache)</span><br><span class="line">difference = gradient_check_n(parameters, gradients, X, Y)</span><br></pre></td></tr></table></figure>

<pre><code>[93mThere is a mistake in the backward propagation! difference = 0.28509315678069896[0m</code></pre><p>It seems that there were errors in the <code>backward_propagation_n</code> code we gave you! Good that you’ve implemented the gradient check. Go back to backward_propagation and try to find/correct the errors (Hint: check dW2 and db1). Return the gradient check when you think you’ve fixed it. Remember you’ll need to re-execute the cell defining <code>backward_propagation_n()</code> if you modify the code.</p>
<p>Can you get gradient check to declare your derivative computation correct? Even though this part of the assignment isn’t graded, we strongly urge you to try to find the bug and re-run gradient check until you’re convinced backprop is now correctly implemented.</p>
<p><strong>Note</strong></p>
<ul>
<li>Gradient Checking is slow! Approximating the gradient with $\frac{∂J}{∂θ}≈\frac{J(θ+ε)−J(θ−ε)}{2ε}$ is computationally costly. For this reason, we don’t run gradient checking at every iteration during training. Just a few times to check if the gradient is correct. </li>
<li>Gradient Checking, at least as we’ve presented it, doesn’t work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout.</li>
</ul>
<p>Congrats, you can be confident that your deep learning model for fraud detection is working correctly! You can even use this to convince your CEO. :)</p>
<p>*<em>What you should remember from this notebook: *</em></p>
<ul>
<li>Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation). </li>
<li>Gradient checking is slow, so we don’t run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/08/summary_of_neural-networks-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/08/summary_of_neural-networks-deep-learning/" class="post-title-link" itemprop="url">summary of neural-networks-deep-learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-08 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-08T00:00:00+05:30">2018-02-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:10" itemprop="dateModified" datetime="2020-04-06T20:25:10+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal summary after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a>, which belongs to <a href="https://www.coursera.org/specializations/deep-learning" target="_blank" rel="noopener">Deep Learning Specialization</a>. and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="My-personal-notes"><a href="#My-personal-notes" class="headerlink" title="My personal notes"></a>My personal notes</h2><p>$1_{st}$ week: <a href="/2018/02/01/01_introduction-to-deep-learning/">introduction-to-deep-learning</a></p>
<ul>
<li><a href="/2018-04-02/01_introduction-to-deep-learning/#01_introduction-to-deep-learning">01_introduction-to-deep-learning</a><ul>
<li>[01_What is neural network?](/2018-04-02/01_introduction-to-deep-learning/#01_What is neural network?)<ul>
<li>[Example 1 – single neural network](/2018-04-02//01_introduction-to-deep-learning/#Example 1 – single neural network)</li>
<li>[Example 2 – Multiple neural network](/2018-04-02//01_introduction-to-deep-learning/#Example 2 – Multiple neural network)</li>
</ul>
</li>
</ul>
</li>
<li><a href="/2018-04-02/01_introduction-to-deep-learning/#02_supervised-learning-with-neural-networks">02_supervised-learning-with-neural-networks</a><ul>
<li>[Supervised learning for Neural Network](/2018-04-02/01_introduction-to-deep-learning/#Supervised learning for Neural Network)</li>
<li>[Structured vs unstructured data](/2018-04-02/01_introduction-to-deep-learning/#Structured vs unstructured data)</li>
</ul>
</li>
<li><a href="/2018-04-02/01_introduction-to-deep-learning/#03_why-is-deep-learning-taking-off">03_why-is-deep-learning-taking-off</a><ul>
<li>[Why is deep learning taking off?](/2018-04-02/01_introduction-to-deep-learning/#Why is deep learning taking off?)</li>
</ul>
</li>
<li><a href="/2018-04-02/01_introduction-to-deep-learning/#04_about-this-course">04_about-this-course</a><ul>
<li>[Outline of this Course](/2018-04-02/01_introduction-to-deep-learning/#Outline of this Course)</li>
</ul>
</li>
</ul>
<p>$2_{nd}$ week: <a href="/2018/02/02/02_neural-networks-basics/">neural-networks-basics</a></p>
<ul>
<li><a href="/2018/02/02/02_neural-networks-basics/#01_logistic-regression-as-a-neural-network">01_logistic-regression-as-a-neural-network</a><ul>
<li><a href="/2018/02/02/02_neural-networks-basics/#01_binary-classification">01_binary-classification</a><ul>
<li>[Binary Classification](/2018/02/02/02_neural-networks-basics/#Binary Classification)</li>
<li><a href="/2018/02/02/02_neural-networks-basics/#notation">notation</a></li>
</ul>
</li>
<li>[02_Logistic Regression](/2018/02/02/02_neural-networks-basics/#02_Logistic Regression)<ul>
<li>[Example: Cat vs No - cat](/2018/02/02/02_neural-networks-basics/#Example: Cat vs No - cat)</li>
<li><a href="/2018/02/02/02_neural-networks-basics/#notation">notation</a></li>
</ul>
</li>
<li><a href="/2018/02/02/02_neural-networks-basics/#03_logistic-regression-cost-function">03_logistic-regression-cost-function</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#04_gradient-descent">04_gradient-descent</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#05_06_derivatives">05_06_derivatives</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#07_computation-graph">07_computation-graph</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#09_logistic-regression-gradient-descent">09_logistic-regression-gradient-descent</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#10_gradient-descent-on-m-examples">10_gradient-descent-on-m-examples</a><ul>
<li>[one single step gradient descent](/2018/02/02/02_neural-networks-basics/#one single step gradient descent)</li>
</ul>
</li>
</ul>
</li>
<li><a href="/2018/02/02/02_neural-networks-basics/#02_python-and-vectorization">02_python-and-vectorization</a><ul>
<li><a href="/2018/02/02/02_neural-networks-basics/#01_vectorization">01_vectorization</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#02_more-vectorization-examples">02_more-vectorization-examples</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#03_vectorizing-logistic-regression">03_vectorizing-logistic-regression</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#04_vectorizing-logistic-regressions-gradient-output">04_vectorizing-logistic-regressions-gradient-output</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#05_broadcasting-in-python">05_broadcasting-in-python</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#06_a-note-on-python-numpy-vectors">06_a-note-on-python-numpy-vectors</a><ul>
<li>[one rank array](/2018/02/02/02_neural-networks-basics/#one rank array)</li>
<li>[practical tips](/2018/02/02/02_neural-networks-basics/#practical tips)</li>
</ul>
</li>
<li><a href="/2018/02/02/02_neural-networks-basics/#07_quick-tour-of-jupyter-ipython-notebooks">07_quick-tour-of-jupyter-ipython-notebooks</a></li>
<li><a href="/2018/02/02/02_neural-networks-basics/#08_explanation-of-logistic-regression-cost-function-optional">08_explanation-of-logistic-regression-cost-function-optional</a></li>
</ul>
</li>
</ul>
<p>$3_{rd}$ week: <a href="/2018/02/03/03_shallow-neural-networks/">shallow-neural-networks</a></p>
<ul>
<li><a href="/2018/02/03/03_shallow-neural-networks/#01_neural-networks-overview">01_neural-networks-overview</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#02_neural-network-representation">02_neural-network-representation</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#03_computing-a-neural-networks-output">03_computing-a-neural-networks-output</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#04_vectorizing-across-multiple-examples">04_vectorizing-across-multiple-examples</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#05_explanation-for-vectorized-implementation">05_explanation-for-vectorized-implementation</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#06_activation-functions">06_activation-functions</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#07_why-do-you-need-non-linear-activation-functions">07_why-do-you-need-non-linear-activation-functions</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#08_derivatives-of-activation-functions">08_derivatives-of-activation-functions</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#09_gradient-descent-for-neural-networks">09_gradient-descent-for-neural-networks</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#10_backpropagation-intuition-optional">10_backpropagation-intuition-optional</a></li>
<li><a href="/2018/02/03/03_shallow-neural-networks/#11_random-initialization">11_random-initialization</a></li>
</ul>
<p>$4_{th}$ week: <a href="/2018/02/04/04_deep-neural-networks/">deep-neural-networks</a></p>
<ul>
<li><a href="/2018/02/04/04_deep-neural-networks/#01_deep-neural-network">01_deep-neural-network</a></li>
<li><a href="/2018/02/04/04_deep-neural-networks/#02_forward-propagation-in-a-deep-network">02_forward-propagation-in-a-deep-network</a></li>
<li><a href="/2018/02/04/04_deep-neural-networks/#03_getting-your-matrix-dimensions-right">03_getting-your-matrix-dimensions-right</a><ul>
<li>[one training example](/2018/02/04/04_deep-neural-networks/#one training example)</li>
<li>[m training examples](/2018/02/04/04_deep-neural-networks/#m training examples)</li>
</ul>
</li>
<li><a href="/2018/02/04/04_deep-neural-networks/#04_why-deep-representations">04_why-deep-representations</a></li>
<li><a href="/2018/02/04/04_deep-neural-networks/#05_building-blocks-of-deep-neural-networks">05_building-blocks-of-deep-neural-networks</a></li>
<li><a href="/2018/02/04/04_deep-neural-networks/#06_forward-and-backward-propagation">06_forward-and-backward-propagation</a></li>
<li><a href="/2018/02/04/04_deep-neural-networks/#07_parameters-vs-hyperparameters">07_parameters-vs-hyperparameters</a></li>
<li><a href="/2018/02/04/04_deep-neural-networks/#08_what-does-this-have-to-do-with-the-brain">08_what-does-this-have-to-do-with-the-brain</a></li>
</ul>
<h2 id="My-personal-programming-assignments"><a href="#My-personal-programming-assignments" class="headerlink" title="My personal programming assignments"></a>My personal programming assignments</h2><p>week 1 and week 2: <a href="/2018/02/05/logistic-regression-with-a-neural-network-mindset_week1_and_week2/">logistic-regression-with-a-neural-network-mindset</a><br>week 3: <a href="/2018/02/06/planar-data-classification-with-one-hidden%20layer_week3/">Planar data classification with a hidden layer</a><br>week 4 part 1: <a href="/2018/02/07/Building-your-Deep-Neural-Network_week4/">Building your deep neural network: Step by Step</a><br>week 4 part 2: <a href="/2018/02/07/Building-your-Deep-Neural-Network_week4/#Part-2%EF%BC%9ADeep-Neural-Network-for-Image-Classification-Application">deep-neural-network-application</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/07/Building-your-Deep-Neural-Network_week4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/07/Building-your-Deep-Neural-Network_week4/" class="post-title-link" itemprop="url">Building your Deep Neural Network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-07 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-07T00:00:00+05:30">2018-02-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:31" itemprop="dateModified" datetime="2020-04-06T20:25:31+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>These are my personal programming assignments at the 4th week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h1 id="Part-1：Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Part-1：Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Part 1：Building your Deep Neural Network: Step by Step"></a>Part 1：Building your Deep Neural Network: Step by Step</h1><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1. Packages"></a>1. Packages</h2><p>Let’s first import all the packages that you will need during this assignment. </p>
<ul>
<li>numpy is the main package for scientific computing with Python. </li>
<li>matplotlib is a library to plot graphs in Python. </li>
<li><code>dnn_utils</code> provides some necessary functions for this notebook. </li>
<li><code>testCases</code> provides some test cases to assess the correctness of your functions </li>
<li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line"><span class="keyword">import</span> h5py;</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt;</span><br><span class="line"><span class="keyword">from</span> testCases_v3 <span class="keyword">import</span> *;</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward;</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>); <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span>;</span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span>;</span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<p>You can get the support code from here.</p>
<p>the <code>sigmoid</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(Z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the sigmoid activation in numpy</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- numpy array of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of sigmoid(z), same shape as Z</span></span><br><span class="line"><span class="string">    cache -- returns Z as well, useful during backpropagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    A = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-Z));</span><br><span class="line">    cache = Z;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache;</span><br></pre></td></tr></table></figure>

<p>the <code>sigmoid_backward</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single SIGMOID unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    Z = cache;</span><br><span class="line"></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-Z));</span><br><span class="line">    dZ = dA * s * (<span class="number">1</span> - s);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dZ;</span><br></pre></td></tr></table></figure>

<p>the <code>relu</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(Z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the RELU function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z -- Output of the linear layer, of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- Post-activation parameter, of the same shape as Z</span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "A" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    A = np.maximum(<span class="number">0</span>,Z);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == Z.shape);</span><br><span class="line"></span><br><span class="line">    cache = Z; </span><br><span class="line">    <span class="keyword">return</span> A, cache;</span><br></pre></td></tr></table></figure>

<p>the <code>relu_backward</code> function：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dA, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a single RELU unit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient, of any shape</span></span><br><span class="line"><span class="string">    cache -- 'Z' where we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to Z</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    Z = cache;</span><br><span class="line">    dZ = np.array(dA, copy = <span class="literal">True</span>); <span class="comment"># just converting dz to a correct object.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># When z &lt;= 0, you should set dz to 0 as well. </span></span><br><span class="line">    dZ[Z &lt;= <span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (dZ.shape == Z.shape);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dZ;</span><br></pre></td></tr></table></figure>

<h2 id="2-Outline-of-the-Assignment"><a href="#2-Outline-of-the-Assignment" class="headerlink" title="2. Outline of the Assignment"></a>2. Outline of the Assignment</h2><p>To build your neural network, you will be implementing several “helper functions”. These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of this assignment, you will:</p>
<ul>
<li><p>Initialize the parameters for a two-layer network and for an L-layer neural network.</p>
</li>
<li><p>Implement the forward propagation module (shown in purple in the figure below). </p>
<ul>
<li>Complete the LINEAR part of a layer’s forward propagation step (resulting in Z[l]).</li>
<li>We give you the ACTIVATION function (relu/sigmoid).</li>
<li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</li>
<li>Stack the [LINEAR-&gt;RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.</li>
</ul>
</li>
<li><p>Compute the loss.</p>
</li>
<li><p>Implement the backward propagation module (denoted in red in the figure below). </p>
<ul>
<li>Complete the LINEAR part of a layer’s backward propagation step.</li>
<li>We give you the gradient of the ACTIVATE function (relu_backward/sigmoid_backward)</li>
<li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function.</li>
<li>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</li>
</ul>
</li>
<li><p>Finally update the parameters.</p>
</li>
</ul>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/1.png" alt=""></p>
<p><strong>Note</strong> that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps.</p>
<h2 id="3-Initialization"><a href="#3-Initialization" class="headerlink" title="3. Initialization"></a>3. Initialization</h2><p>You will write two helper functions that will initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one will generalize this initialization process to L layers.</p>
<h3 id="3-1-2-layer-Neural-Network"><a href="#3-1-2-layer-Neural-Network" class="headerlink" title="3.1 2-layer Neural Network"></a>3.1 2-layer Neural Network</h3><p><strong>Exercise</strong>: Create and initialize the parameters of the 2-layer neural network.</p>
<p><strong>Instructions</strong>: </p>
<ul>
<li>The model’s structure is: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. </li>
<li>Use random initialization for the weight matrices. Use <code>np.random.randn(shape)*0.01</code> with the correct shape. </li>
<li>Use zero initialization for the biases. Use <code>np.zeros(shape)</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span>;</span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>));</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span>;</span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>));</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x));</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>));</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h));</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>);</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]));</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]));</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]));</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 0.01624345 -0.00611756 -0.00528172]
 [-0.01072969  0.00865408 -0.02301539]]
b1 = [[0.]
 [0.]]
W2 = [[ 0.01744812 -0.00761207]]
b2 = [[0.]]</code></pre><h3 id="3-2-L-layer-Neural-Network"><a href="#3-2-L-layer-Neural-Network" class="headerlink" title="3.2 L-layer Neural Network"></a>3.2 L-layer Neural Network</h3><p>The initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the <code>initialize_parameters_deep</code>, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units in layer $l$. Thus for example if the size of our input $X$ is $(12288,209)$ (with $m=209$ examples) then:</p>
<p>| |Shape of W|Shape of b|Activation|Shape of Activation|<br>|————–|—————-|—————|———————–|<br>|Layer 1|(n[1],12288)|(n[1],1)|Z[1]=W[1]X+b[1]|(n[1],209)|<br>|Layer 2|(n[2],n[1]) |(n[2],1)|Z[2]=W[2]A[1]+b[2]|(n[2],209)|<br>|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|<br>|Layer L-1|(n[L−1],n[L−2])|(n[L−1],1)|Z[L−1]=W[L−1]A[L−2]+b[L−1]|(n[L−1],209)|<br>|Layer L|(n[L],n[L−1])|(n[L],1)|Z[L]=W[L]A[L−1]+b[L]|(n[L],209)|</p>
<p>Remember that when we compute $WX+b$ in python, it carries out <strong>broadcasting</strong>. For example, if:</p>

$$W = \begin{bmatrix}
    j  & k  & l\\
    m  & n & o \\
    p  & q & r 
\end{bmatrix}\;\;\; X = \begin{bmatrix}
    a  & b  & c\\
    d  & e & f \\
    g  & h & i 
\end{bmatrix} \;\;\; b =\begin{bmatrix}
    s  \\
    t  \\
    u
\end{bmatrix}\tag{1}$$


<p>Then $WX+b$ will be:</p>

$$WX + b = \begin{bmatrix}
    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\
    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\
    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u
\end{bmatrix}\tag{2}$$


<p><strong>Exercise</strong>: Implement initialization for an L-layer Neural Network.</p>
<p><strong>Instructions</strong>: </p>
<ul>
<li><p>The model’s structure is [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID. I.e., it has L−1 layers using a ReLU activation function followed by an output layer with a sigmoid activation function. </p>
</li>
<li><p>Use random initialization for the weight matrices. Use <code>np.random.rand(shape) * 0.01</code>. </p>
</li>
<li><p>Use zeros initialization for the biases. Use <code>np.zeros(shape)</code>. </p>
</li>
<li><p>We will store $n^{[l]}$, the number of units in different layers, in a variable <code>layer_dims</code>. For example, the <code>layer_dims</code> for the “Planar Data classification model” from last week would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. Thus means <code>W1</code>’s shape was (4,2), <code>b1</code> was (4,1), <code>W2</code> was (1,4) and <code>b2</code> was (1,1). Now you will generalize this to L layers! </p>
</li>
<li><p>Here is the implementation for L=1 (one layer neural network). It should inspire you to implement the general case (L-layer neural network).</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> L == <span class="number">1</span>:</span><br><span class="line">parameters[<span class="string">"W"</span> + str(L)] = np.random.randn(layer_dims[<span class="number">1</span>], layer_dims[<span class="number">0</span>]) * <span class="number">0.01</span>;</span><br><span class="line">parameters[<span class="string">"b"</span> + str(L)] = np.zeros((layer_dims[<span class="number">1</span>], <span class="number">1</span>));</span><br></pre></td></tr></table></figure>


</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters_deep</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">3</span>);</span><br><span class="line">    parameters = &#123;&#125;;</span><br><span class="line">    L = len(layer_dims);     <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - <span class="number">1</span>]) * <span class="number">0.01</span>;</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>));</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]));</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = initialize_parameters_deep([<span class="number">5</span>,<span class="number">4</span>,<span class="number">3</span>]);</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]));</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]));</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]));</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]
 [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]
 [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]
 [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]
 [-0.01023785 -0.00712993  0.00625245 -0.00160513]
 [-0.00768836 -0.00230031  0.00745056  0.01976111]]
b2 = [[0.]
 [0.]
 [0.]]</code></pre><h2 id="4-Forward-propagation-module"><a href="#4-Forward-propagation-module" class="headerlink" title="4 Forward propagation module"></a>4 Forward propagation module</h2><h3 id="4-1-Linear-Forward"><a href="#4-1-Linear-Forward" class="headerlink" title="4.1 Linear Forward"></a>4.1 Linear Forward</h3><p>Now that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order:</p>
<ul>
<li>LINEAR</li>
<li>LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.</li>
<li>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID (whole model)</li>
</ul>
<p>The linear forward module (vectorized over all the examples) computes the following equations:<br>$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{3}$$</p>
<p>where $A^{[0]}=X$.</p>
<p><strong>Exercise</strong>: Build the linear part of forward propagation.</p>
<p><strong>Reminder</strong>:<br>The mathematical representation of this unit is $Z^{[l]}=W^{[l]}A^{[l−1]}+b^{[l]}$. You may also find <code>np.dot()</code> useful. If your dimensions don’t match, printing <code>W.shape</code> may help.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    Z = np.dot(W, A) + b;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]));</span><br><span class="line">    cache = (A, W, b);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z, cache;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A, W, b = linear_forward_test_case();</span><br><span class="line">Z, linear_cache = linear_forward(A, W, b);</span><br><span class="line">print(<span class="string">"Z = "</span> + str(Z));</span><br></pre></td></tr></table></figure>

<pre><code>Z = [[ 3.26295337 -1.23429987]]</code></pre><p>linear_forward_test_case:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward_test_case</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>);</span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>);</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>);</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> A, W, b;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-Linear-Activation-Forward"><a href="#4-2-Linear-Activation-Forward" class="headerlink" title="4.2 Linear-Activation Forward"></a>4.2 Linear-Activation Forward</h3><p>In this notebook, you will use two activation functions:</p>
<ul>
<li><strong>Sigmoid</strong>: $\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)} }$ We have provided you with the <code>sigmoid</code> function. This function returns two items: the activation value “<code>a</code>” and a “<code>cache</code>” that contains “<code>Z</code>” (it’s what we will feed in to the corresponding backward function). To use it you could just call:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = sigmoid(Z);</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>ReLU</strong>: The mathematical formula for ReLu is <code>A=RELU(Z)=max(0,Z)</code>. We have provided you with the relu function. This function returns two items: the activation value “<code>A</code>” and a “<code>cache</code>” that contains “<code>Z</code>” (it’s what we will feed in to the corresponding backward function). To use it you could just call:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A, activation_cache = relu(Z);</span><br></pre></td></tr></table></figure>

<p>For more convenience, you are going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step.</p>
<p><strong>Exercise</strong>: Implement the forward propagation of the LINEAR-&gt;ACTIVATION layer. Mathematical relation is:<br>$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation “<code>g</code>” can be <code>sigmoid()</code> or <code>relu()</code>. Use <code>linear_forward()</code> and the correct activation function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_activation_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b); <span class="comment"># Z, (W, A_prev, B)</span></span><br><span class="line">        A, activation_cache = sigmoid(Z); <span class="comment"># A, (Z)</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b); </span><br><span class="line">        A, activation_cache = relu(Z);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]));</span><br><span class="line">    cache = (linear_cache, activation_cache); <span class="comment">#, ((W, A_prev, B) ,(Z))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A_prev, W, b = linear_activation_forward_test_case();</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"sigmoid"</span>);</span><br><span class="line">print(<span class="string">"With sigmoid: A = "</span> + str(A));</span><br><span class="line"></span><br><span class="line">A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = <span class="string">"relu"</span>);</span><br><span class="line">print(<span class="string">"With ReLU: A = "</span> + str(A));</span><br></pre></td></tr></table></figure>

<pre><code>With sigmoid: A = [[0.96890023 0.11013289]]
With ReLU: A = [[3.43896131 0.        ]]</code></pre><p>linear_activation_forward_test_case function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward_test_case</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    A_prev = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> A_prev, W, b</span><br></pre></td></tr></table></figure>

<p><strong>Note</strong>: In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers.</p>
<h3 id="4-3-L-Layer-Model"><a href="#4-3-L-Layer-Model" class="headerlink" title="4.3 L-Layer Model"></a>4.3 L-Layer Model</h3><p>For even more convenience when implementing the L-layer Neural Net, you will need a function that replicates the previous one (<code>linear_activation_forward</code> with RELU) L−1 times, then follows that with one <code>linear_activation_forward</code> with SIGMOID.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/2.png" alt=""></p>
<p><strong>Exercise</strong>: Implement the forward propagation of the above model.</p>
<p><strong>Instruction</strong>: In the code below, the variable <code>AL</code> will denote $A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is sometimes also called <code>Yhat</code>, i.e., this is $\hat{Y}$.)</p>
<p><strong>Tips</strong>: </p>
<ul>
<li>Use the functions you had previously written </li>
<li>Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times </li>
<li>Don’t forget to keep track of the caches in the “<code>caches</code>” list. To add a new value <code>c</code> to a list, you can use <code>list.append(c)</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L_model_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters) // <span class="number">2</span>                  <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A, linear_activation_cache = linear_activation_forward(A_prev, parameters[<span class="string">"W"</span> + str(l)], parameters[<span class="string">"b"</span> + str(l)], <span class="string">"relu"</span>);</span><br><span class="line">        caches.append(linear_activation_cache);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    AL, linear_activation_cache = linear_activation_forward(A, parameters[<span class="string">"W"</span> + str(L)], parameters[<span class="string">"b"</span> + str(L)], <span class="string">"sigmoid"</span>);</span><br><span class="line">    caches.append(linear_activation_cache);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]));</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> AL, caches;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X, parameters = L_model_forward_test_case_2hidden();</span><br><span class="line">AL, caches = L_model_forward(X, parameters);</span><br><span class="line">print(<span class="string">"AL = "</span> + str(AL));</span><br><span class="line">print(<span class="string">"Length of caches list = "</span> + str(len(caches)));</span><br></pre></td></tr></table></figure>

<pre><code>AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]
Length of caches list = 3</code></pre><p>L_model_forward_test_case function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward_test_case</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>);</span><br><span class="line">    X = np.random.randn(<span class="number">4</span>,<span class="number">2</span>);</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>);</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>);</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>);</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X, parameters;</span><br></pre></td></tr></table></figure>

<p>Great! Now you have a full forward propagation that takes the input $X$ and outputs a row vector $A^{[L]}$ containing your predictions. It also records all intermediate values in “<code>caches</code>”. Using $A^{[L]}$, you can compute the cost of your predictions.</p>
<h2 id="5-Cost-function"><a href="#5-Cost-function" class="headerlink" title="5. Cost function"></a>5. Cost function</h2><p>Now you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning.</p>
<p><strong>Exercise</strong>: Compute the cross-entropy cost $J$, using the following formula:<br>$$-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{<a href="i">L</a>}\right)) \tag{4}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from aL and y.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 lines of code)</span></span><br><span class="line">    cost = <span class="number">-1</span> / m * (np.dot(Y, np.log(AL).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - AL).T));</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost);      <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="comment">#assert(isinstance(cost, float));</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ());</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y, AL = compute_cost_test_case();</span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(AL, Y)));</span><br></pre></td></tr></table></figure>

<pre><code>cost = 0.41493159961539694</code></pre><p>compute_cost_test_case function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_test_case</span><span class="params">()</span>:</span> </span><br><span class="line">    Y = np.asarray([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]);</span><br><span class="line">    aL = np.array([[<span class="number">.8</span>,<span class="number">.9</span>,<span class="number">0.4</span>]]); </span><br><span class="line">    <span class="keyword">return</span> Y, aL;</span><br></pre></td></tr></table></figure>

<h2 id="6-Backward-propagation-module"><a href="#6-Backward-propagation-module" class="headerlink" title="6. Backward propagation module"></a>6. Backward propagation module</h2><p>Just like with forward propagation, you will implement helper functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters.</p>
<p><strong>Reminder</strong>: </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/3.png" alt=""></p>
<p><strong>Figure 3</strong> : Forward and Backward propagation for LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID </p>
<p>The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.</p>
{% raw %}
$$\frac{d \mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \frac{d\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\frac{{da^{[2]}}}{{dz^{[2]}}}\frac{{dz^{[2]}}}{{da^{[1]}}}\frac{{da^{[1]}}}{{dz^{[1]}}} \tag{5}$$
{% endraw %}

<p>In order to calculate the gradient $dW^{[1]} = \frac{\partial L}{\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial W^{[1]}}$, . During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted. Equivalently, in order to calculate the gradient $db^{[1]} = \frac{\partial L}{\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial b^{[1]}}$. This is why we talk about <strong>backpropagation</strong>. </p>
<p>Now, similar to forward propagation, you are going to build the backward propagation in three steps: </p>
<ul>
<li>LINEAR backward </li>
<li>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation </li>
<li>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li>
</ul>
<h3 id="6-1-Linear-backward"><a href="#6-1-Linear-backward" class="headerlink" title="6.1 Linear backward"></a>6.1 Linear backward</h3><p>For layer l, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$, (followed by an activation).<br>Suppose you have already calculated the derivative $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/4.png" alt=""></p>
<p>The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$, are computed using the input $dZ^{[l]}$. Here are the formulas you need:<br>$$dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{5}$$</p>
<p>$$db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{<a href="i">l</a>}\tag{6}$$</p>
<p>$$dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{7}$$</p>
<p><strong>Exercise</strong>: Use the 3 formulas above to implement <code>linear_backward()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache;</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    dW = <span class="number">1</span> / m * np.dot(dZ, A_prev.T);</span><br><span class="line">    db = <span class="number">1</span> / m * np.sum(dZ, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line">    dA_prev = np.dot(W.T, dZ);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape);</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape);</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Set up some test inputs</span></span><br><span class="line">dZ, linear_cache = linear_backward_test_case();</span><br><span class="line">dA_prev, dW, db = linear_backward(dZ, linear_cache);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db));</span><br></pre></td></tr></table></figure>

<pre><code>dA_prev = [[ 0.51822968 -0.19517421]
 [-0.40506361  0.15255393]
 [ 2.37496825 -0.89445391]]
dW = [[-0.10076895  1.40685096  1.64992505]]
db = [[0.50629448]]</code></pre><p>linear_backward_test_case function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward_test_case</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>);</span><br><span class="line">    dZ = np.random.randn(<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>);</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>);</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">    linear_cache = (A, W, b);</span><br><span class="line">    <span class="keyword">return</span> dZ, linear_cache;</span><br></pre></td></tr></table></figure>

<h3 id="6-2-Linear-Activation-backward"><a href="#6-2-Linear-Activation-backward" class="headerlink" title="6.2 Linear-Activation backward"></a>6.2 Linear-Activation backward</h3><p>Next, you will create a function that merges the two helper functions: <code>linear_backward</code> and the backward step for the activation <code>linear_activation_backward</code>.</p>
<p>To help you implement <code>linear_activation_backward</code>, we provided two backward functions: </p>
<ul>
<li><code>sigmoid_backward</code>: Implements the backward propagation for SIGMOID unit. You can call it as follows:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = sigmoid_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>relu_backward</code>: Implements the backward propagation for RELU unit. You can call it as follows:</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dZ = relu_backward(dA, activation_cache)</span><br></pre></td></tr></table></figure>

<p>If g(.) is the activation function,<br><code>sigmoid_backward</code> and <code>relu_backward</code> compute:</p>
<p>$$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]}) \tag{8}$$</p>
<p><strong>Exercise</strong>: Implement the backpropagation for the LINEAR-&gt;ACTIVATION layer.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: linear_activation_backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dZ = relu_backward(dA, activation_cache);</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache);</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">AL, linear_activation_cache = linear_activation_backward_test_case();</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">"sigmoid"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid:"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db) + <span class="string">"\n"</span>);</span><br><span class="line"></span><br><span class="line">dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = <span class="string">"relu"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"relu:"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dA_prev = "</span>+ str(dA_prev));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW = "</span> + str(dW));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(db));</span><br></pre></td></tr></table></figure>

<pre><code>sigmoid:
dA_prev = [[ 0.11017994  0.01105339]
 [ 0.09466817  0.00949723]
 [-0.05743092 -0.00576154]]
dW = [[ 0.10266786  0.09778551 -0.01968084]]
db = [[-0.05729622]]

relu:
dA_prev = [[ 0.44090989  0.        ]
 [ 0.37883606  0.        ]
 [-0.2298228   0.        ]]
dW = [[ 0.44513824  0.37371418 -0.10478989]]
db = [[-0.20837892]]</code></pre><p><code>linear_activation_backward_test_case</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward_test_case</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>);</span><br><span class="line">    dA = np.random.randn(<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line">    A = np.random.randn(<span class="number">3</span>,<span class="number">2</span>);</span><br><span class="line">    W = np.random.randn(<span class="number">1</span>,<span class="number">3</span>);</span><br><span class="line">    b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">    Z = np.random.randn(<span class="number">1</span>,<span class="number">2</span>);</span><br><span class="line">    linear_cache = (A, W, b);</span><br><span class="line">    activation_cache = Z;</span><br><span class="line">    linear_activation_cache = (linear_cache, activation_cache);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA, linear_activation_cache;</span><br></pre></td></tr></table></figure>

<h3 id="6-3-L-Model-Backward"><a href="#6-3-L-Model-Backward" class="headerlink" title="6.3 L-Model Backward"></a>6.3 L-Model Backward</h3><p>Now you will implement the backward function for the whole network. Recall that when you implemented the <code>L_model_forward function</code>, at each iteration, you stored a cache which contains <strong>(X,W,b, and z)</strong>. In the back propagation module, you will use those variables to compute the gradients. Therefore, in the <code>L_model_backward</code> function, you will iterate through all the hidden layers backward, starting from layer L. On each step, you will use the cached values for layer l to backpropagate through layer l. Figure 5 below shows the backward pass.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/5.png" alt=""></p>
<p><strong>Initializing backpropagation</strong>: </p>
<p>To backpropagate through this network, we know that the output is, $A^{[L]} = \sigma(Z^{[L]})$ . Your code thus needs to compute $= \frac{\partial \mathcal{L}}{\partial A^{[L]}}$.<br>To do so, use this formula (derived using calculus which you don’t need in-depth knowledge of):</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL)) <span class="comment"># derivative of cost with respect to AL</span></span><br></pre></td></tr></table></figure>
<p>You can then use this post-activation gradient <code>dAL</code> to keep going backward. As seen in Figure 5, you can now feed in <code>dAL</code> into the LINEAR-&gt;SIGMOID backward function you implemented (which will use the cached values stored by the <code>L_model_forward</code> function). After that, you will have to use a for loop to iterate through all the other layers using the LINEAR-&gt;RELU backward function. You should store each <code>dA</code>, <code>dW</code>, and <code>db</code> in the grads dictionary. To do so, use this formula :<br>$$grads[“dW” + str(l)] = dW^{[l]}\tag{9}$$</p>
<p>For example, for <code>l=3</code> this would store $dW^{[l]}$ in <code>grads[&quot;dW3&quot;]</code>.</p>
<p><strong>Exercise</strong>: Implement backpropagation for the [LINEAR-&gt;RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L_model_backward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ... </span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ... </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;;</span><br><span class="line">    L = len(caches); <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>];</span><br><span class="line">    Y = Y.reshape(AL.shape); <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL));</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "AL, Y, caches". Outputs: "grads["dAL"], grads["dWL"], grads["dbL"]</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">    dA_prev, dW, db = linear_activation_backward(dAL, caches[L - <span class="number">1</span>], <span class="string">"sigmoid"</span>);</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = dA_prev, dW, db;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 2)], caches". Outputs: "grads["dA" + str(l + 1)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></span><br><span class="line">        dA = dA_prev;</span><br><span class="line">        dA_prev, dW, db = linear_activation_backward(dA, caches[l], <span class="string">"relu"</span>);</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)] = dA_prev;</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW;</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db;</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AL, Y_assess, caches = L_model_backward_test_case();</span><br><span class="line">grads = L_model_backward(AL, Y_assess, caches);</span><br><span class="line">print_grads(grads);</span><br></pre></td></tr></table></figure>

<pre><code>dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]
 [0.         0.         0.         0.        ]
 [0.05283652 0.01005865 0.01777766 0.0135308 ]]
db1 = [[-0.22007063]
 [ 0.        ]
 [-0.02835349]]
dA1 = [[ 0.12913162 -0.44014127]
 [-0.14175655  0.48317296]
 [ 0.01663708 -0.05670698]]</code></pre><p><code>L_model_backward_test_case</code> function in <code>testCases_v3.py</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward_test_case</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    X = np.random.rand(3,2)</span></span><br><span class="line"><span class="string">    Y = np.array([[1, 1]])</span></span><br><span class="line"><span class="string">    parameters = &#123;'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],</span></span><br><span class="line"><span class="string">           [ 0.02738759,  0.67046751],</span></span><br><span class="line"><span class="string">           [ 0.4173048 ,  0.55868983]]),</span></span><br><span class="line"><span class="string">    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),</span></span><br><span class="line"><span class="string">    np.array([[ 0.]])),</span></span><br><span class="line"><span class="string">   np.array([[ 0.41791293,  1.91720367]]))])</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    AL = np.random.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    Y = np.array([[<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    A1 = np.random.randn(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    Z1 = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache_activation_1 = ((A1, W1, b1), Z1)</span><br><span class="line"></span><br><span class="line">    A2 = np.random.randn(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    Z2 = np.random.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    linear_cache_activation_2 = ((A2, W2, b2), Z2)</span><br><span class="line"></span><br><span class="line">    caches = (linear_cache_activation_1, linear_cache_activation_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> AL, Y, caches</span><br></pre></td></tr></table></figure>

<h3 id="6-4-Update-Parameters"><a href="#6-4-Update-Parameters" class="headerlink" title="6.4 Update Parameters"></a>6.4 Update Parameters</h3><p>In this section you will update the parameters of the model, using gradient descent:<br>$$W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{10}$$</p>
<p>$$b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{11}$$</p>
<p>where $α$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary.</p>
<p><strong>Exercise</strong>: Implement update_parameters() to update your parameters using gradient descent.</p>
<p><strong>Instructions</strong>:<br>Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l=1,2,…,L$.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter. Use a for loop.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l + <span class="number">1</span>)] -= learning_rate * grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)];</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case();</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span>+ str(parameters[<span class="string">"W1"</span>]));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span>+ str(parameters[<span class="string">"b1"</span>]));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span>+ str(parameters[<span class="string">"W2"</span>]));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span>+ str(parameters[<span class="string">"b2"</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]]
b1 = [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]]
W2 = [[-0.55569196  0.0354055   1.32964895]]
b2 = [[-0.84610769]]</code></pre><p><code>update_parameters_test_case</code> function in <code>testCases_v3.py</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_test_case</span><span class="params">()</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    W1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    b1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    W2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    b2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    dW1 = np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">    db1 = np.random.randn(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">    dW2 = np.random.randn(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">    db2 = np.random.randn(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters, grads</span><br></pre></td></tr></table></figure>

<h2 id="7-Conclusion"><a href="#7-Conclusion" class="headerlink" title="7. Conclusion"></a>7. Conclusion</h2><p>Congrats on implementing all the functions required for building a deep neural network!</p>
<p>We know it was a long assignment but going forward it will only get better. The next part of the assignment is easier.</p>
<p>In the next assignment you will put all these together to build two models: </p>
<ul>
<li>A two-layer neural network </li>
<li>An L-layer neural network</li>
</ul>
<p>You will in fact use these models to classify cat vs non-cat images!</p>
<h1 id="Part-2：Deep-Neural-Network-for-Image-Classification-Application"><a href="#Part-2：Deep-Neural-Network-for-Image-Classification-Application" class="headerlink" title="Part 2：Deep Neural Network for Image Classification: Application"></a>Part 2：Deep Neural Network for Image Classification: Application</h1><h2 id="1-Packages-1"><a href="#1-Packages-1" class="headerlink" title="1. Packages"></a>1. Packages</h2><p>Let’s first import all the packages that you will need during this assignment. </p>
<ul>
<li>numpy is the fundamental package for scientific computing with Python. </li>
<li>matplotlib is a library to plot graphs in Python. </li>
<li>h5py is a common package to interact with a dataset that is stored on an H5 file. </li>
<li>PIL and scipy are used here to test your model with your own picture at the end. </li>
<li><code>dnn_app_utils</code> provides the functions implemented in the “Building your Deep Neural Network: Step by Step” assignment to this notebook. </li>
<li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload</code></pre><h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2. Dataset"></a>2. Dataset</h2><p>You will use the same “Cat vs non-Cat” dataset as in “Logistic Regression as a Neural Network” (Assignment 2). The model you had built had 70% test accuracy on classifying cats vs non-cats images. Hopefully, your new model will perform a better!</p>
<p>Problem Statement: You are given a dataset (“data.h5”) containing: </p>
<ul>
<li>a training set of m_train images labelled as cat (1) or non-cat (0) </li>
<li>a test set of m_test images labelled as cat and non-cat </li>
<li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB).</li>
</ul>
<p>Let’s get more familiar with the dataset. Load the data by running the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data();</span><br></pre></td></tr></table></figure>

<p>The following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to see other images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">10</span>;</span><br><span class="line">plt.imshow(train_x_orig[index]);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_y[<span class="number">0</span>,index]) + <span class="string">". It's a "</span> + classes[train_y[<span class="number">0</span>,index]].decode(<span class="string">"utf-8"</span>) +  <span class="string">" picture."</span>);</span><br></pre></td></tr></table></figure>

<pre><code>y = 0. It&apos;s a non-cat picture.</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/output_51_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Explore your dataset </span></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>];</span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>];</span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: "</span> + str(m_train));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: "</span> + str(m_test));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x_orig shape: "</span> + str(train_x_orig.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_y shape: "</span> + str(train_y.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x_orig shape: "</span> + str(test_x_orig.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_y shape: "</span> + str(test_y.shape));</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: 209
Number of testing examples: 50
Each image is of size: (64, 64, 3)
train_x_orig shape: (209, 64, 64, 3)
train_y shape: (1, 209)
test_x_orig shape: (50, 64, 64, 3)
test_y shape: (1, 50)</code></pre><p>As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/6.png" alt="Figure 1: Image to vector conversion"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test examples </span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T;   <span class="comment"># The "-1" makes reshape flatten the remaining dimensions</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten / <span class="number">255.</span>;</span><br><span class="line">test_x = test_x_flatten / <span class="number">255.</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape));</span><br></pre></td></tr></table></figure>

<pre><code>train_x&apos;s shape: (12288, 209)
test_x&apos;s shape: (12288, 50)</code></pre><p>$12288$ equals $64×64×3$ which is the size of one reshaped image vector.</p>
<h2 id="3-Architecture-of-your-model"><a href="#3-Architecture-of-your-model" class="headerlink" title="3. Architecture of your model"></a>3. Architecture of your model</h2><p>Now that you are familiar with the dataset, it is time to build a deep neural network to distinguish cat images from non-cat images.</p>
<p>You will build two different models: </p>
<ul>
<li>A 2-layer neural network </li>
<li>An L-layer deep neural network</li>
</ul>
<p>You will then compare the performance of these models, and also try out different values for L.</p>
<p>Let’s look at the two architectures.</p>
<h3 id="3-1-2-layer-neural-network"><a href="#3-1-2-layer-neural-network" class="headerlink" title="3.1 2-layer neural network"></a>3.1 2-layer neural network</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/7.png" alt="Figure 2: 2-layer neural network"><br>The model can be summarized as: <strong>INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT</strong></p>
<p>Detailed Architecture of figure 2: </p>
<ul>
<li>The input is a $(64,64,3)$ image which is flattened to a vector of size $(12288,1)$. </li>
<li>The corresponding vector: $[x_0,x_1,…,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]},12288)$. </li>
<li>You then add a bias term and take its relu to get the following vector: $[a^{[1]}<em>0,a^{[1]}_1,…,a^{[1]}</em>{n^{[1]}−1}]^T$. </li>
<li>You then repeat the same process. </li>
<li>You multiply the resulting vector by $W^{[2]}$ and add your intercept (bias). </li>
<li>Finally, you take the sigmoid of the result. If it is greater than $0.5$, you classify it to be a cat.</li>
</ul>
<h3 id="3-2-L-layer-deep-neural-network"><a href="#3-2-L-layer-deep-neural-network" class="headerlink" title="3.2 L-layer deep neural network"></a>3.2 L-layer deep neural network</h3><p>It is hard to represent an L-layer deep neural network with the above representation. However, here is a simplified network representation:<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/8.png" alt="Figure 3: L-layer neural network"><br>The model can be summarized as: <strong>[LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID</strong></p>
<p>Detailed Architecture of figure 3: </p>
<ul>
<li>The input is a $(64,64,3)$ image which is flattened to a vector of size $(12288,1)$. </li>
<li>The corresponding vector: $[x_0,x_1,…,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ and then you add the intercept $b^{[l]}$. The result is called the linear unit. </li>
<li>Next, you take the relu of the linear unit. This process could be repeated several times for each $(W^{[l]},b^{[l]})$ depending on the model architecture. </li>
<li>Finally, you take the sigmoid of the final linear unit. If it is greater than $0.5$, you classify it to be a cat.</li>
</ul>
<h3 id="3-3-General-methodology"><a href="#3-3-General-methodology" class="headerlink" title="3.3 General methodology"></a>3.3 General methodology</h3><p>As usual you will follow the Deep Learning methodology to build the model: </p>
<ol>
<li>Initialize parameters / Define hyperparameters </li>
<li>Loop for num_iterations: <ol>
<li>Forward propagation </li>
<li>Compute cost function </li>
<li>Backward propagation </li>
<li>Update parameters (using parameters, and grads from backprop) </li>
</ol>
</li>
<li>Use trained parameters to predict labels</li>
</ol>
<p>Let’s now implement those two models!</p>
<h2 id="4-Two-layer-neural-network"><a href="#4-Two-layer-neural-network" class="headerlink" title="4. Two-layer neural network"></a>4. Two-layer neural network</h2><p><strong>Question</strong>: Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following </p>
<p><strong>structure</strong>: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. The functions you may need and their inputs are:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### CONSTANTS DEFINING THE MODEL ####</span></span><br><span class="line">n_x = <span class="number">12288</span>;    <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span>;</span><br><span class="line">n_y = <span class="number">1</span>;</span><br><span class="line">layers_dims = (n_x, n_h, n_y);</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#GRADED FUNCTION: two_layer_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- If set to True, this will print the cost every 100 iterations </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>);</span><br><span class="line">    grads = &#123;&#125;;</span><br><span class="line">    costs = [];                              <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>];                           <span class="comment"># number of examples</span></span><br><span class="line">    (n_x, n_h, n_y) = layers_dims;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters dictionary, by calling one of the functions you'd previously implemented</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get W1, b1, W2 and b2 from the dictionary parameters.</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>];</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>];</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>];</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1". Output: "A1, cache1, A2, cache2".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">"relu"</span>);</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">"sigmoid"</span>);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        cost = compute_cost(A2, Y);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initializing backward propagation</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2));</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">"sigmoid"</span>);</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">"relu"</span>);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2</span></span><br><span class="line">        grads[<span class="string">'dW1'</span>] = dW1;</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1;</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2;</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 1 line of code)</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve W1, b1, W2, b2 from parameters</span></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>];</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>];</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>];</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)));</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost);</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs));</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>);</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>);</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate));</span><br><span class="line">    plt.show();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>

<p>Run the cell below to train your parameters. See if your model runs. The cost should be decreasing. It may take up to 5 minutes to run 2500 iterations. Check if the “Cost after iteration 0” matches the expected output below, if not click on the black square button on the upper bar of the notebook to stop the cell and try to find your error.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost = <span class="literal">True</span>);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.693049735659989
Cost after iteration 100: 0.6464320953428849
Cost after iteration 200: 0.6325140647912678
Cost after iteration 300: 0.6015024920354665
Cost after iteration 400: 0.5601966311605748
Cost after iteration 500: 0.5158304772764729
Cost after iteration 600: 0.4754901313943325
Cost after iteration 700: 0.43391631512257495
Cost after iteration 800: 0.4007977536203886
Cost after iteration 900: 0.35807050113237976
Cost after iteration 1000: 0.33942815383664127
Cost after iteration 1100: 0.3052753636196264
Cost after iteration 1200: 0.2749137728213016
Cost after iteration 1300: 0.2468176821061484
Cost after iteration 1400: 0.19850735037466102
Cost after iteration 1500: 0.1744831811255665
Cost after iteration 1600: 0.17080762978096942
Cost after iteration 1700: 0.11306524562164715
Cost after iteration 1800: 0.09629426845937152
Cost after iteration 1900: 0.0834261795972687
Cost after iteration 2000: 0.07439078704319087
Cost after iteration 2100: 0.06630748132267934
Cost after iteration 2200: 0.05919329501038172
Cost after iteration 2300: 0.053361403485605585
Cost after iteration 2400: 0.04855478562877019</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/output_59_1.png" alt="png"></p>
<p>Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this.</p>
<p>Now, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy: 0.9999999999999998</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions_test = predict(test_x, test_y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy: 0.72</code></pre><p>the <code>prediction</code> function:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X, y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function is used to predict the results of a  L-layer neural network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data set of examples you would like to label</span></span><br><span class="line"><span class="string">    parameters -- parameters of the trained model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    p -- predictions for the given dataset X</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    n = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural network</span></span><br><span class="line">    p = np.zeros((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    probas, caches = L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># convert probas to 0/1 predictions</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, probas.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> probas[<span class="number">0</span>,i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            p[<span class="number">0</span>,i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p[<span class="number">0</span>,i] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Accuracy: "</span>  + str(np.sum((p == y)/m)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p</span><br></pre></td></tr></table></figure>

<p><strong>Note</strong>: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and we will talk about it in the next course. Early stopping is a way to prevent overfitting.</p>
<p>Congratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, assignment week 2). Let’s see if you can do even better with an L-layer model.</p>
<h2 id="5-L-layer-Neural-Network"><a href="#5-L-layer-Neural-Network" class="headerlink" title="5. L-layer Neural Network"></a>5. L-layer Neural Network</h2><p><strong>Question</strong>: Use the helper functions you have implemented previously to build an L-layer neural network with the following structure: [LINEAR -&gt; RELU]×(L-1) -&gt; LINEAR -&gt; SIGMOID. The functions you may need and their inputs are:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> cost</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> grads</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### CONSTANTS ###</span></span><br><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  5-layer model</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L_layer_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span><span class="comment">#lr was 0.009</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []                         <span class="comment"># keep track of cost</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parameters initialization.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        AL, caches =L_model_forward(X, parameters);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute cost.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        cost = compute_cost(AL, Y);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        grads = L_model_backward(AL, Y, caches);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training example</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost));</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost);</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs));</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>);</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>);</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate));</span><br><span class="line">    plt.show();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters;</span><br></pre></td></tr></table></figure>

<p>You will now train the model as a 5-layer neural network.</p>
<p>Run the cell below to train your model. The cost should decrease on every iteration. It may take up to 5 minutes to run 2500 iterations. Check if the “Cost after iteration 0” matches the expected output below, if not click on the black square button on the upper bar of the notebook to stop the cell and try to find your error.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="literal">True</span>);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.771749
Cost after iteration 100: 0.672053
Cost after iteration 200: 0.648263
Cost after iteration 300: 0.611507
Cost after iteration 400: 0.567047
Cost after iteration 500: 0.540138
Cost after iteration 600: 0.527930
Cost after iteration 700: 0.465477
Cost after iteration 800: 0.369126
Cost after iteration 900: 0.391747
Cost after iteration 1000: 0.315187
Cost after iteration 1100: 0.272700
Cost after iteration 1200: 0.237419
Cost after iteration 1300: 0.199601
Cost after iteration 1400: 0.189263
Cost after iteration 1500: 0.161189
Cost after iteration 1600: 0.148214
Cost after iteration 1700: 0.137775
Cost after iteration 1800: 0.129740
Cost after iteration 1900: 0.121225
Cost after iteration 2000: 0.113821
Cost after iteration 2100: 0.107839
Cost after iteration 2200: 0.102855
Cost after iteration 2300: 0.100897
Cost after iteration 2400: 0.092878</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/output_67_1.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy: 0.9856459330143539</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_test = predict(test_x, test_y, parameters);</span><br></pre></td></tr></table></figure>

<pre><code>Accuracy: 0.8</code></pre><p>Congrats! It seems that your 5-layer neural network has better performance $(80%) $than your 2-layer neural network $(72%)$ on the same test set.</p>
<p>This is good performance for this task. Nice job!</p>
<p>Though in the next course on “Improving deep neural networks” you will learn how to obtain even higher accuracy by systematically searching for better hyperparameters (learning_rate, layers_dims, num_iterations, and others you’ll also learn in the next course).</p>
<h2 id="6-Results-Analysis"><a href="#6-Results-Analysis" class="headerlink" title="6. Results Analysis"></a>6. Results Analysis</h2><p>First, let’s take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, pred_test);</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/output_71_0.png" alt="png"></p>
<p>A few type of images the model tends to do poorly on include: </p>
<ul>
<li>Cat body in an unusual position </li>
<li>Cat appears against a background of a similar color </li>
<li>Unusual cat color and species </li>
<li>Camera Angle </li>
<li>Brightness of the picture </li>
<li>Scale variation (cat is very large or small in image)</li>
</ul>
<h2 id="7-Test-with-your-own-image-optional-ungraded-exercise"><a href="#7-Test-with-your-own-image-optional-ungraded-exercise" class="headerlink" title="7. Test with your own image (optional/ungraded exercise)"></a>7. Test with your own image (optional/ungraded exercise)</h2><p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: </p>
<ol>
<li>Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. </li>
<li>Add your image to this Jupyter Notebook’s directory, in the “images” folder </li>
<li>Change your image’s name in the following code </li>
<li>Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## START CODE HERE ##</span></span><br><span class="line">my_image = <span class="string">"1.png"</span>; <span class="comment"># change this to the name of your image file </span></span><br><span class="line">my_label_y = [<span class="number">1</span>]; <span class="comment"># the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span></span><br><span class="line"><span class="comment">## END CODE HERE ##</span></span><br><span class="line"></span><br><span class="line">fname = <span class="string">"images/"</span> + my_image;</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="literal">False</span>));</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px * num_px * <span class="number">3</span>,<span class="number">1</span>));</span><br><span class="line">my_predicted_image = predict(my_image, my_label_y, parameters);</span><br><span class="line"></span><br><span class="line">plt.imshow(image);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your L-layer model predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>);</span><br></pre></td></tr></table></figure>


<pre><code>Accuracy: 1.0
y = 1.0, your L-layer model predicts a &quot;cat&quot; picture.</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week4/output_73_2.png" alt="png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/05/logistic-regression-with-a-neural-network-mindset_week1_and_week2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/05/logistic-regression-with-a-neural-network-mindset_week1_and_week2/" class="post-title-link" itemprop="url">logistic-regression-with-a-neural-network-mindset</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-05 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-05T00:00:00+05:30">2018-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:35" itemprop="dateModified" datetime="2020-04-06T20:25:35+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>These are my personal programming assignments at the first and second week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h1 id="Part-1：Python-Basics-with-Numpy-optional-assignment"><a href="#Part-1：Python-Basics-with-Numpy-optional-assignment" class="headerlink" title="Part 1：Python Basics with Numpy (optional assignment)"></a>Part 1：Python Basics with Numpy (optional assignment)</h1><h2 id="1-Building-basic-functions-with-numpy"><a href="#1-Building-basic-functions-with-numpy" class="headerlink" title="1. Building basic functions with numpy"></a>1. Building basic functions with numpy</h2><p>Numpy is the main package for scientific computing in Python. It is maintained by a large community (<a href="http://www.numpy.org" target="_blank" rel="noopener">www.numpy.org</a>). In this exercise you will learn several key numpy functions such as <code>np.exp</code>, np.log, and np.reshape. You will need to know how to use these functions for future assignments.</p>
<h3 id="1-1-sigmoid-function-np-exp"><a href="#1-1-sigmoid-function-np-exp" class="headerlink" title="1.1 sigmoid function, np.exp()"></a>1.1 sigmoid function, np.exp()</h3><p><strong>Exercise</strong>: Build a function that returns the sigmoid of a real number $x$. Use <code>math.exp(x)</code> for the exponential function.</p>
<p><strong>Reminder</strong>:<br>$sigmoid(x)=\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/1.png" alt="sigmoid function"></p>
<p>To refer to a function belonging to a specific package you could call it using <code>package_name.function()</code>. Run the code below to see an example with <code>math.exp()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: basic_sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">basic_sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute sigmoid of x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + math.exp(-x));</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">basic_sigmoid(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>0.9525741268224334</code></pre><p>Actually, we rarely use the “math” library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### One reason why we use "numpy" instead of "math" in Deep Learning ###</span></span><br><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">basic_sigmoid(x) <span class="comment"># you will see this give an error when you run it, because x is a vector.</span></span><br></pre></td></tr></table></figure>


<p>In fact, if $x=(x_1,x_2,…,x_n)$ is a row vector then <code>np.exp(x)</code> will apply the exponential function to every element of $x$. The output will thus be: $np.exp(x)=(e^{x_1},e^{x_2},…,e^{x_n})$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># example of np.exp</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(np.exp(x)) <span class="comment"># result is (exp(1), exp(2), exp(3))</span></span><br></pre></td></tr></table></figure>

<pre><code>[ 2.71828183  7.3890561  20.08553692]</code></pre><p>Furthermore, if $x$ is a vector, then a Python operation such as $s=x+3$ or $s=\frac{1}{x}$ will output s as a vector of the same size as $x$.</p>
<p><strong>Exercise</strong>: Implement the sigmoid function using numpy.</p>
<p><strong>Instructions</strong>: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices…) are called numpy arrays. You don’t need to know more for now. </p>
<p>$$\text{For } x \in \mathbb{R}^n \text{,     } sigmoid(x) = sigmoid\begin{pmatrix}<br>    x_1  \<br>    x_2  \<br>    …  \<br>    x_n  \<br>\end{pmatrix} = \begin{pmatrix}<br>    \frac{1}{1+e^{-x_1}}  \<br>    \frac{1}{1+e^{-x_2}}  \<br>    …  \<br>    \frac{1}{1+e^{-x_n}}  \<br>\end{pmatrix}\tag{1}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># this means you can access numpy functions by writing np.function() instead of numpy.function()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x));</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]);</span><br><span class="line">sigmoid(x)</span><br></pre></td></tr></table></figure>




<pre><code>array([0.73105858, 0.88079708, 0.95257413])</code></pre><h3 id="1-2-Sigmoid-gradient"><a href="#1-2-Sigmoid-gradient" class="headerlink" title="1.2 Sigmoid gradient"></a>1.2 Sigmoid gradient</h3><p><strong>Exercise</strong>: Implement the function <code>sigmoid_grad()</code> to compute the gradient of the sigmoid function with respect to its input $x$. The formula is:<br>$$sigmoid_derivative(x) = \sigma’(x) = \sigma(x) (1 - \sigma(x))\tag{2}$$<br>You often code this function in two steps: </p>
<ol>
<li>Set s to be the sigmoid of x. You might find your <code>sigmoid(x)</code> function useful. </li>
<li>Compute $\sigma’(x) = s(1-s)$</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid_derivative</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np; <span class="comment"># this means you can access numpy functions by writing np.function() instead of numpy.function()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> +  np.exp(-x));</span><br><span class="line">    ds = s * (<span class="number">1</span> - s);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ds;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid_derivative(x) = "</span> + str(sigmoid_derivative(x)))</span><br></pre></td></tr></table></figure>

<pre><code>sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]</code></pre><h3 id="1-3-Reshaping-arrays"><a href="#1-3-Reshaping-arrays" class="headerlink" title="1.3  Reshaping arrays"></a>1.3  Reshaping arrays</h3><p>Two common numpy functions used in deep learning are <code>np.shape</code> and <code>np.reshape()</code>. </p>
<ul>
<li><code>X.shape</code> is used to get the shape (dimension) of a matrix/vector X. </li>
<li><code>X.reshape()</code> is used to reshape X into some other dimension.</li>
</ul>
<p>For example, in computer science, an image is represented by a 3D array of shape (length,height,depth=3). However, when you read an image as the input of an algorithm you convert it to a vector of <code>shape (length∗height∗3,1)</code>. In other words, you “unroll”, or reshape, the 3D array into a 1D vector.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/2.png" alt=""></p>
<p><strong>Exercise</strong>: Implement <code>image2vector(</code>) that takes an input of shape(length, height, 3) and returns a vector of <code>shape(length * height * 3, 1)</code>. For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v = v.reshape(v.shape[<span class="number">0</span>] * v.shape[<span class="number">1</span>], v.shape[<span class="number">2</span>]) <span class="comment"># v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c;</span></span><br></pre></td></tr></table></figure>
<p><strong>Please don’t hardcode the dimensions of image as a constant. Instead look up the quantities you need with <code>image.shape[0]</code>, etc.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: image2vector</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    v = image.reshape(image.shape[<span class="number">0</span>] * image.shape[<span class="number">1</span>] * image.shape[<span class="number">2</span>], <span class="number">1</span>); </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> v;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line">image = np.random.rand(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>);</span><br><span class="line">image2vector(image)</span><br></pre></td></tr></table></figure>




<pre><code>array([[0.51571749],
       [0.44538647],
       [0.53561213],
       [0.1172449 ],
       [0.89271698],
       [0.30177735],
       [0.61210542],
       [0.5702647 ],
       [0.14097692],
       [0.30515161],
       [0.28477894],
       [0.69207277],
       [0.74081467],
       [0.36062328],
       [0.3069694 ],
       [0.90502389],
       [0.21609838],
       [0.92749893],
       [0.80694438],
       [0.98316829],
       [0.87806386],
       [0.41072457],
       [0.74295058],
       [0.30800667],
       [0.85316743],
       [0.46848715],
       [0.56193027]])</code></pre><h3 id="1-4-Normalizing-rows"><a href="#1-4-Normalizing-rows" class="headerlink" title="1.4 Normalizing rows"></a>1.4 Normalizing rows</h3><p>Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to ${x\over||x||}$ (dividing each row vector of x by its norm).</p>
<p>For example, if</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line">x = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">2</span>,<span class="number">3</span>));</span><br><span class="line">print(x);</span><br></pre></td></tr></table></figure>

<pre><code>[[9 5 8]
 [5 3 6]]</code></pre><p>then<br>$$| x| = np.linalg.norm(x, axis = 0, keepdims = True) \tag{3}$$</p>
<p>and<br>$$x_{normalized} = \frac{x}{| x|} \tag{4}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line">x = np.random.randint(<span class="number">1</span>,<span class="number">10</span>,(<span class="number">2</span>,<span class="number">3</span>));</span><br><span class="line">print(x);</span><br><span class="line">x_norm = np.linalg.norm(x, axis = <span class="number">0</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line">print(x_norm);</span><br><span class="line">x_normalized = x / x_norm;</span><br><span class="line">print(x_normalized);</span><br></pre></td></tr></table></figure>

<pre><code>[[6 4 3]
 [8 2 1]]
[[10.          4.47213595  3.16227766]]
[[0.6        0.89442719 0.9486833 ]
 [0.8        0.4472136  0.31622777]]</code></pre><p>Note that you can divide matrices of different sizes and it works fine: this is called <strong>broadcasting</strong> and you’re going to learn about it in part 5.</p>
<p><strong>Exercise</strong>: Implement <code>normalizeRows()</code> to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: normalizeRows</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n, m)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)</span></span><br><span class="line">    x_norm = np.linalg.norm(x, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Divide x by its norm.</span></span><br><span class="line">    x = x / x_norm;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">9</span>, <span class="number">0</span>, <span class="number">16</span>]])</span><br><span class="line">print(x);</span><br><span class="line">print(normalizeRows(x));</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0  3  4]
 [ 9  0 16]]
[[0.         0.6        0.8       ]
 [0.49026124 0.         0.87157554]]</code></pre><p><strong>Note</strong>:<br>In <code>normalizeRows()</code>, you can try to print the shapes of x_norm and x, and then rerun the assessment. You’ll find out that they have different shapes. This is normal given that x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column. So how did it work when you divided x by x_norm? This is called broadcasting and we’ll talk about it now!</p>
<h3 id="1-5-Broadcasting-and-the-softmax-function"><a href="#1-5-Broadcasting-and-the-softmax-function" class="headerlink" title="1.5 Broadcasting and the softmax function"></a>1.5 Broadcasting and the softmax function</h3><p>A very important concept to understand in numpy is “broadcasting”. It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official broadcasting documentation.</p>
<p><strong>Exercise</strong>: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.</p>
<p><strong>Instructions</strong>:<br>$$softmax(x) = softmax\begin{bmatrix}<br>    x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1n} \<br>    x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2n} \<br>    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>    x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots  &amp; x_{mn}<br>\end{bmatrix} = \begin{bmatrix}<br>    \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} &amp; \dots  &amp; \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \<br>    \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} &amp; \dots  &amp; \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \<br>    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} &amp; \dots  &amp; \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}<br>\end{bmatrix} \= \begin{pmatrix}<br>    softmax\text{(first row of x)}  \<br>    softmax\text{(second row of x)} \<br>    …  \<br>    softmax\text{(last row of x)} \<br>\end{pmatrix}$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: softmax</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Calculates the softmax for each row of the input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Your code should work for a row vector and also for matrices of shape (n, m).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n,m)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="comment"># Apply exp() element-wise to x. Use np.exp(...).</span></span><br><span class="line">    x_exp = np.exp(x);</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).</span></span><br><span class="line">    x_norm = np.linalg.norm(x_exp, axis = <span class="number">1</span>, keepdims = <span class="literal">True</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span></span><br><span class="line">    s = x_exp / x_norm;</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span> ,<span class="number">0</span>]])</span><br><span class="line">print(softmax(x));</span><br></pre></td></tr></table></figure>

<pre><code>[[9.99831880e-01 9.11728660e-04 1.83125597e-02 1.23389056e-04
  1.23389056e-04]
 [9.90964875e-01 1.34112512e-01 9.03642998e-04 9.03642998e-04
  9.03642998e-04]]</code></pre><p><strong>Note</strong>: </p>
<ul>
<li>If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). x_exp/x_sum works due to python broadcasting.</li>
</ul>
<p><strong>What you need to remember</strong>: </p>
<ul>
<li><code>np.exp(x)</code> works for any np.array x and applies the exponential function to every coordinate </li>
<li>the sigmoid function and its gradient </li>
<li>image2vector is commonly used in deep learning </li>
<li><code>np.reshape</code> is widely used. In the future, you’ll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. </li>
<li>numpy has efficient built-in functions </li>
<li><strong>broadcasting</strong> is extremely useful</li>
</ul>
<h2 id="2-Vectorization"><a href="#2-Vectorization" class="headerlink" title="2 Vectorization"></a>2 Vectorization</h2><p>In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">x1 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">x2 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">    dot+= x1[i] * x2[i]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dot = "</span> + str(dot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC OUTER PRODUCT IMPLEMENTATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">outer = np.zeros((len(x1),len(x2))) <span class="comment"># we create a len(x1)*len(x2) matrix with only zeros</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x2)):</span><br><span class="line">        outer[i,j] = x1[i] * x2[j]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"outer = "</span> + str(outer) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC ELEMENTWISE IMPLEMENTATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">mul = np.zeros(len(x1))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">    mul[i] = x1[i] * x2[i]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"elementwise multiplication = "</span> + str(mul) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###</span></span><br><span class="line">W = np.random.rand(<span class="number">3</span>,len(x1)) <span class="comment"># Random 3*len(x1) numpy array</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">gdot = np.zeros(W.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(W.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">        gdot[i] += W[i,j] * x1[j]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"gdot = "</span> + str(gdot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span> * (toc - tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>dot = 278
 ----- Computation time = 0.0ms
outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
 ----- Computation time = 0.0ms
elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]
 ----- Computation time = 0.0ms
gdot = [19.43421812 18.68022029 16.86207096]
 ----- Computation time = 0.0ms</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x1 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">x2 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED DOT PRODUCT OF VECTORS ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = np.dot(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dot = "</span> + str(dot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED OUTER PRODUCT ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">outer = np.outer(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"outer = "</span> + str(outer) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED ELEMENTWISE MULTIPLICATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">mul = np.multiply(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"elementwise multiplication = "</span> + str(mul) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED GENERAL DOT PRODUCT ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = np.dot(W,x1)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"gdot = "</span> + str(dot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>dot = 278
 ----- Computation time = 0.0ms
outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]
 ----- Computation time = 0.0ms
elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]
 ----- Computation time = 0.0ms
gdot = [19.43421812 18.68022029 16.86207096]
 ----- Computation time = 0.0ms</code></pre><p><strong>As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger</strong>.</p>
<p><strong>Note</strong> that <code>np.dot()</code> performs a matrix-matrix or matrix-vector multiplication. This is different from <code>np.multiply()</code> and the <code>*</code> operator (which is equivalent to <code>.*</code> in Matlab/Octave), which performs an element-wise multiplication.</p>
<h3 id="2-1-Implement-the-L1-and-L2-loss-functions"><a href="#2-1-Implement-the-L1-and-L2-loss-functions" class="headerlink" title="2.1 Implement the L1 and L2 loss functions"></a>2.1 Implement the L1 and L2 loss functions</h3><p><strong>Exercise</strong>: Implement the numpy vectorized version of the L1 loss. You may find the function <code>abs(x)</code> (absolute value of x) useful.</p>
<p><strong>Reminder</strong>: </p>
<ul>
<li>The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions $(\hat{y})$ are from the true values $(y)$. In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost. </li>
<li>L1 loss is defined as: </li>
</ul>

$$\begin{align*} & L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align*}\tag{5}$$



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L1</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.sum(np.abs(y - yhat));</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.random.randn(<span class="number">1</span>,<span class="number">5</span>);</span><br><span class="line">print(yhat);</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]);</span><br><span class="line">print(<span class="string">"L1 = "</span> + str(L1(yhat,y)));</span><br></pre></td></tr></table></figure>

<pre><code>[[ 0.17368857 -1.46853016  0.27681907 -0.05448256  0.9010455 ]]
L1 = 3.7250977210513185</code></pre><p>Exercise: Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function <code>np.dot()</code> useful. As a reminder, if $x = [x_1, x_2, …, x_n]$ , then $np.dot(x,x) = \sum_{j=0}^n x_j^{2}$ .</p>
<ul>
<li>L2 loss is defined as</li>
</ul>

$$\begin{align*} & L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}\tag{6}$$



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L2</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss =np.dot(y - yhat, y - yhat);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(L2(yhat,y));</span><br></pre></td></tr></table></figure>

<pre><code>0.43</code></pre><p><strong>What to remember</strong>: </p>
<ul>
<li>Vectorization is very important in deep learning. It provides computational efficiency and clarity. </li>
<li>You have reviewed the L1 and L2 loss. </li>
<li>You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc…</li>
</ul>
<h1 id="Part-2：-Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Part-2：-Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Part 2： Logistic Regression with a Neural Network mindset"></a>Part 2： Logistic Regression with a Neural Network mindset</h1><p>You will learn to: </p>
<ul>
<li>Build the general architecture of a learning algorithm, including: </li>
<li>Initializing parameters </li>
<li>Calculating the cost function and its gradient </li>
<li>Using an optimization algorithm (gradient descent) </li>
<li>Gather all three functions above into a main model function, in the right order.</li>
</ul>
<h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1. Packages"></a>1. Packages</h2><p>First, let’s run the cell below to import all the packages that you will need during this assignment. </p>
<ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="noopener">numpy</a> is the fundamental package for scientific computing with Python. </li>
<li><a href="http://www.h5py.org/" target="_blank" rel="noopener">h5py</a> is a common package to interact with a dataset that is stored on an H5 file. </li>
<li><a href="http://matplotlib.org/" target="_blank" rel="noopener">matplotlib</a> is a famous library to plot graphs in Python. </li>
<li><a href="http://www.pythonware.com/products/pil/" target="_blank" rel="noopener">PIL</a> and <a href="https://www.scipy.org/" target="_blank" rel="noopener">scipy</a> are used here to test your model with your own picture at the end.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">% matplotlib inline</span><br></pre></td></tr></table></figure>

<h2 id="2-Overview-of-the-Problem-set"><a href="#2-Overview-of-the-Problem-set" class="headerlink" title="2. Overview of the Problem set"></a>2. Overview of the Problem set</h2><p>Problem Statement: You are given a dataset (“data.h5”) containing: </p>
<ul>
<li>a training set of m_train images labeled as cat (y=1) or non-cat (y=0) </li>
<li>a test set of m_test images labeled as cat or non-cat </li>
<li>each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).</li>
</ul>
<p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p>
<p>Let’s get more familiar with the dataset. Load the data by running the following code.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset();</span><br></pre></td></tr></table></figure>

<p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p>
<p>Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">25</span></span><br><span class="line">plt.imshow(train_set_x_orig[index])</span><br><span class="line">print(<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>);</span><br></pre></td></tr></table></figure>

<pre><code>y = [1], it&apos;s a &apos;cat&apos; picture.</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/output_58_1.png" alt="png"></p>
<p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p>
<p>Exercise: Find the values for: </p>
<ul>
<li>m_train (number of training examples) </li>
<li>m_test (number of test examples) </li>
<li>num_px (= height = width of a training image)<br>Remember that <code>train_set_x_orig</code> is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing <code>train_set_x_orig.shape[0]</code>.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>];</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>];</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">1</span>];</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape));</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: m_train = 209
Number of testing examples: m_test = 50
Height/Width of each image: num_px = 64
Each image is of size: (64, 64, 3)
train_set_x shape: (209, 64, 64, 3)
train_set_y shape: (1, 209)
test_set_x shape: (50, 64, 64, 3)
test_set_y shape: (1, 50)</code></pre><p>For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px ∗ num_px ∗ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.</p>
<p><strong>Exercise</strong>: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px ∗ num_px ∗ 3, 1).</p>
<p><strong>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b∗c∗d, a) is to use:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[<span class="number">0</span>], <span class="number">-1</span>).T      <span class="comment"># X.T is the transpose of X</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(m_train, <span class="number">-1</span>).T;</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(m_test, <span class="number">-1</span>).T;</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]));</span><br></pre></td></tr></table></figure>

<pre><code>train_set_x_flatten shape: (12288, 209)
train_set_y shape: (1, 209)
test_set_x_flatten shape: (12288, 50)
test_set_y shape: (1, 50)
sanity check after reshaping: [17 31 56 22 33]</code></pre><p>To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.</p>
<p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. <strong>But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</strong></p>
<p>Let’s standardize our dataset.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x = train_set_x_flatten / <span class="number">255</span>;</span><br><span class="line">test_set_x = test_set_x_flatten / <span class="number">255</span>;</span><br></pre></td></tr></table></figure>

<p><strong>What you need to remember:</strong></p>
<p>Common steps for pre-processing a new dataset are: </p>
<ol>
<li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) </li>
<li>Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) </li>
<li>“Standardize”the data</li>
</ol>
<h2 id="3-General-Architecture-of-the-learning-algorithm"><a href="#3-General-Architecture-of-the-learning-algorithm" class="headerlink" title="3. General Architecture of the learning algorithm"></a>3. General Architecture of the learning algorithm</h2><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p>
<p>You will build a Logistic Regression, using a Neural Network mindset. <strong>The following Figure explains why Logistic Regression is actually a very simple Neural Network!</strong></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/3.png" alt=""></p>
<p>Mathematical expression of the algorithm:</p>
<p>For one example: $x^{(i)}$</p>
<p>$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$<br>$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$<br>$$\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})\tag{3}$$<br>The cost is then computed by summing over all training examples:<br>$$J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{4}$$</p>
<p><strong>Key steps</strong>: </p>
<p>In this exercise, you will carry out the following steps: </p>
<ol>
<li>Initialize the parameters of the model </li>
<li>Learn the parameters for the model by minimizing the cost </li>
<li>Use the learned parameters to make predictions (on the test set) </li>
<li>Analyse the results and conclude</li>
</ol>
<h2 id="4-Building-the-parts-of-our-algorithm"><a href="#4-Building-the-parts-of-our-algorithm" class="headerlink" title="4. Building the parts of our algorithm"></a>4. Building the parts of our algorithm</h2><p><strong>The main steps for building a Neural Network</strong> are: </p>
<ol>
<li>Define the model structure (such as number of input features) </li>
<li>Initialize the model’s parameters </li>
<li>Loop: </li>
</ol>
<ul>
<li>Calculate current loss (forward propagation) </li>
<li>Calculate current gradient (backward propagation) </li>
<li>Update parameters (gradient descent)</li>
</ul>
<p>You often build 1-3 separately and integrate them into one function we call <code>model()</code>.</p>
<h3 id="4-1-Helper-functions"><a href="#4-1-Helper-functions" class="headerlink" title="4.1 Helper functions"></a>4.1 Helper functions</h3><p><strong>Exercise</strong>: Using your code from “Python Basics”, implement <code>sigmoid()</code>. As you’ve seen in the figure above, you need to compute  $sigmoid(w^Tx+b)=\frac{1}{1 + e^{−(w^Tx+b)}}$  to make predictions. Use <code>np.exp()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z));</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"sigmoid([0, 2]) = "</span> + str(sigmoid(np.array([<span class="number">0</span>,<span class="number">2</span>]))));</span><br></pre></td></tr></table></figure>

<pre><code>sigmoid([0, 2]) = [0.5        0.88079708]</code></pre><h3 id="4-2-Initializing-parameters"><a href="#4-2-Initializing-parameters" class="headerlink" title="4.2 Initializing parameters"></a>4.2 Initializing parameters</h3><p><strong>Exercise</strong>: Implement parameter initialization in the cell below. You have to initialize $w$ as a vector of zeros. If you don’t know what numpy function to use, look up <code>np.zeros()</code> in the Numpy library’s documentation.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_with_zeros</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>));</span><br><span class="line">    b = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>));</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> w, b;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">2</span>;</span><br><span class="line">w, b = initialize_with_zeros(dim);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(w));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(b));</span><br></pre></td></tr></table></figure>

<pre><code>w = [[0.]
 [0.]]
b = 0</code></pre><p>For image inputs, w will be of shape $(num_px \times num_px \times 3, 1)$.</p>
<h3 id="4-3-Forward-and-Backward-propagation"><a href="#4-3-Forward-and-Backward-propagation" class="headerlink" title="4.3. Forward and Backward propagation"></a>4.3. Forward and Backward propagation</h3><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p>
<p><strong>Exercise</strong>: Implement a function propagate() that computes the cost function and its gradient.</p>
<p><strong>Hints</strong>:</p>
<p>Forward Propagation: </p>
<ul>
<li>You get $X$ </li>
<li>You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, …, a^{(m-1)}, a^{(m)})$</li>
<li>You calculate the cost function $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</li>
<li>Here are the two formulas you will be using:<br>$$\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{5}$$<br>$$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{6}$$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: propagate</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)                                 =&gt; w = n * 1</span></span><br><span class="line"><span class="string">    b -- bias, a scalar                                                                          =&gt; b = 1 * 1</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)                                  =&gt; X = n * m</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) =&gt; Y = 1 * m</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    a = sigmoid(np.dot(w.T, X) + b);                                                         <span class="comment"># compute activation</span></span><br><span class="line">    cost = - <span class="number">1</span> / m * (np.dot(Y, np.log(a).T) + np.dot(<span class="number">1</span> - Y, np.log(<span class="number">1</span> - a).T));              <span class="comment"># compute cost</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dw = <span class="number">1</span> / m * np.dot(X,(a - Y).T);</span><br><span class="line">    db = <span class="number">1</span> / m * np.sum(a - Y);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape);</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float);</span><br><span class="line">    cost = np.squeeze(cost);</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ());</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads, cost;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np;</span><br><span class="line">w, b, X, Y = np.array([[<span class="number">1.</span>],[<span class="number">2.</span>]]), <span class="number">2.</span>, np.array([[<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">-1.</span>],[<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">-3.2</span>]]), np.array([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]]);</span><br><span class="line">grads, cost = propagate(w, b, X, Y);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cost = "</span> + str(cost));</span><br></pre></td></tr></table></figure>

<pre><code>dw = [[0.99845601]
 [2.39507239]]
db = 0.001455578136784208
cost = 5.801545319394553</code></pre><h3 id="4-4-Optimization"><a href="#4-4-Optimization" class="headerlink" title="4.4. Optimization"></a>4.4. Optimization</h3><ul>
<li>You have initialized your parameters.</li>
<li>You are also able to compute a cost function and its gradient.</li>
<li>Now, you want to update the parameters using gradient descent.</li>
</ul>
<p><strong>Exercise</strong>: Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $\theta=\theta−\alpha d\theta$, where $\alpha$ is the learning rate.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y);</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>];</span><br><span class="line">        db = grads[<span class="string">"db"</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        w -= learning_rate * dw;</span><br><span class="line">        b -= learning_rate * db;</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line"></span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params, grads, costs = optimize(w, b, X, Y, num_iterations= <span class="number">100</span>, learning_rate = <span class="number">0.009</span>, print_cost = <span class="literal">False</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(params[<span class="string">"w"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(params[<span class="string">"b"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>w = [[-0.08608643]
 [ 0.10971233]]
b = -0.1442742664803268
dw = [[0.12311093]
 [0.13629247]]
db = -0.14923915884638042</code></pre><p><strong>Exercise</strong>: The previous function will output the learned $w$ and $b$. We are able to use w and b to predict the labels for a dataset $X$. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p>
<ol>
<li>Calculate $\hat{Y}=A=σ(w^TX+b)$</li>
<li>Convert the entries of a into 0 (<code>if activation &lt;= 0.5</code>) or 1 (<code>if activation &gt; 0.5</code>), stores the predictions in a vector <code>Y_prediction</code>. If you wish, you can use an <code>if/else</code> statement in a <code>for</code> loop (though there is also a way to vectorize this).</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b);</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>,i] &gt; <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i] = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Y_prediction;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([[<span class="number">0.1124579</span>],[<span class="number">0.23106775</span>]]);</span><br><span class="line">b = <span class="number">-0.3</span>;</span><br><span class="line">X = np.array([[<span class="number">1.</span>,<span class="number">-1.1</span>,<span class="number">-3.2</span>],[<span class="number">1.2</span>,<span class="number">2.</span>,<span class="number">0.1</span>]]);</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"predictions = "</span> + str(predict(w, b, X)));</span><br></pre></td></tr></table></figure>

<pre><code>predictions = [[1. 1. 0.]]</code></pre><p><strong>What to remember</strong>:</p>
<p>You’ve implemented several functions that: </p>
<ul>
<li>Initialize (w,b) </li>
<li>Optimize the loss iteratively to learn parameters (w,b): </li>
<li>computing the cost and its gradient </li>
<li>updating the parameters using gradient descent </li>
<li>Use the learned (w,b) to predict the labels for a given set of examples</li>
</ul>
<h2 id="5-Merge-all-functions-into-a-model"><a href="#5-Merge-all-functions-into-a-model" class="headerlink" title="5. Merge all functions into a model"></a>5. Merge all functions into a model</h2><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p>
<p>Exercise: Implement the model function. Use the following notation: </p>
<ul>
<li><code>Y_prediction</code> for your predictions on the test set </li>
<li><code>Y_prediction_train</code> for your predictions on the train set </li>
<li><code>w, costs, grads</code> for the outputs of <code>optimize()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></span><br><span class="line">    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost);</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = params[<span class="string">"w"</span>];</span><br><span class="line">    b = params[<span class="string">"b"</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></span><br><span class="line">    Y_prediction_train = predict(w, b, X_train);</span><br><span class="line">    Y_prediction_test = predict(w, b, X_test);</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>);</span><br></pre></td></tr></table></figure>

<pre><code>Cost after iteration 0: 0.693147
Cost after iteration 100: 0.584508
Cost after iteration 200: 0.466949
Cost after iteration 300: 0.376007
Cost after iteration 400: 0.331463
Cost after iteration 500: 0.303273
Cost after iteration 600: 0.279880
Cost after iteration 700: 0.260042
Cost after iteration 800: 0.242941
Cost after iteration 900: 0.228004
Cost after iteration 1000: 0.214820
Cost after iteration 1100: 0.203078
Cost after iteration 1200: 0.192544
Cost after iteration 1300: 0.183033
Cost after iteration 1400: 0.174399
Cost after iteration 1500: 0.166521
Cost after iteration 1600: 0.159305
Cost after iteration 1700: 0.152667
Cost after iteration 1800: 0.146542
Cost after iteration 1900: 0.140872
train accuracy: 99.04306220095694 %
test accuracy: 70.0 %</code></pre><p><strong>Comment</strong>: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p>
<p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce <strong>overfitting</strong>, for example by using <strong>regularization</strong>. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture that was wrongly classified.</span></span><br><span class="line">index = <span class="number">15</span>;</span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)));</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(test_set_y[<span class="number">0</span>, index]) + <span class="string">", you predicted that it is a \""</span> + classes[int(d[<span class="string">"Y_prediction_test"</span>][<span class="number">0</span>,index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>);</span><br><span class="line"><span class="comment">#print("y = " + str(train_set_y[:, index]) + ", it's a '" + classes[np.squeeze(train_set_y[:, index])].decode("utf-8") +  "' picture.");</span></span><br></pre></td></tr></table></figure>

<pre><code>y = 1, you predicted that it is a &quot;cat&quot; picture.</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/output_93_1.png" alt="png"></p>
<p>Let’s also plot the cost function and the gradients.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>]);</span><br><span class="line">plt.plot(costs);</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>);</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>);</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]));</span><br><span class="line">plt.show();</span><br></pre></td></tr></table></figure>


<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/output_95_0.png" alt="png"></p>
<p><strong>Interpretation</strong>:<br>You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p>
<h2 id="6-Further-analysis-optional-ungraded-exercise"><a href="#6-Further-analysis-optional-ungraded-exercise" class="headerlink" title="6. Further analysis (optional/ungraded exercise)"></a>6. Further analysis (optional/ungraded exercise)</h2><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate $α$.</p>
<p><strong>Choice of learning rate</strong></p>
<p><strong>Reminder</strong>:<br>In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $α$ determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p>
<p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>];</span><br><span class="line">models = &#123;&#125;;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i));</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>);</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]));</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>);</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>);</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="literal">True</span>);</span><br><span class="line">frame = legend.get_frame();</span><br><span class="line">frame.set_facecolor(<span class="string">'0.50'</span>);</span><br><span class="line">plt.show();</span><br></pre></td></tr></table></figure>

<pre><code>learning rate is: 0.01
train accuracy: 99.52153110047847 %
test accuracy: 68.0 %

-------------------------------------------------------

learning rate is: 0.001
train accuracy: 88.99521531100478 %
test accuracy: 64.0 %

-------------------------------------------------------

learning rate is: 0.0001
train accuracy: 68.42105263157895 %
test accuracy: 36.0 %

-------------------------------------------------------</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/output_98_1.png" alt="png"></p>
<p><strong>Interpretation</strong>: </p>
<ul>
<li>Different learning rates give different costs and thus different predictions results. </li>
<li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). </li>
<li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. </li>
<li>In deep learning, we usually recommend that you: <ul>
<li>Choose the learning rate that better minimizes the cost function. </li>
<li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</li>
</ul>
</li>
</ul>
<h2 id="7-Test-with-your-own-image-optional-ungraded-exercise"><a href="#7-Test-with-your-own-image-optional-ungraded-exercise" class="headerlink" title="7. Test with your own image (optional/ungraded exercise)"></a>7. Test with your own image (optional/ungraded exercise)</h2><p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: </p>
<ol>
<li>Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. </li>
<li>Add your image to this Jupyter Notebook’s directory, in the “images” folder </li>
<li>Change your image’s name in the following code </li>
<li>Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## START CODE HERE ## (PUT YOUR IMAGE NAME) </span></span><br><span class="line">my_image = <span class="string">"isacatornot.jpg"</span>;   <span class="comment"># change this to the name of your image file </span></span><br><span class="line"><span class="comment">## END CODE HERE ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We preprocess the image to fit your algorithm.</span></span><br><span class="line">fname = <span class="string">"images/"</span> + my_image;</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="literal">False</span>));</span><br><span class="line"></span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((<span class="number">1</span>, num_px*num_px*<span class="number">3</span>)).T;</span><br><span class="line">my_predicted_image = predict(d[<span class="string">"w"</span>], d[<span class="string">"b"</span>], my_image);</span><br><span class="line"></span><br><span class="line">plt.imshow(image);</span><br><span class="line">print(<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your algorithm predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>);</span><br></pre></td></tr></table></figure>

<pre><code>y = 1.0, your algorithm predicts a &quot;cat&quot; picture.</code></pre><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/programming_assignments/week1_week2/output_101_2.png" alt="png"></p>
<p><strong>What to remember from this assignment</strong>: </p>
<ol>
<li>Preprocessing the dataset is important. </li>
<li>You implemented each function separately: <code>initialize()</code>, <code>propagate()</code>, <code>optimize()</code>. Then you built a <code>model()</code>. </li>
<li>Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/04/04_deep-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/04/04_deep-neural-networks/" class="post-title-link" itemprop="url">04_deep-neural-network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-04 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-04T00:00:00+05:30">2018-02-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:22" itemprop="dateModified" datetime="2020-04-06T20:25:22+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the 4th week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="01-deep-neural-network"><a href="#01-deep-neural-network" class="headerlink" title="01_deep-neural-network"></a>01_deep-neural-network</h2><p>Welcome to the fourth week of this course. By now, you’ve seen four promulgation and back promulgation in the context of a neural network, with a single hidden layer, as well as logistic regression, and you’ve learned about vectorization, and when it’s important to initialize the ways randomly. If you’ve done the past couple weeks homework, you’ve also implemented and seen some of these ideas work for yourself. So by now, you’ve actually seen most of the ideas you need to implement a deep neural network. What we’re going to do this week, is take those ideas and put them together so that you’ll be able to implement your own deep neural network. Because this week’s problem exercise is longer, it just has been more work, I’m going to keep the videos for this week shorter as you can get through the videos a little bit more quickly, and then have more time to do a significant problem exercise at then end, which I hope will leave you having thoughts deep in neural network, that if you feel proud of.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/1.png" alt=""></p>
<p>but over the last several years the AI, on the machine learning community, has realized that there are functions that very deep neural networks can learn that shallower models are often unable to. Although for any given problem, it might be hard to predict in advance exactly how deep in your network you would want. So it would be reasonable to try logistic regression, try one and then two hidden layers, and view the number of hidden layers as another hyper parameter that you could try a variety of values of, and evaluate on all that across validation data, or on your development set. See more about that later as well.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/2.png" alt=""></p>
<h2 id="02-forward-propagation-in-a-deep-network"><a href="#02-forward-propagation-in-a-deep-network" class="headerlink" title="02_forward-propagation-in-a-deep-network"></a>02_forward-propagation-in-a-deep-network</h2><p>In the last video we distract what is the deep neural network and also talked about the notation we use to describe such networks in this video you see how you can perform for propagation in a deep network.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/3.png" alt=""></p>
<p>One of the ways to increase your odds of having bug-free implementation is to think very systematic and carefully about the matrix dimensions you’re working with so when I’m trying to develop my own code I often pull a piece of paper and just think carefully through so the dimensions of the matrix I’m working with let’s see how you could do that in the next video.</p>
<h2 id="03-getting-your-matrix-dimensions-right"><a href="#03-getting-your-matrix-dimensions-right" class="headerlink" title="03_getting-your-matrix-dimensions-right"></a>03_getting-your-matrix-dimensions-right</h2><p>When implementing a deep neural network, one of the debugging tools I often use to check the correctness of my code is to pull a piece of paper, and just work through the dimensions and matrix I’m working with. So let me show you how to do that, since I hope this will make it easier for you to implement your deep nets as well.</p>
<h3 id="one-training-example"><a href="#one-training-example" class="headerlink" title="one training example"></a>one training example</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/4.png" alt=""></p>
<p>$\because \text{the dimensions of x}(a^{[0]}) \text{: } (n^{[0]}, 1)$</p>
<p>$\therefore $<br>$W^{[l]}: (n^{[l]}, n^{[l-1]})$</p>
<p>$b^{[l]}: (n^{[l]}, 1)$</p>
<p>$dW^{[l]}: (n^{[l]}, n^{[l-1]})$</p>
<p>$db^{[l]}: (n^{[l]}, 1)$</p>
<p>$dz^{[l]}: (n^{[l]}, 1)$</p>
<p>$da^{[l]}: (n^{[l]}, 1)$ is the same shape of $z^{[l]}$.</p>
<h3 id="m-training-examples"><a href="#m-training-examples" class="headerlink" title="m training examples"></a>m training examples</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/5.png" alt=""></p>
<p>$\because \text{the dimensions of X}(A^{[0]}) \text{: } (n^{[0]}, m)$</p>
<p>$\therefore$</p>
<p>$W^{[l]} : (n^{[l]}, n^{[l-1]})$</p>
<p>$B^{[l]} : (n^{[l]}, m) $</p>
<p>$dW^{[l]} : (n^{[l]}, n^{[l-1]})$</p>
<p>$dB^{[l]} : (n^{[l]}, m)$</p>
<p>$dZ^{[l]} : (n^{[l]}, m)$</p>
<p>$dA^{[l]} : (n^{[l]}, m) \text{ is the same shape of } Z^{[l]}$</p>
<p>So I hope the little exercise we went through helps clarify the dimensions that the various matrices you’d be working with. <strong>When you implement backpropagation for a deep neural network, so long as you work through your code and make sure that all the matrices’ dimensions are consistent. That will usually help, it’ll go some ways toward eliminating some cause of possible bugs</strong>. So I hope that exercise for figuring out the dimensions of various matrices you’ll been working with is helpful. When you implement a deep neural network, if you keep straight the dimensions of these various matrices and vectors you’re working with. Hopefully they’ll help you eliminate some cause of possible bugs, <strong>it certainly helps me get my code right</strong>. So next, we’ve now seen some of the mechanics of how to do forward propagation in a neural network. But why are deep neural networks so effective, and why do they do better than shallow representations? Let’s spend a few minutes in the next video to discuss that.</p>
<h2 id="04-why-deep-representations"><a href="#04-why-deep-representations" class="headerlink" title="04_why-deep-representations"></a>04_why-deep-representations</h2><p>We’ve all been hearing that deep neural networks work really well for a lot of problems, and it’s not just that they need to be big neural networks, is that specifically, they need to be deep or to have a lot of hidden layers. So why is that? Let’s go through a couple examples and try to gain some intuition for why deep networks might work well.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/6.png" alt=""></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/7.png" alt=""></p>
<p>To compute $y=x_{1}\oplus x_{2}\oplus x_{3}\oplus \cdots\oplus x_{n}$, the depth of deep neural network is $O(log_{2}^{n})$, the number of activiation units or nodes is $2^{\log_{2}(n)-1} + \cdots + 2 + 1 = 1\cdot \dfrac{1-2^{\log_{2}(n)}}{1-2}=2^{\log_{2}(n)}-1=n-1$. But in one-hidden-layer neural network, the number of activiation units or nodes is $2^{n-1}$.</p>
<p>Now, in addition to this reasons for preferring deep neural networks to be roughly on, is I think the other reasons the term deep learning has taken off is just branding. This things just we call neural networks belong to hidden layers, but the phrase deep learning is just a great brand, it’s just so deep. So I think that once that term caught on that really new networks rebranded or new networks with many hidden layers rebranded, help to capture the popular imagination as well. They regard as the PR(public relations) branding deep networks do work well. Sometimes people go overboard and insist on using tons of hidden layers. <strong>But when I’m starting out a new problem, I’ll often really start out with even logistic regression then try something with one or two hidden layers and use that as a hyper parameter. Use that as a parameter or hyper parameter that you tune in order to try to find the right depth for your neural network. But over the last several years there has been a trend toward people finding that for some applications, very, very deep neural networks here with maybe many dozens of layers sometimes, can sometimes be the best model for a problem</strong>. So that’s it for the intuitions for why deep learning seems to work well. Let’s now take a look at the mechanics of how to implement not just front propagation, but also back propagation.</p>
<h3 id="05-building-blocks-of-deep-neural-networks"><a href="#05-building-blocks-of-deep-neural-networks" class="headerlink" title="05_building-blocks-of-deep-neural-networks"></a>05_building-blocks-of-deep-neural-networks</h3><p>In the earlier videos from this week as well as from the videos from the past several weeks you’ve already seen the basic building blocks of board propagation and back propagation the key components you need to implement a deep neural network let’s see how you can put these components together to build a deep net use the network with a few layers.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/8.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/9.png" alt=""></p>
<h3 id="06-forward-and-backward-propagation"><a href="#06-forward-and-backward-propagation" class="headerlink" title="06_forward-and-backward-propagation"></a>06_forward-and-backward-propagation</h3><p>In a previous video you saw the basic blocks of implementing a deep neural network for propagation step for each layer and a corresponding backward propagation step let’s see how you can actually implement these steps.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/10.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/11.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/12.png" alt=""></p>
<p>although I have to say you know even today when I implement a learning algorithm sometimes even I’m surprised when my learning algorithm implementation works and it’s because longer complexity of machine learning comes from the data rather than from the lines of code so sometimes you feel like you implement a few lines of code not question what it did but there’s almost magically work and it’s because of all the magic is actually not in the piece of code you write which is often you know not too long it’s not it’s not exactly simple but there’s not you know 10,000 100,000 lines of code but you feed it so much data that sometimes even though I work the machine only for a long time sometimes it’s so you know surprises me a bit when my learning algorithm works because lots of complexity of your learning algorithm comes from the data rather than necessarily from your writing you know thousands and thousands of lines of code all right so that’s um how do you implement deep neural networks and again this will become more concrete when you’ve done the programming exercise before moving on I want to discuss in the next video want to discuss hyper parameters and parameters it turns out that when you’re training deep nets being able to organize your hyper params as well will help you be more efficient in developing your networks in the next video let’s talk about exactly what that means.</p>
<h3 id="07-parameters-vs-hyperparameters"><a href="#07-parameters-vs-hyperparameters" class="headerlink" title="07_parameters-vs-hyperparameters"></a>07_parameters-vs-hyperparameters</h3><p>Being effective in developing your deep neural Nets requires that you not only organize your parameters well but also your hyper parameters, so what are hyper parameters.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/13.png" alt=""></p>
<p>so <strong>when you’re training a deep net for your own application you find that there may be a lot of possible settings for the hyper parameters that you need to just try out so applied deep learning today is a very empirical process where often you might have an idea</strong>.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/14.png" alt=""><br>For example you might have an idea for the best value for the learning rate you might say well maybe alpha equals 0.01 I want to try that then you implemented try it out and then see how that works and then based on that outcome you might say you know what I’ve changed online I want to increase the learning rate to 0.05 and so if you’re not sure what’s the best value for the learning ready-to-use you might try one value of the learning rate alpha and see their cost function j go down like this then you might try a larger value for the learning rate alpha and see the cost function blow up and diverge then you might try another version and see it go down really fast it’s inverse to higher value you might try another version and see it you know see the cost function J do that then I’ll be China so the values you might say okay looks like this the value of alpha gives me a pretty fast learning and allows me to converge to a lower cost function jennice I’m going to use this value of alpha.</p>
<p>you saw in a previous slide that there are a lot of different hybrid parameters and <strong>it turns out that when you’re starting on the new application I should find it very difficult to know in advance exactly what’s the best value of the hyper parameters so what often happen is you just have to try out many different values and go around this cycle your trial some value really try five hidden layers with this many number of hidden units implement that see if it works and then iterate so the title of this slide is that apply deep learning is very empirical process</strong> and empirical process is maybe a fancy way of saying you just have to try a lot of things and see what works.</p>
<p>another effect I’ve seen is that deep learning today is applied to so many problems ranging from computer vision to speech recognition to natural language processing to a lot of structured data applications such as maybe a online advertising or web search or product recommendations and so on and what I’ve seen is that <strong>first</strong> I’ve seen researchers from one discipline any one of these try to go to a different one and sometimes the intuitions about hyper parameters carries over and sometimes it doesn’t so I often advise people especially when starting on a new problem to just try out a range of values and see what works and then the next course we’ll see a systematic way for trying out a range of values all right and <strong>second</strong> even if you’re working on one application for a long time, you know, maybe you’re working on online advertising. As you make progress on the problem, it is quite possible there the best value for the learning rate, a number of hidden units and so on might change, so even if you tune your system to the best value of hyper parameters to daily as possible you find that the best value might change a year from now. Maybe because the computer infrastructure I’d be you know CPUs or the type of GPU running on or something has changed, but so <strong>maybe one rule of thumb is you know every now and then maybe every few months if you’re working on a problem for an extended period of time for many years just try a few values for the hyper parameters and double check if there’s a better value for the hyper parameters and as you do</strong>. So you slowly gain intuition as well about the hyper parameters that work best for your problems and I know that this might seem like an unsatisfying part of deep learning that you just have to try on all the values for these hyper parameters but maybe this is one area where deep learning research is still advancing and maybe over time we’ll be able to give better guidance for the best hyper parameters to use, but it’s also possible that because CPUs and GPUs and networks and data says are all changing and it is possible that the guidance won’t to converge for some time and you just need to keep trying out different values and evaluate them on a hold on cross-validation set or something and pick the value that works for your problems. So that was a brief discussion of hyper parameters in the second course we’ll also give some suggestions for how to systematically explore the space of hyper parameters but by now you actually have pretty much all the tools you need to do their programming exercise before you do that adjust or share view one more set of ideas which is I often ask what does deep learning have to do the human brain.</p>
<h3 id="08-what-does-this-have-to-do-with-the-brain"><a href="#08-what-does-this-have-to-do-with-the-brain" class="headerlink" title="08_what-does-this-have-to-do-with-the-brain"></a>08_what-does-this-have-to-do-with-the-brain</h3><p>So what a deep learning have to do the punchline I would say not a whole lot but let’s take a quick look at why people keep making the analogy between deep learning and the human brain.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/04_deep-neural-networks/15.png" alt=""></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/03/03_shallow-neural-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/03/03_shallow-neural-networks/" class="post-title-link" itemprop="url">03_shallow-neural-networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-03 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-03T00:00:00+05:30">2018-02-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:22" itemprop="dateModified" datetime="2020-04-06T20:25:22+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the third week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="01-shallow-neural-network"><a href="#01-shallow-neural-network" class="headerlink" title="01_shallow-neural-network"></a>01_shallow-neural-network</h2><h3 id="01-neural-networks-overview"><a href="#01-neural-networks-overview" class="headerlink" title="01_neural-networks-overview"></a>01_neural-networks-overview</h3><p>welcome back in this week’s you learn to implement a neural network before diving into the technical details I wanted in this video to give you a quick overview of what you’ll be seeing in this week’s videos so if you don’t follow all the details in this video don’t worry about it we’ll delve in the technical details in the next few videos but for now let’s give a quick overview of how you implement in your network. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/1.png" alt="what&#39;s a Neural Network"></p>
<h3 id="02-neural-network-representation"><a href="#02-neural-network-representation" class="headerlink" title="02_neural-network-representation"></a>02_neural-network-representation</h3><p>You see me draw a few pictures of neural networks. In this video, we’ll talk about exactly what those pictures means. In other words, exactly what those neural networks that we’ve been drawing represent. And we’ll start with focusing on the case of neural networks with what was called a single hidden layer. Here’s a picture of a neural network. Let’s give different parts of<br>these pictures some names.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/2.png" alt="2 layer NN"></p>
<p><strong>note</strong>:</p>
<ol>
<li><p>The term hidden layer refers to the fact that in the training set, the true values for these nodes in the middle are not observed. That is you don’t see what they should be in the training set. You see what the inputs are. You see what the output should be. But the things in the hidden layer are not seen in the training set. So that kind of explains the name hidden there just because you don’t see it in the training set. </p>
</li>
<li><p>One funny thing about notational conventions in neural networks is that this network that you’ve seen here is called a two layer neural network. And the reason is that when we count layers in neural networks, we don’t count the input layer. So the hidden layer is layer one and the output layer is layer two. In our notational convention, we’re calling the input layer layer zero, so technically maybe there are three layers in this neural network, because there’s the input layer, the hidden layer, and the output layer. But in conventional usage, if you read research papers and elsewhere in the course, you see people refer to this particular neural network as a two layer neural network, because we don’t count the input layer as an official layer.</p>
</li>
<li><p>the shape of the parameter $w$  between 2 layers is <strong>(the_number_of_nodes_in_output_layer, the_number_of_nodes_in_intput_layer)</strong>, that’s merely a conventional way. And the parameter $b$ is only a constant, that’s a (1, 1) matrix. </p>
</li>
</ol>
<h3 id="03-computing-a-neural-networks-output"><a href="#03-computing-a-neural-networks-output" class="headerlink" title="03_computing-a-neural-networks-output"></a>03_computing-a-neural-networks-output</h3><p>In the last video you saw what a single hidden layer neural network looks like in this video let’s go through the details of exactly how this neural network computers outputs what you see is that is like logistic regression the repeat of all the times.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/3.png" alt="2 layer NN"></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/4.png" alt="vectorization 2 layer NN"></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/5.png" alt="vectorization 2 layer NN"></p>
<h3 id="04-vectorizing-across-multiple-examples"><a href="#04-vectorizing-across-multiple-examples" class="headerlink" title="04_vectorizing-across-multiple-examples"></a>04_vectorizing-across-multiple-examples</h3><p>In the last video, you saw how to compute the prediction on a neural network, given a single training example. In this video, you see how to vectorize across multiple training examples. And the outcome will be quite similar to what you saw for logistic regression. Whereby stacking up different training examples in different columns of the matrix, you’d be able to take the equations you had from the previous video. And with very little modification, change them to make the neural network compute the outputs on all the examples on pretty much all at the same time. So let’s see the details on how to do that. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/6.png" alt="vectorization 2 layer NN"></p>
<p><strong>note</strong> : the row and column indices of the matrix $A, Z$ respectively correspond to the sequence numbers of the train examples and the nodes(units) in layers. And, the row and column indices of the matrix $X$ separately correspond to the sequence numbers of the train examples and the features of a example. Finally,  the row and column indices of the matrix $W$ separately correspond to the sequence numbers of the output units and the input units in the layer.</p>
<h3 id="05-explanation-for-vectorized-implementation"><a href="#05-explanation-for-vectorized-implementation" class="headerlink" title="05_explanation-for-vectorized-implementation"></a>05_explanation-for-vectorized-implementation</h3><p>In the previous video, we saw how with your training examples stacked up horizontally in the matrix $X$, you can derive a vectorized implementation for propagation through your neural network. Let’s give a bit more justification for why the equations we wrote down is a correct implementation of vectorizing across multiple examples.</p>
<p>So let’s go through part of the propagation calculation for the few examples.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/7.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/8.png" alt=""></p>
<h3 id="06-activation-functions"><a href="#06-activation-functions" class="headerlink" title="06_activation-functions"></a>06_activation-functions</h3><p>When you boost a neural network, one of the choices you get to make is what activation functions use independent layers as well as at the output unit of your neural network so far we’ve just been using the sigmoid activation function but sometimes other choices can work much better let’s take a look at some of the options.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/9.png" alt=""></p>
<p>In the forward propagation steps for the neural network we have these two steps where we use the sigmoid function here so that sigmoid is called an <strong>activation function</strong>, so in the more general case we can have a different function G of z, $g(z)$, where G could be a nonlinear function that may not be the sigmoid function.       </p>
<p>So for example the sigmoid function goes between 0 &amp; 1, an activation function that almost always works better than the sigmoid function is the tanh function or the hyperbolic tangent function. The <strong>tanh</strong> function or the <strong>hyperbolic tangent</strong> function is actually mathematically a shifted version of the sigmoid function so as a you know sigmoid function just like that but shift it so that it now crosses a zero zero point and rescale. So it goes to minus one and plus one and it turns out that for hidden units if you let the function G of Z, $g(z)$, be equal to $tanh(z)$. This almost always works better than the sigmoid function because with values between plus one and minus one. The mean of the activations that come out of your head in there are closer to having a zero mean and so just as sometimes when you train a learning algorithm you might Center the data and have your data have zero mean. Using a $tanh$ function instead of a $sigmoid$ function kind of has the effect of centering your data, so that the mean of the data is close to the zero rather than maybe a 0.5 and this actually makes learning for the next layer a little bit easier we’ll say more about this in the second course when we talk about optimization algorithms as well but <strong>one takeaway is that I pretty much never use the sigmoid activation function anymore, the tanh function is almost always strictly superior</strong>. <strong>The only one exception</strong> is for the output layer because if Y is either 0 or 1 then it makes sense for $\hat{y}$ to be a number that you want to output. There’s between 0 and 1 rather than between minus 1 and 1 so the one exception where I would use the sigmoid activation function is when you’re using binary classification in which case you might use the sigmoid activation function for the output layer.</p>
<p>Now <strong>one of the downsides of both the sigmoid function and the tanh function</strong> is that if Z is either very large or very small then the gradient of the derivative or the slope of this function becomes very small so Z is very large or Z is very small the slope of the function you know ends up being close to zero and so this can slow down gradient descent, so one of the toys that is very popular in machine learning is what’s called the <strong>rectified linear unit, ReLU</strong>. so the value function looks like the down-left function of the above slide and the formula is a equals max of 0 comma Z, $max={0, z}$ , So the derivative is 1 so long as Z is positive and derivative or the slope is 0 when Z is negative. If you’re implementing this technically the derivative when Z is exactly 0 is not well-defined but when you implement is in the computer, the odds that you get exactly the equals 0 0 0 0 0 0 0 0 0 0 it is very small, so you don’t need to worry about it. In practice you could pretend a derivative when Z is equal to 0 you can pretend is either 1 or 0 and you can work just fine athlough the fact that is not differentiable.</p>
<p>So here are <strong>some rules of thumb for choosing activation functions</strong>: </p>
<p>If your output is 0 1 value if you’re I’m using binary classification then the sigmoid activation function is very natural for the output layer and then for all other units on ReLU or the rectified linear unit is increasingly the default choice of activation function. so if you’re not sure what to use for your head in there I would just use the relu activation function that’s what you see most people using these days although sometimes people also use the tannish activation function. <strong>One advantage of the ReLU</strong> is that the derivative is equal to zero when z is negative in practice this works just fine, but there is another version of the ReLU called the <strong>leaky ReLU</strong> will give you the formula on the next slide, but instead of it being zero** when z is negative it just takes a slight slope** like the down-right of the above slide. So this is called the leaky ReLU. This usually works better than the RELU activation function although it’s just not used as much in practice. Either one should be fine although if you had to pick one I usually just use the revenue and <strong>the advantage of both the ReLU and the leaky ReLU</strong> is that for a lot of the space of Z the derivative of the activation function the slope of the activation function is very different from zero and so in practice using the ReLU activation function your new network will often learn much faster than using the tanh or the sigmoid activation function and the main reason is that on this less of this effect of the slope of the function going to zero which slows down learning and I know that for half of the range of Z the slope of relu is zero but in practice enough of your hidden units will have Z greater than zero so learning can still be quite mask for most training examples.</p>
<p>On the above slide, the leaky ReLU is $max={0.01z, z}$. You might say you know why is that constant 0.01 well you can also make that another parameter of the learning algorithm and some people say that works even better but I hardly see people do that so but if you feel like trying it in your application you know please feel free to do so and and you can just see how it works and how long works and stick with it if it gives you good result.</p>
<p>One of the themes we’ll see in deep learning is that you often have a lot of different choices in how you code your neural network ranging from number of credit units to the chosen activation function to how you initialize the parameters which we’ll see later a lot of choices like that and it turns out that is sometimes difficult to get good guidelines for exactly what will work best for your problem so so these three courses. I’ll keep on giving you a sense of what I see in the industry in terms of what’s more or less popular but for your application with your applications idiosyncrasy. <strong>It’s actually very difficult to know in advance exactly what will work best so a piece of advice would be if you’re not sure which one of these activation functions work best you know try them all and then evaluate on like a holdout validation set or like a development set which we’ll talk about later and see which one works better and then go of that</strong> and I think that by testing these different choices for your application you’d be better at future proofing your neural network architecture against the idiosyncrasy in our problem as well evolutions of the algorithms rather than you know if I were to tell you always use a ReLU activation and don’t use anything else that just may or may not apply for whatever problem you end up working on you know either either in the near future.</p>
<h3 id="07-why-do-you-need-non-linear-activation-functions"><a href="#07-why-do-you-need-non-linear-activation-functions" class="headerlink" title="07_why-do-you-need-non-linear-activation-functions"></a>07_why-do-you-need-non-linear-activation-functions</h3><p>Why does your nerual network need a nonlinear activation function turns out that for your new network to compute interesting functions you do need to take a nonlinear activation function unless you want.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/10.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<p>No matter how many layers your neural network has always doing is just computing a linear activation function so you might as well not have any hidden layers some of the cases that briefly mentioned it turns out that if you have a linear activation function here and a sigmoid function here(on output layer) then this model is no more expressive than standard logistic regression without any hidden layer so I won’t bother to prove that but you could try to do so if you want but the take-home is that a linear hidden layer is more or less useless because <strong>the composition of two linear functions is itself a linear function</strong>.</p>
<p>there is <strong>just one place where you might use a linear activation function G of Z equals Z and that’s if you are doing machine learning on a regression problem so if Y is a real number</strong> so for example if you’re trying to predict housing prices so Y is a it’s not 0 1 but it’s a real number you know anywhere from zero dollars is a price of holes up to however expensive right house of kin I guess maybe however can be you know potentially millions of dollars so however however much houses cost in your data set but if Y takes on these real values then it might be OK to have a linear activation function here so that your output Y hat is also a real number going anywhere from minus infinity to plus infinity but then the hidden units should not use the new activation functions they could use relu or 10h or these you relu or maybe something else so the one place you might use a linear activation function others usually in the output layer but other than that using a linear activation function in a fitting layer except for some very special circumstances relating to compression that won’t want to talk about using a linear activation function is extremely rare oh and of course today actually predicting housing prices as you saw on the week 1 video because housing prices are all non-negative perhaps even then you can use a value activation function so that your outputs Y hat are all greater than or equal to 0.</p>
<p>so I hope that gives you a sense of why having a nonlinear activation function is a critical part of neural networks.</p>
<h3 id="08-derivatives-of-activation-functions"><a href="#08-derivatives-of-activation-functions" class="headerlink" title="08_derivatives-of-activation-functions"></a>08_derivatives-of-activation-functions</h3><p>When you implement back-propagation for your neural network you need to really compute the slope or the derivative of the activation functions so let’s take a look at our choices of activation functions and how you can compute the slope of these functions.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/11.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<p>Sometimes instead of writing this thing $\frac{dg(z)}{dz}$, the shorthand for the derivative is G prime of Z, $g’(z)$ . so G prime of Z in calculus the the little dash on top is called <strong>prime</strong> but so G prime of Z is a shorthand for the in calculus for the derivative of the function of G with respect to the input variable Z and then in a new network we have $a = g(z)$,  right equals this then this formula also simplifies to $a(1-a)$. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/12.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/13.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<h3 id="09-gradient-descent-for-neural-networks"><a href="#09-gradient-descent-for-neural-networks" class="headerlink" title="09_gradient-descent-for-neural-networks"></a>09_gradient-descent-for-neural-networks</h3><p>All right I think that’s be an exciting video in this video you see how to implement gradient descent for your neural network with one hidden layer in this video I’m going to just give you the equations you need to implement in order to get back propagation of the gradient descent working and then in the video after this one I’ll give some more intuition about why these particular equations are the accurate equations or the correct equations for computing the gradients you need for your neural network.</p>
<p>In logistic regression, what we want to do is to modify the parameters, W and B, in order to reduce this loss.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/15.png" alt=""></p>
<p>$$da = \frac{\partial{L}}{\partial{a}} =\frac{\partial \left{ {-(ylog(a)+(1-y)log(1-a))} \right} }{\partial{a}} = -\frac{y}{a} + \frac{1-y}{1-a} $$</p>
<p>$$dz=\frac{\partial{L}}{\partial{z}}=\frac{\partial{L}}{\partial{a}}\cdot \frac{\partial{a}}{\partial{z}} = \left(-\frac{y}{a} + \frac{1-y}{1-a}\right) \cdot a(1-a) = a - y$$</p>
<p>$$dw_1=\frac{\partial{L}}{\partial{w_1}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_1}} = x_1\cdot dz = x_1(a-y)$$</p>
<p>$$dw_2=\frac{\partial{L}}{\partial{w_2}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_2}} = x_2\cdot dz = x_2(a-y)$$</p>
<p>$$db=\frac{\partial{L}}{\partial{b}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{b}} = 1 \cdot dz = a - y$$</p>
<p>$$w_1 := w_1 - \alpha dw_1$$</p>
<p>$$w_2 := w_2 - \alpha dw_2$$</p>
<p>$$b := b - \alpha db$$</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/14.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/15.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<h3 id="11-random-initialization"><a href="#11-random-initialization" class="headerlink" title="11_random-initialization"></a>11_random-initialization</h3><p>When you change your neural network, it’s important to initialize the weights randomly. For logistic regression, it was okay to initialize the weights to zero. But for a neural network of initialize the weights to parameters to all zero and then applied gradient descent, it won’t work. Let’s see why.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/16.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<p>So you have here two input features, so n0=2, and two hidden units, so n1=2. And so the matrix associated with the hidden layer, w 1, is going to be two-by-two. Let’s say that you initialize it to all 0s, so 0 0 0 0, two-by-two matrix. And let’s say B1 is also equal to 0 0. It turns out initializing the bias terms b to 0 is actually okay, but initializing w to all 0s is a problem. So the problem with this formalization is that for any example you give it, you’ll have that a1,1 and a1,2, will be equal, right? So this activation and this activation will be the same, because both of these hidden units are computing exactly the same function. And then, when you compute backpropagation, it turns out that dz11 and dz12 will also be the same colored by symmetry, right? Both of these hidden unit will initialize the same way. Technically, for what I’m saying, I’m assuming that the outgoing weights or also identical. So that’s w2 is equal to 0 0. But if you initialize the neural network this way, then this hidden unit and this hidden unit are completely identical. Sometimes you say they’re completely symmetric, which just means that they’re completing exactly the same function. And by kind of a proof by induction, it turns out that after every single iteration of training your two hidden units are still computing exactly the same function. Since [INAUDIBLE] show that dw will be a matrix that looks like this. Where every row takes on the same value. So we perform a weight update. So when you perform a weight update, w1 gets updated as w1- alpha times dw. You find that w1, after every iteration, will have the first row equal to the second row. So it’s possible to construct a proof by induction that if you initialize all the ways, all the values of w to 0, then because both hidden units start off computing the same function. And both hidden the units have the same influence on the output unit, then after one iteration, that same statement is still true, the two hidden units are still symmetric. And therefore, by induction, after two iterations, three iterations and so on, no matter how long you train your neural network, both hidden units are still computing exactly the same function. And so in this case, there’s really no point to having more than one hidden unit. Because they are all computing the same thing. And of course, for larger neural networks, let’s say of three features and maybe a very large number of hidden units, a similar argument works to show that with a neural network like this. [INAUDIBLE] drawing all the edges, if you initialize the weights to zero, then all of your hidden units are symmetric. And no matter how long you’re upgrading the center, all continue to compute exactly the same function. So that’s not helpful, because you want the different hidden units to compute different functions. </p>
<p>The solution to this is to initialize your parameters randomly. So here’s what you do.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/03_shallow-neural-networks/17.png" alt="why-do-you-need-non-linear-activation-functions"></p>
<p>You can set w1 = np.random.randn. This generates a gaussian random variable (2,2). And then usually, you multiply this by very small number, such as 0.01. So you initialize it to very small random values. And then b, <strong>it turns out that b does not have the symmetry problem, what’s called the symmetry breaking problem</strong>. So it’s okay to initialize b to just zeros. Because so long as w is initialized randomly, you start off with the different hidden units computing different things. And so you no longer have this symmetry breaking problem. And then similarly, for w2, you’re going to initialize that randomly. And b2, you can initialize that to 0. <strong>So you might be wondering, where did the constant come from and why is it 0.01? Why not put the number 100 or 1000? Turns out that we usually prefer to initialize the weights to very small random values. Because if you are using a tanh or sigmoid activation function, or the other sigmoid, even just at the output layer. If the weights are too large, then when you compute the activation values, remember that z[1]=w1 x + b. And then a1 is the activation function applied to z1. So if w is very big, z will be very, or at least some values of z will be either very large or very small. And so in that case, you’re more likely to end up at these fat parts of the tanh function or the sigmoid function, where the slope or the gradient is very small. Meaning that gradient descent will be very slow. So learning was very slow. So just a recap, if w is too large, you’re more likely to end up even at the very start of training, with very large values of z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don’t have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue. But if you’re doing binary classification, and your output unit is a sigmoid function, then you just don’t want the initial parameters to be too large. So that’s why multiplying by 0.01 would be something reasonable to try, or any other small number</strong>. And same for w2, right? This can be random.random. I guess this would be 1 by 2 in this example, times 0.01. Missing an s there. So finally, it turns out that sometimes they can be better constants than 0.01. When you’re training a neural network with just one hidden layer, it is a relatively shallow neural network, without too many hidden layers. Set it to 0.01 will probably work okay. But when you’re training a very very deep neural network, then you might want to pick a different constant than 0.01. And in next week’s material, we’ll talk a little bit about how and when you might want to choose a different constant than 0.01. But either way, it will usually end up being a relatively small number. So that’s it for this week’s videos. You now know how to set up a neural network of a hidden layer, initialize the parameters, make predictions using. As well as compute derivatives andimplement gradient descent, using backprop. </p>
<p>So that,you should be able to do the quizzes, as well as this week’s programming exercises. Best of luck with that. I hope you have fun with the problem exercise, and look forward to seeing you in the week four materials.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/02/greek-letters/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/02/greek-letters/" class="post-title-link" itemprop="url">Greek Letters</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-02 18:06:02" itemprop="dateCreated datePublished" datetime="2018-02-02T18:06:02+05:30">2018-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:33" itemprop="dateModified" datetime="2020-04-06T20:25:33+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <img src = "https://upload.wikimedia.org/wikipedia/en/3/37/Greek_ext.png">
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/02/02_neural-networks-basics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/02/02_neural-networks-basics/" class="post-title-link" itemprop="url">02_logistic-regression-as-a-neural-network</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-02 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-02T00:00:00+05:30">2018-02-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:18" itemprop="dateModified" datetime="2020-04-06T20:25:18+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the 2nd week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="01-logistic-regression-as-a-neural-network"><a href="#01-logistic-regression-as-a-neural-network" class="headerlink" title="01_logistic-regression-as-a-neural-network"></a>01_logistic-regression-as-a-neural-network</h2><h3 id="01-binary-classification"><a href="#01-binary-classification" class="headerlink" title="01_binary-classification"></a>01_binary-classification</h3><h4 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h4><p>In a binary classification problem, the result is a discrete value output. For example </p>
<ul>
<li>account hacked (1) or compromised (0)</li>
<li>a tumor malign (1) or benign (0)</li>
</ul>
<p><strong>Example: Cat vs Non-Cat</strong><br>The goal is to train a classifier that the input is an image represented by a feature vector, $x$, and predicts whether the corresponding label $y$ is 1 or 0. In this case, whether this is a cat image (1) or a non-cat image (0).</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/1.png" alt=""></p>
<p>An image is store in the computer in three separate matrices corresponding to the Red, Green, and Blue color channels of the image. The three matrices have the same size as the image, for example, the resolution of the cat image is 64 pixels X 64 pixels, the three matrices (RGB) are 64 X 64 each.<br>The value in a cell represents the pixel intensity which will be used to create a feature vector of ndimension. In pattern recognition and machine learning, a feature vector represents an object, in this case, a cat or no cat.<br>To create a feature vector, $x$, the pixel intensity values will be “unroll” or “reshape” for each color. The dimension of the input feature vector $x$ is $ n_x = 64 \times 64 \times 3 = 12 288.$<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/2.png" alt=""></p>
<h4 id="notation"><a href="#notation" class="headerlink" title="notation"></a>notation</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/3.png" alt=""></p>
<h3 id="02-Logistic-Regression"><a href="#02-Logistic-Regression" class="headerlink" title="02_Logistic Regression"></a>02_Logistic Regression</h3><p>Logistic regression is a learning algorithm used in a supervised learning problem when the output $y$ are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data.</p>
<h4 id="Example-Cat-vs-No-cat"><a href="#Example-Cat-vs-No-cat" class="headerlink" title="Example: Cat vs No - cat"></a>Example: Cat vs No - cat</h4><p>Given an image represented by a feature vector $x$, the algorithm will evaluate the probability of a cat being in that image.</p>
<p>$$\text{Civen }x, \hat{y}=P(y=1|x), \text{where } 0 \le \hat{y} \le 1$$</p>
<p>The parameters used in Logistic regression are:<br>• The input features vector: $x ∈ ℝ^{n_x}$, where $n_x$ is the number of features<br>• The training label: $y ∈ 0,1$<br>• The weights: $w ∈ ℝ^{n_x}$ , where $n_x$ is the number of features<br>• The threshold: $b ∈ ℝ$<br>• The output: $\hat{y} = \sigma(w^Tx+b)$<br>• Sigmoid function: $s = \sigma(w^Tx+b) = \sigma(z)= \frac{1}{1+e^{-z}}$</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/5.png" alt=""></p>
<p>$(w^Tx +b )$ is a linear function $(ax + b)$, but since we are looking for a probability constraint between [0,1], the sigmoid function is used. The function is bounded between [0,1] as shown in the graph above.<br>Some observations from the graph:<br>• If $z$ is a large positive number, then $\sigma(z) = 1$<br>• If $z$ is small or large negative number, then $\sigma(z) = 0$<br>• If $z$ = 0, then $\sigma(z) = 0.5$</p>
<h4 id="notation-1"><a href="#notation-1" class="headerlink" title="notation"></a>notation</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/4.png" alt=""></p>
<h3 id="03-logistic-regression-cost-function"><a href="#03-logistic-regression-cost-function" class="headerlink" title="03_logistic-regression-cost-function"></a>03_logistic-regression-cost-function</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/6.png" alt=""></p>
<h3 id="04-gradient-descent"><a href="#04-gradient-descent" class="headerlink" title="04_gradient-descent"></a>04_gradient-descent</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/7.png" alt=""></p>
<h3 id="05-06-derivatives"><a href="#05-06-derivatives" class="headerlink" title="05_06_derivatives"></a>05_06_derivatives</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/8.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/9.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/10.png" alt=""></p>
<h3 id="07-computation-graph"><a href="#07-computation-graph" class="headerlink" title="07_computation-graph"></a>07_computation-graph</h3><p>You’ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives. The computation graph explains why it is organized this way.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/11.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/12.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/13.png" alt=""></p>
<h3 id="09-logistic-regression-gradient-descent"><a href="#09-logistic-regression-gradient-descent" class="headerlink" title="09_logistic-regression-gradient-descent"></a>09_logistic-regression-gradient-descent</h3><p>Welcome back. In this video, we’ll talk about how to compute derivatives for you to implement gradient descent for logistic regression. <strong>The key takeaways will be what you need to implement. That is, the key equations you need in order to implement gradient descent for logistic regression</strong>. In this video, I want to do this computation using the computation graph. I have to admit, using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about fully-fledged neural networks. To that, let’s dive into gradient descent for logistic regression. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/14.png" alt=""><br>In logistic regression, what we want to do is to modify the parameters, W and B, in order to reduce this loss.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/15.png" alt=""></p>
<p>$$da = \frac{\partial{L}}{\partial{a}} =\frac{\partial \left{ {-(ylog(a)+(1-y)log(1-a))} \right} }{\partial{a}} = -\frac{y}{a} + \frac{1-y}{1-a} $$</p>
<p>$$dz=\frac{\partial{L}}{\partial{z}}=\frac{\partial{L}}{\partial{a}}\cdot \frac{\partial{a}}{\partial{z}} = \left(-\frac{y}{a} + \frac{1-y}{1-a}\right) \cdot a(1-a) = a - y$$</p>
<p>$$dw_1=\frac{\partial{L}}{\partial{w_1}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_1}} = x_1\cdot dz = x_1(a-y)$$</p>
<p>$$dw_2=\frac{\partial{L}}{\partial{w_2}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_2}} = x_2\cdot dz = x_2(a-y)$$</p>
<p>$$db=\frac{\partial{L}}{\partial{b}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{b}} = 1 \cdot dz = a - y$$</p>
<p>$$w_1 := w_1 - \alpha dw_1$$</p>
<p>$$w_2 := w_2 - \alpha dw_2$$</p>
<p>$$b := b - \alpha db$$</p>
<h3 id="10-gradient-descent-on-m-examples"><a href="#10-gradient-descent-on-m-examples" class="headerlink" title="10_gradient-descent-on-m-examples"></a>10_gradient-descent-on-m-examples</h3><p>in a previous video you saw how to compute derivatives and implement gradient descent with respect to just one training example for religious regression now we want to do it for m training examples.</p>
<h4 id="one-single-step-gradient-descent"><a href="#one-single-step-gradient-descent" class="headerlink" title="one single step gradient descent"></a>one single step gradient descent</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/16.png" alt="one single step gradient descent"></p>
<h2 id="02-python-and-vectorization"><a href="#02-python-and-vectorization" class="headerlink" title="02_python-and-vectorization"></a>02_python-and-vectorization</h2><h3 id="01-vectorization"><a href="#01-vectorization" class="headerlink" title="01_vectorization"></a>01_vectorization</h3><p>Welcome back. Vectorization is basically the art of getting rid of explicit folders in your code. In the deep learning era safety in deep learning in practice, you often find yourself training on relatively large data sets, because that’s when deep learning algorithms tend to shine. And so, it’s important that your code very quickly because otherwise, if it’s running on a big data set, your code might take a long time to run then you just find yourself waiting a very long time to get the result. So in the deep learning era, I think the ability to perform vectorization has become a key skill. </p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/18.png" alt="what is vectorization"></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/17.png" alt="a example of the difference of run time between vectorization implementation and non-vectorization implementation"><br>Yeah. Vectorize version 1.5 milliseconds seconds and the four loop. So 481 milliseconds, again, about <strong>300 times slower</strong> to do the explicit four loop. If the engine x slows down, it’s the difference between your code taking maybe one minute to run versus taking say five hours to run. And when you are implementing deep learning algorithms, you can really get a result back faster. It will be much faster if you vectorize your code. Some of you might have heard that a lot of <strong>scaleable deep learning implementations</strong> are done on a GPU or a graphics processing unit. But all the demos I did just now in the Jupiter notebook where actually on the CPU. And it turns out that both GPU and CPU have parallelization instructions. They’re sometimes called <strong>SIMD instructions</strong>. This stands for a <strong>single instruction multiple data</strong>. But what this basically means is that, if you use built-in functions such as this np.function or other functions that don’t require you explicitly implementing a for loop. It enables Phyton Pi to take much better advantage of parallelism to do your computations much faster. And this is true both computations on CPUs and computations on GPUs. It’s just that GPUs are remarkably good at these SIMD calculations but CPU is actually also not too bad at that. Maybe just not as good as GPUs. You’re seeing how vectorization can significantly speed up your code. <strong>The rule of thumb to remember is whenever possible, avoid using explicit four loops.</strong></p>
<h3 id="02-more-vectorization-examples"><a href="#02-more-vectorization-examples" class="headerlink" title="02_more-vectorization-examples"></a>02_more-vectorization-examples</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/20.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/21.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/22.png" alt=""></p>
<h3 id="03-vectorizing-logistic-regression"><a href="#03-vectorizing-logistic-regression" class="headerlink" title="03_vectorizing-logistic-regression"></a>03_vectorizing-logistic-regression</h3><p>We have talked about how vectorization lets you speed up your code significantly. In this video, we’ll talk about how you can vectorize the implementation of logistic regression, so they can process an entire training set, that is implement a single elevation of grading descent with respect to an entire training set without using even a single explicit for loop. I’m super excited about this technique, and when we talk about neural networks later without using even a single explicit for loop.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/23.png" alt="vectorization implementation of logistic regression of forward of propagation"></p>
<p>Here are details of <a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">python broadcasting</a></p>
<h3 id="04-vectorizing-logistic-regressions-gradient-output"><a href="#04-vectorizing-logistic-regressions-gradient-output" class="headerlink" title="04_vectorizing-logistic-regressions-gradient-output"></a>04_vectorizing-logistic-regressions-gradient-output</h3><p>In the previous video, you saw how you can use vectorization to compute their predictions. The lowercase a’s for an entire training set O at the same time. In this video, you see <strong>how you can use vectorization to also perform the gradient computations for all M training samples</strong>. Again, all sort of at the same time. And then at the end of this video, we’ll put it all together and show how you can derive a very efficient implementation of logistic regression.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/24.png" alt="vectorizing-logistic-regressions-gradient-output"></p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/25.png" alt="vectorizing-logistic-regressions-gradient-output"></p>
<h3 id="05-broadcasting-in-python"><a href="#05-broadcasting-in-python" class="headerlink" title="05_broadcasting-in-python"></a>05_broadcasting-in-python</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/26.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/29.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/27.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/28.png" alt=""></p>
<p><strong>Summary: Python or Numpy automatically expands two arrays or numbers to the same dimensions and operate element-wise.</strong></p>
<h3 id="06-a-note-on-python-numpy-vectors"><a href="#06-a-note-on-python-numpy-vectors" class="headerlink" title="06_a-note-on-python-numpy-vectors"></a>06_a-note-on-python-numpy-vectors</h3><p>The ability of python to allow you to use broadcasting operations and more generally, <strong>the great flexibility of the python numpy program language is, I think, both a strength as well as a weakness of the programming language</strong>. I think it’s a strength because they create expressivity of the language. A great flexibility of the language lets you get a lot done even with just a single line of code. But there’s also weakness because with broadcasting and <strong>this great amount of flexibility, sometimes it’s possible you can introduce very subtle bugs or very strange looking bugs</strong>, if you’re not familiar with all of the intricacies of how broadcasting and how features like broadcasting work. <strong>For example, if you take a column vector and add it to a row vector, you would expect it to throw up a dimension mismatch or type error or something. But you might actually get back a matrix as a sum of a row vector and a column vector. So there is an internal logic to these strange effects of Python.</strong> But if you’re not familiar with Python, I’ve seen some students have very strange, very hard to find bugs. So what I want to do in this video is share with you some couple tips and tricks that have been very useful for me to eliminate or simplify and eliminate all the strange looking bugs in my own code. <strong>And I hope that with these tips and tricks, you’ll also be able to much more easily write bug-free, python and numpy code</strong>.</p>
<p>To illustrate one of the less intuitive effects of Python-Numpy, especially how you construct vectors in Python-Numpy, let me do a <strong>quick demo</strong>.</p>
<h4 id="one-rank-array"><a href="#one-rank-array" class="headerlink" title="one rank array"></a>one rank array</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/30.png" alt=""></p>
<h4 id="practical-tips"><a href="#practical-tips" class="headerlink" title="practical tips"></a>practical tips</h4><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/31.png" alt=""></p>
<h3 id="07-quick-tour-of-jupyter-ipython-notebooks"><a href="#07-quick-tour-of-jupyter-ipython-notebooks" class="headerlink" title="07_quick-tour-of-jupyter-ipython-notebooks"></a>07_quick-tour-of-jupyter-ipython-notebooks</h3><p>With everything you’ve learned, you’re just about ready to tackle your first programming assignment. Before you do that, let me just give you a quick tour of iPython notebooks in Coursera.</p>
<p>Please see <a href="">the video</a> to get details.</p>
<h3 id="08-explanation-of-logistic-regression-cost-function-optional"><a href="#08-explanation-of-logistic-regression-cost-function-optional" class="headerlink" title="08_explanation-of-logistic-regression-cost-function-optional"></a>08_explanation-of-logistic-regression-cost-function-optional</h3><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/32.png" alt=""><br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/02_neural-networks-basics/33.png" alt=""></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/02/01/01_introduction-to-deep-learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/02/01/01_introduction-to-deep-learning/" class="post-title-link" itemprop="url">01_introduction-to-deep-learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-02-01 00:00:00" itemprop="dateCreated datePublished" datetime="2018-02-01T00:00:00+05:30">2018-02-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:13" itemprop="dateModified" datetime="2020-04-06T20:25:13+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>This is my personal note at the first week after studying the course <a href="https://www.coursera.org/learn/neural-networks-deep-learning/" target="_blank" rel="noopener">neural-networks-deep-learning</a> and the copyright belongs to <a href="https://www.deeplearning.ai/" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<h2 id="01-introduction-to-deep-learning"><a href="#01-introduction-to-deep-learning" class="headerlink" title="01_introduction-to-deep-learning"></a>01_introduction-to-deep-learning</h2><h3 id="01-What-is-neural-network"><a href="#01-What-is-neural-network" class="headerlink" title="01_What is neural network?"></a>01_What is neural network?</h3><p>It is a powerful learning algorithm inspired by how the brain works.</p>
<h4 id="Example-1-–-single-neural-network"><a href="#Example-1-–-single-neural-network" class="headerlink" title="Example 1 – single neural network"></a>Example 1 – single neural network</h4><p>Given data about the size of houses on the real estate market and you want to fit a function that will predict their price. It is a linear regression problem because the price as a function of size is a continuous output.<br>We know the prices can never be negative so we are creating a function called <strong>Rectified Linear Unit</strong> (ReLU) which starts at zero.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/1.png" alt="example of a neuron"><br>The input is the size of the house (x)<br>The output is the price (y)<br>The “neuron” implements the function ReLU (blue line)</p>
<h4 id="Example-2-–-Multiple-neural-network"><a href="#Example-2-–-Multiple-neural-network" class="headerlink" title="Example 2 – Multiple neural network"></a>Example 2 – Multiple neural network</h4><p>The price of a house can be affected by other features such as size, number of bedrooms, zip code andwealth. The role of the neural network is to predicted the price and it will automatically generate the hidden units. We only need to give the inputs x and the output y.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/2.png" alt="example of simple neural network"></p>
<h3 id="02-supervised-learning-with-neural-networks"><a href="#02-supervised-learning-with-neural-networks" class="headerlink" title="02_supervised-learning-with-neural-networks"></a>02_supervised-learning-with-neural-networks</h3><h4 id="Supervised-learning-for-Neural-Network"><a href="#Supervised-learning-for-Neural-Network" class="headerlink" title="Supervised learning for Neural Network"></a>Supervised learning for Neural Network</h4><p>In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.<br>Supervised learning problems are categorized into “<strong>regression</strong>“ and “<strong>classification</strong>“ problems. In a regression problem, we are trying to predict results within a <strong>continuous outpu</strong>t, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a <strong>discrete output</strong>. In other words, we are trying to map input variables into discrete categories.<br>Here are some examples of supervised learning.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/3.png" alt="some examples of supervised learning"></p>
<p>There are different types of neural network, for example <strong>Convolution Neural Network</strong> (CNN) used often for image application and <strong>Recurrent Neural Network</strong> (RNN) used for one-dimensional sequence data such as translating English to Chinses or a temporal component such as text transcript. As for the autonomous driving, it is a hybrid neural network architecture.</p>
<h4 id="Structured-vs-unstructured-data"><a href="#Structured-vs-unstructured-data" class="headerlink" title="Structured vs unstructured data"></a>Structured vs unstructured data</h4><p>Structured data refers to things that has a defined meaning such as price, age whereas unstructured data refers to thing like pixel, raw audio, text.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/4.png" alt="Structured data vs Unstructured data"></p>
<h3 id="03-why-is-deep-learning-taking-off"><a href="#03-why-is-deep-learning-taking-off" class="headerlink" title="03_why-is-deep-learning-taking-off"></a>03_why-is-deep-learning-taking-off</h3><h4 id="Why-is-deep-learning-taking-off"><a href="#Why-is-deep-learning-taking-off" class="headerlink" title="Why is deep learning taking off?"></a>Why is deep learning taking off?</h4><p>Deep learning is taking off due to a large amount of data available through the digitization of the society, faster computation and innovation in the development of neural network algorithm.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/5.png" alt=""></p>
<p>Two things have to be considered to get to the high level of performance:</p>
<ol>
<li>Being able to train a big enough neural network</li>
<li>Huge amount of labeled data</li>
</ol>
<p>The process of training a neural network is iterative.</p>
<p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/deeplearning.ai/neural-networks-deep-learning/01_introduction-to-deep-learning/6.png" alt=""></p>
<p>It could take a good amount of time to train a neural network, which affects your productivity. Faster computation helps to iterate and improve new algorithm.</p>
<h3 id="04-about-this-course"><a href="#04-about-this-course" class="headerlink" title="04_about-this-course"></a>04_about-this-course</h3><h4 id="Outline-of-this-Course"><a href="#Outline-of-this-Course" class="headerlink" title="Outline of this Course"></a>Outline of this Course</h4><ul>
<li>Week 1: Introduction</li>
<li>Week 2: Basics of Neural Network programming </li>
<li>Week 3: One hidden layer Neural Networks</li>
<li>Week 4: Deep Neural Networks</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://snakecoding.com/2018/01/20/summary_of_ml-coursera-andrew-ng/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Karan">
      <meta itemprop="description" content="Refuse to Fall">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Machine Learning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2018/01/20/summary_of_ml-coursera-andrew-ng/" class="post-title-link" itemprop="url">summary of Machine Learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2018-01-20 00:00:00" itemprop="dateCreated datePublished" datetime="2018-01-20T00:00:00+05:30">2018-01-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-04-06 20:25:09" itemprop="dateModified" datetime="2020-04-06T20:25:09+05:30">2020-04-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="The-content-table-of-Machine-Learning"><a href="#The-content-table-of-Machine-Learning" class="headerlink" title="The content table of Machine Learning"></a>The content table of Machine Learning</h2><p><a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="noopener">This course</a> is a coursera version teached by Andrew NG, AP of Stanford University, which corresponds to the full-time campus version CS229 at Stanford university, that is increasingly difficult version.</p>
<p><a href="/2018/01/01/01_what-is-machine-learning/">01_introduction</a><br><a href="/2018/01/02/02_linear-regression-with-one-variable/">02_linear-regression-with-one-variable</a><br><a href="/2018/01/03/03_linear-algebra-review/">03_linear-algebra-review</a><br><a href="/2018/01/04/04_linear-regression-with-multiple-variables/">04_linear-regression-with-multiple-variables</a><br><a href="/2018/01/01/05_octave-matlab-tutorial/">05_octave-matlab-tutorial</a><br><a href="/2018/01/06/06_logistic-regression/">06_logistic-regression</a><br><a href="/2018/01/07/07_regularization/">07_regularization</a><br><a href="/2018/01/08/08_neural-networks-representation/">08_neural-networks-representation</a><br><a href="/2018/01/09/09_neural-networks-learning/">09_neural-networks-learning</a><br><a href="/2018/01/10/10_advice-for-applying-machine-learning/">10_advice-for-applying-machine-learning</a><br><a href="/2018/01/12/12_support-vector-machines/">11_machine-learning-system-design</a><br><a href="/2018/01/12/12_support-vector-machines/">12_support-vector-machines</a><br><a href="/2018/01/13/13_unsupervised-learning/">13_unsupervised-learning</a><br><a href="/2018/01/14/14_dimensionality-reduction/">14_dimensionality-reduction</a><br><a href="/2018/01/15/15_anomaly-detection/">15_anomaly-detection</a><br><a href="/2018/01/16/16_recommender-systems/">16_recommender-systems</a><br><a href="/2018/01/17/17_large-scale-machine-learning/">17_large-scale-machine-learning</a><br><a href="/2018/01/18/18_application-example-photo-ocr/">18_application-example-photo-ocr</a></p>
<h2 id="my-class-assignments"><a href="#my-class-assignments" class="headerlink" title="my class assignments"></a>my class assignments</h2><p><a href="https://github.com/SnailDove/Coursera-Standford-ML" target="_blank" rel="noopener">ex1 -&gt; ex8</a></p>
<h2 id="Conclusion-of-Andrew-NG"><a href="#Conclusion-of-Andrew-NG" class="headerlink" title="Conclusion of Andrew NG"></a>Conclusion of Andrew NG</h2><p>Welcome to the final video of this Machine Learning class. We’ve been through a lot of different videos together. In this video I would like to just quickly summarize the main topics of this course and then say a few words at the end and that will wrap up the class. So what have we done? </p>
<h2 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h2><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/18/40.png" alt=""><br>In this class we spent a lot of time talking about supervised learning algorithms like linear regression, logistic regression, neural networks, SVMs. for problems where you have labelled data and labelled examples like x(i), y(i) And we also spent quite a lot of time talking about unsupervised learning like K-means clustering, Principal Components Analysis for dimensionality reduction and Anomaly Detection algorithms for when you have only unlabelled data x(i) Although Anomaly Detection can also use some labelled data to evaluate the algorithm. We also spent some time talking about special applications or special topics like Recommender Systems and large scale machine learning systems including parallelized and rapid-use systems as well as some special applications like sliding windows object classification for computer vision. And finally we also spent a lot of time talking about different aspects of, sort of, advice on building a machine learning system. And this involved both trying to understand what is it that makes a machine learning algorithm work or not work. So we talked about things like bias and variance, and how regularization can help with some variance problems. And we also spent a little bit of time talking about this question of how to decide what to work on next. So, how to prioritize how you spend your time when you’re developing a machine learning system. So we talked about evaluation of learning algorithms, evaluation metrics like  precision recall, F1 score as well as practical aspects of evaluation like the training, cross-validation and test sets. And we also spent a lot of time talking about debugging learning algorithms and making sure the learning algorithm is working. So we talked about diagnostics like learning curves and also talked about things like error analysis and ceiling analysis. And so all of these were different tools for helping you to decide what to do next and how to spend your valuable time when you’re developing a machine learning system. And in addition to having the tools of machine learning at your disposal so knowing the tools of machine learning like supervised learning and unsupervised learning and so on, I hope that you now not only have the tools, but that you know how to apply these tools really well to build powerful machine learning systems. So, that’s it. Those were the topics of this class and if you worked all the way through this course you should now consider yourself an expert in machine learning. As you know, machine learning is a technology that’s having huge impact on science, technology and industry. And you’re now well qualified to use these tools of machine learning to great effect. </p>
<h2 id="THX"><a href="#THX" class="headerlink" title="THX"></a>THX</h2><p>I hope that many of you in this class will find ways to use machine learning to build cool systems and cool applications and cool products. And I hope that you find ways to use machine learning not only to make <i>your</i> life better but maybe someday to use it to make many other people’s life better as well. I also wanted to let you know that this class has been great fun for me to teach. So, thank you for that. And before wrapping up, there’s just one last thing I wanted to say. Which is that: It was maybe not so long ago, that I was a student myself. And even today, you know, I still try to take different courses when I have time to try to learn new things. And so I know how time-consuming it is to learn this stuff. I know that you’re probably a busy person with many, many other things going on in your life. And so the fact that you still found the time or took the time to watch these videos and, you know, many of these videos just went on for hours, right? And the fact many of you took the time to go through the review questions and that many of you took the time to work through the programming exercises. And these were long and complicate programming exercises. I wanted to say thank you for that. And I know that many of you have worked hard on this class and that many of you have put a lot of time into this class, that many of you have put a lot of yourselves into this class. So I hope that you also got a lot of out this class. And I wanted to say: Thank you very much for having been a student in this class.<br><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/18/41.png" alt=""></p>
<h2 id="My-Grades"><a href="#My-Grades" class="headerlink" title="My Grades"></a>My Grades</h2><p><img src="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/MyTotalGrade.png" alt="Overall grade of this course"><br>Here are <a href="http://q6gm8fomw.bkt.clouddn.com/gitpage/ml-andrew-ng/ML-Grades.pdf" target="_blank" rel="noopener">the details of my grades</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Karan"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Karan</p>
  <div class="site-description" itemprop="description">Refuse to Fall</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">60</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yourname" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:yourname@gmail.com" title="E-Mail → mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://youtube.com/yourname" title="YouTube → https:&#x2F;&#x2F;youtube.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://instagram.com/yourname" title="Instagram → https:&#x2F;&#x2F;instagram.com&#x2F;yourname" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="/www.massivefile.com" title="www.massivefile.com">DataBases</a>
        </li>
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Karan</span>
</div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>

<script src="/js/utils.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


    </div>
</body>
</html>
