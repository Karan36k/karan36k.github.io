<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>Machine Learning</title>
  
  <subtitle>Data Science</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://massivefile.com/"/>
  <updated>2020-04-12T15:02:34.893Z</updated>
  <id>https://massivefile.com/</id>
  
  <author>
    <name>Karan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Quiz 1 || Neural Networks and Deep Learning (Week 1)</title>
    <link href="https://massivefile.com/Quiz01/"/>
    <id>https://massivefile.com/Quiz01/</id>
    <published>2020-04-12T00:56:53.000Z</published>
    <updated>2020-04-12T15:02:34.893Z</updated>
    
    <content type="html"><![CDATA[<p>title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 1) Neural Networks and Deep Learning (Week 1)<br><b>Note - These are my notes on the first course of DeepLearning.ai by AndrewNg</b></p><a id="more"></a><div class="pdfobject-container" data-target="../DownloadableData/quiz1.pdf" data-height="500px"></div><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 1) Neural Networks and Deep Learning (Week 1)&lt;br&gt;&lt;b&gt;Note - These are my notes on the first course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="new post" scheme="https://massivefile.com/categories/new-post/"/>
    
      <category term="2020" scheme="https://massivefile.com/categories/new-post/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/categories/new-post/2020/Quiz/"/>
    
    
      <category term="new post" scheme="https://massivefile.com/tags/new-post/"/>
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/tags/Quiz/"/>
    
  </entry>
  
  <entry>
    <title>Quiz 2 || Neural Networks and Deep Learning (Week 2)</title>
    <link href="https://massivefile.com/quiz2/"/>
    <id>https://massivefile.com/quiz2/</id>
    <published>2020-04-12T00:56:53.000Z</published>
    <updated>2020-04-12T15:02:48.178Z</updated>
    
    <content type="html"><![CDATA[<p>title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 2) Neural Networks and Deep Learning (Week 2)<br><b>Note - These are my notes on the first course of DeepLearning.ai by AndrewNg</b></p><a id="more"></a><div class="pdfobject-container" data-target="../DownloadableData/quiz2.pdf" data-height="500px"></div><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 2) Neural Networks and Deep Learning (Week 2)&lt;br&gt;&lt;b&gt;Note - These are my notes on the first course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="new post" scheme="https://massivefile.com/categories/new-post/"/>
    
      <category term="2020" scheme="https://massivefile.com/categories/new-post/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/categories/new-post/2020/Quiz/"/>
    
    
      <category term="new post" scheme="https://massivefile.com/tags/new-post/"/>
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/tags/Quiz/"/>
    
  </entry>
  
  <entry>
    <title>Logistic Regression with a Neural Network mindset || Neural Networks and Deep Learning (Week 2)</title>
    <link href="https://massivefile.com/Logistic_Regression_with_a_Neural_Network_mindset/"/>
    <id>https://massivefile.com/Logistic_Regression_with_a_Neural_Network_mindset/</id>
    <published>2020-04-12T00:56:53.000Z</published>
    <updated>2020-04-12T17:42:25.580Z</updated>
    
    <content type="html"><![CDATA[<p>Logistic Regression with a Neural Network mindset Assignment || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)<br><b>Note - These are my notes on the second course of DeepLearning.ai by AndrewNg</b><br>All Credits to the teacher <b>andrew ng </b> :)</p><a id="more"></a><h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1> <p><div class="lev1 toc-item"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" data-toc-modified-id="Logistic-Regression-with-a-Neural-Network-mindset-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Logistic Regression with a Neural Network mindset</a></div><div class="lev2 toc-item"><a href="#1---Packages" data-toc-modified-id="1---Packages-11"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>1 - Packages</a></div><div class="lev1 toc-item"><a href="#load-lr_utils.py" data-toc-modified-id="load-lr_utils.py-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>load lr_utils.py</a></div><div class="lev2 toc-item"><a href="#2---Overview-of-the-Problem-set" data-toc-modified-id="2---Overview-of-the-Problem-set-21"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>2 - Overview of the Problem set</a></div><div class="lev2 toc-item"><a href="#3---General-Architecture-of-the-learning-algorithm" data-toc-modified-id="3---General-Architecture-of-the-learning-algorithm-22"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>3 - General Architecture of the learning algorithm</a></div><div class="lev2 toc-item"><a href="#4---Building-the-parts-of-our-algorithm" data-toc-modified-id="4---Building-the-parts-of-our-algorithm-23"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>4 - Building the parts of our algorithm</a></div><div class="lev3 toc-item"><a href="#4.1---Helper-functions" data-toc-modified-id="4.1---Helper-functions-231"><span class="toc-item-num">2.3.1&nbsp;&nbsp;</span>4.1 - Helper functions</a></div><div class="lev3 toc-item"><a href="#4.2---Initializing-parameters" data-toc-modified-id="4.2---Initializing-parameters-232"><span class="toc-item-num">2.3.2&nbsp;&nbsp;</span>4.2 - Initializing parameters</a></div><div class="lev3 toc-item"><a href="#4.3---Forward-and-Backward-propagation" data-toc-modified-id="4.3---Forward-and-Backward-propagation-233"><span class="toc-item-num">2.3.3&nbsp;&nbsp;</span>4.3 - Forward and Backward propagation</a></div><div class="lev3 toc-item"><a href="#d)-Optimization" data-toc-modified-id="d)-Optimization-234"><span class="toc-item-num">2.3.4&nbsp;&nbsp;</span>d) Optimization</a></div><div class="lev2 toc-item"><a href="#5---Merge-all-functions-into-a-model" data-toc-modified-id="5---Merge-all-functions-into-a-model-24"><span class="toc-item-num">2.4&nbsp;&nbsp;</span>5 - Merge all functions into a model</a></div><div class="lev2 toc-item"><a href="#6---Further-analysis-(optional/ungraded-exercise)" data-toc-modified-id="6---Further-analysis-(optional/ungraded-exercise)-25"><span class="toc-item-num">2.5&nbsp;&nbsp;</span>6 - Further analysis (optional/ungraded exercise)</a></div><div class="lev4 toc-item"><a href="#Choice-of-learning-rate" data-toc-modified-id="Choice-of-learning-rate-2501"><span class="toc-item-num">2.5.0.1&nbsp;&nbsp;</span>Choice of learning rate</a></div><div class="lev2 toc-item"><a href="#7---Test-with-your-own-image-(optional/ungraded-exercise)" data-toc-modified-id="7---Test-with-your-own-image-(optional/ungraded-exercise)-26"><span class="toc-item-num">2.6&nbsp;&nbsp;</span>7 - Test with your own image (optional/ungraded exercise)</a></div><h1 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a>Logistic Regression with a Neural Network mindset</h1><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p><strong>Instructions:</strong></p><ul><li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li></ul><p><strong>You will learn to:</strong></p><ul><li>Build the general architecture of a learning algorithm, including:<ul><li>Initializing parameters</li><li>Calculating the cost function and its gradient</li><li>Using an optimization algorithm (gradient descent) </li></ul></li><li>Gather all three functions above into a main model function, in the right order.</li></ul><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>First, let’s run the cell below to import all the packages that you will need during this assignment. </p><ul><li><span class="exturl" data-url="aHR0cHM6Ly93d3cubnVtcHkub3Jn">numpy<i class="fa fa-external-link-alt"></i></span> is the fundamental package for scientific computing with Python.</li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuaDVweS5vcmc=">h5py<i class="fa fa-external-link-alt"></i></span> is a common package to interact with a dataset that is stored on an H5 file.</li><li><span class="exturl" data-url="aHR0cHM6Ly9tYXRwbG90bGliLm9yZw==">matplotlib<i class="fa fa-external-link-alt"></i></span> is a famous library to plot graphs in Python.</li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cucHl0aG9ud2FyZS5jb20vcHJvZHVjdHMvcGlsLw==">PIL<i class="fa fa-external-link-alt"></i></span> and <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpcHkub3JnLw==">scipy<i class="fa fa-external-link-alt"></i></span> are used here to test your model with your own picture at the end.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">import</span> skimage</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="load-lr-utils-py"><a href="#load-lr-utils-py" class="headerlink" title="load lr_utils.py"></a>load lr_utils.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    train_dataset = h5py.File(<span class="string">'datasets/train_catvnoncat.h5'</span>, <span class="string">"r"</span>)</span><br><span class="line">    train_set_x_orig = np.array(train_dataset[<span class="string">"train_set_x"</span>][:]) <span class="comment"># your train set features</span></span><br><span class="line">    train_set_y_orig = np.array(train_dataset[<span class="string">"train_set_y"</span>][:]) <span class="comment"># your train set labels</span></span><br><span class="line"></span><br><span class="line">    test_dataset = h5py.File(<span class="string">'datasets/test_catvnoncat.h5'</span>, <span class="string">"r"</span>)</span><br><span class="line">    test_set_x_orig = np.array(test_dataset[<span class="string">"test_set_x"</span>][:]) <span class="comment"># your test set features</span></span><br><span class="line">    test_set_y_orig = np.array(test_dataset[<span class="string">"test_set_y"</span>][:]) <span class="comment"># your test set labels</span></span><br><span class="line"></span><br><span class="line">    classes = np.array(test_dataset[<span class="string">"list_classes"</span>][:]) <span class="comment"># the list of classes</span></span><br><span class="line">    </span><br><span class="line">    train_set_y_orig = train_set_y_orig.reshape((<span class="number">1</span>, train_set_y_orig.shape[<span class="number">0</span>]))</span><br><span class="line">    test_set_y_orig = test_set_y_orig.reshape((<span class="number">1</span>, test_set_y_orig.shape[<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure><h2 id="2-Overview-of-the-Problem-set"><a href="#2-Overview-of-the-Problem-set" class="headerlink" title="2 - Overview of the Problem set"></a>2 - Overview of the Problem set</h2><p><strong>Problem Statement</strong>: You are given a dataset (“data.h5”) containing:<br>    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)<br>    - a test set of m_test images labeled as cat or non-cat<br>    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).</p><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the <code>index</code> value and re-run to see other images. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">25</span></span><br><span class="line">example = train_set_x_orig[index]</span><br><span class="line">plt.imshow(train_set_x_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br></pre></td></tr></table></figure><pre><code>y = [1], it&apos;s a &apos;cat&apos; picture.</code></pre><center><img src="<a href='https://photos.google.com/share/AF1QipOhF5oNEmJO9gTdnvulk979v26GOyVo8vVVYGV6oUfMg2_SUvn3K4DnOr_tj21-Yw?key=aFFZVlhzWFAtUDBCUHBBUC1DOWdxSzBxa000MzF3&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/p7ly3mtv7h5Y3fl4JtiFmtb-i_iAerowOHSG4EXhu3SESUPMjmZhwIjaXxWp9GOqejyWazhp-maQ9NsyVNsruLAErIu1N7cZjBWTN5NnV43bukiGrcBmGm_k4WqWKoLdbAoYUyNgPw=w2400' /></a>"</img></center><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. </p><p><strong>Exercise:</strong> Find the values for:<br>    - m_train (number of training examples)<br>    - m_test (number of test examples)<br>    - num_px (= height = width of a training image)<br>Remember that <code>train_set_x_orig</code> is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access <code>m_train</code> by writing <code>train_set_x_orig.shape[0]</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">2</span>]</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure><pre><code>Number of training examples: m_train = 209Number of testing examples: m_test = 50Height/Width of each image: num_px = 64Each image is of size: (64, 64, 3)train_set_x shape: (209, 64, 64, 3)train_set_y shape: (1, 209)test_set_x shape: (50, 64, 64, 3)test_set_y shape: (1, 50)</code></pre><p><strong>Expected Output for m_train, m_test and num_px</strong>: </p><table style="width:15%">  <tr>    <td>**m_train**</td>    <td> 209 </td>   </tr>  <tr>    <td>**m_test**</td>    <td> 50 </td>   </tr>  <tr>    <td>**num_px**</td>    <td> 64 </td>   </tr></table><p>For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $<em>$ num_px $</em>$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.</p><p><strong>Exercise:</strong> Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $<em>$ num_px $</em>$ 3, 1).</p><p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$<em>$c$</em>$d, a) is to use: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[<span class="number">0</span>], <span class="number">-1</span>).T      <span class="comment"># X.T is the transpose of X</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>train_set_x_flatten shape: (12288, 209)train_set_y shape: (1, 209)test_set_x_flatten shape: (12288, 50)test_set_y shape: (1, 50)sanity check after reshaping: [17 31 56 22 33]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:35%">  <tr>    <td>**train_set_x_flatten shape**</td>    <td> (12288, 209)</td>   </tr>  <tr>    <td>**train_set_y shape**</td>    <td>(1, 209)</td>   </tr>  <tr>    <td>**test_set_x_flatten shape**</td>    <td>(12288, 50)</td>   </tr>  <tr>    <td>**test_set_y shape**</td>    <td>(1, 50)</td>   </tr>  <tr>  <td>**sanity check after reshaping**</td>  <td>[17 31 56 22 33]</td>   </tr></table><p>To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.</p><p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p><!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> <p>Let’s standardize our dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(train_set_x)</span><br></pre></td></tr></table></figure><pre><code>12288</code></pre><p><b><h4>What you need to remember:</h4></b><br><font color='blue'></p><p>Common steps for pre-processing a new dataset are:</p><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)</li><li>“Standardize” the data</font></li></ul><h2 id="3-General-Architecture-of-the-learning-algorithm"><a href="#3-General-Architecture-of-the-learning-algorithm" class="headerlink" title="3 - General Architecture of the learning algorithm"></a>3 - General Architecture of the learning algorithm</h2><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p><strong>Mathematical expression of the algorithm</strong>:</p><p>For one example $x^{(i)}$:<br>$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$<br>$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$<br>$$ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} ) * $$<br>$$ \ *log(1-a^{(i)})\tag{3}$$</p><p>The cost is then computed by summing over all training examples:<br>$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$</p><p><strong>Key steps</strong>:<br>In this exercise, you will carry out the following steps:<br>    - Initialize the parameters of the model<br>    - Learn the parameters for the model by minimizing the cost<br>    - Use the learned parameters to make predictions (on the test set)<br>    - Analyse the results and conclude</p><h2 id="4-Building-the-parts-of-our-algorithm"><a href="#4-Building-the-parts-of-our-algorithm" class="headerlink" title="4 - Building the parts of our algorithm"></a>4 - Building the parts of our algorithm</h2><p>The main steps for building a Neural Network are:</p><ol><li>Define the model structure (such as number of input features) </li><li>Initialize the model’s parameters</li><li>Loop:<ul><li>Calculate current loss (forward propagation)</li><li>Calculate current gradient (backward propagation)</li><li>Update parameters (gradient descent)</li></ul></li></ol><p>You often build 1-3 separately and integrate them into one function we call <code>model()</code>.</p><h3 id="4-1-Helper-functions"><a href="#4-1-Helper-functions" class="headerlink" title="4.1 - Helper functions"></a>4.1 - Helper functions</h3><p><strong>Exercise</strong>: Using your code from “Python Basics”, implement <code>sigmoid()</code>. As you’ve seen in the figure above, you need to compute $sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1.</span> / ( <span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid([0, 2]) = "</span> + str(sigmoid(np.array([<span class="number">0</span>,<span class="number">2</span>]))))</span><br></pre></td></tr></table></figure><pre><code>sigmoid([0, 2]) = [0.5        0.88079708]</code></pre><p><strong>Expected Output</strong>: </p><table>  <tr>    <td>**sigmoid([0, 2])**</td>    <td> [ 0.5         0.88079708]</td>   </tr></table><h3 id="4-2-Initializing-parameters"><a href="#4-2-Initializing-parameters" class="headerlink" title="4.2 - Initializing parameters"></a>4.2 - Initializing parameters</h3><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_with_zeros</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    w = np.zeros(shape=(dim, <span class="number">1</span>), dtype=np.float32)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">2</span></span><br><span class="line">w, b = initialize_with_zeros(dim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(w))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(b))</span><br></pre></td></tr></table></figure><pre><code>w = [[0.] [0.]]b = 0</code></pre><p><strong>Expected Output</strong>: </p><table style="width:15%">    <tr>        <td>  ** w **  </td>        <td> [[ 0.] [ 0.]] </td>    </tr>    <tr>        <td>  ** b **  </td>        <td> 0 </td>    </tr></table><p>For image inputs, w will be of shape (num_px $\times$ num_px $\times$ 3, 1).</p><h3 id="4-3-Forward-and-Backward-propagation"><a href="#4-3-Forward-and-Backward-propagation" class="headerlink" title="4.3 - Forward and Backward propagation"></a>4.3 - Forward and Backward propagation</h3><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p><strong>Exercise:</strong> Implement a function <code>propagate()</code> that computes the cost function and its gradient.</p><p><strong>Hints</strong>:</p><p>Forward Propagation:</p><ul><li>You get X</li><li>You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, …, a^{(m-1)}, a^{(m)})$</li><li>You calculate the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</li></ul><p>Here are the two formulas you will be using: </p><p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$<br>$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: propagate</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)              <span class="comment"># compute activation</span></span><br><span class="line">    cost = (<span class="number">-1.</span> / m) * np.sum((Y*np.log(A) + (<span class="number">1</span> - Y)*np.log(<span class="number">1</span>-A)), axis=<span class="number">1</span>)     <span class="comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dw = (<span class="number">1.</span>/m)*np.dot(X,((A-Y).T))</span><br><span class="line">    db = (<span class="number">1.</span>/m)*np.sum(A-Y, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w, b, X, Y = np.array([[<span class="number">1</span>],[<span class="number">2</span>]]), <span class="number">2</span>, np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]), np.array([[<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line">grads, cost = propagate(w, b, X, Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cost = "</span> + str(cost))</span><br></pre></td></tr></table></figure><pre><code>dw = [[0.99993216] [1.99980262]]db = [0.49993523]cost = 6.000064773192205</code></pre><p><strong>Expected Output</strong>:</p><table style="width:50%">    <tr>        <td>  ** dw **  </td>        <td> [[ 0.99993216] [ 1.99980262]]</td>    </tr>    <tr>        <td>  ** db **  </td>        <td> 0.499935230625 </td>    </tr>    <tr>        <td>  ** cost **  </td>        <td> 6.000064773192205</td>    </tr></table><h3 id="d-Optimization"><a href="#d-Optimization" class="headerlink" title="d) Optimization"></a>d) Optimization</h3><ul><li>You have initialized your parameters.</li><li>You are also able to compute a cost function and its gradient.</li><li>Now, you want to update the parameters using gradient descent.</li></ul><p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $ \theta = \theta - \alpha \text{ } d\theta$, where $\alpha$ is the learning rate.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        grads, cost = propagate(w=w, b=b, X=X, Y=Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b -  learning_rate*db</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params, grads, costs = optimize(w, b, X, Y, num_iterations= <span class="number">100</span>, learning_rate = <span class="number">0.009</span>, print_cost = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(params[<span class="string">"w"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(params[<span class="string">"b"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br></pre></td></tr></table></figure><pre><code>w = [[0.1124579 ] [0.23106775]]b = [1.55930492]dw = [[0.90158428] [1.76250842]]db = [0.43046207]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%">    <tr>       <td> **w** </td>       <td>[[ 0.1124579 ] [ 0.23106775]] </td>    </tr><pre><code>&lt;tr&gt;   &lt;td&gt; **b** &lt;/td&gt;   &lt;td&gt; 1.55930492484 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;   &lt;td&gt; **dw** &lt;/td&gt;   &lt;td&gt; [[ 0.90158428]</code></pre><p> [ 1.76250842]] </td><br>    </tr><br>    <tr><br>       <td> <strong>db</strong> </td><br>       <td> 0.430462071679 </td><br>    </tr></p></table><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p><ol><li><p>Calculate $\hat{Y} = A = \sigma(w^T X + b)$</p></li><li><p>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_prediction</code>. If you wish, you can use an <code>if</code>/<code>else</code> statement in a <code>for</code> loop (though there is also a way to vectorize this). </p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    [print(x) <span class="keyword">for</span> x <span class="keyword">in</span> A]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"predictions = "</span> + str(predict(w, b, X)))</span><br></pre></td></tr></table></figure><pre><code>[0.99987661 0.99999386]predictions = [[1. 1.]]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:30%">    <tr>         <td>             **predictions**         </td>          <td>            [[ 1.  1.]]         </td>     </tr></table><p><b><h4>What to remember </h4></b><br><font color='blue'><br>You’ve implemented several functions that:</p><ul><li>Initialize (w,b)</li><li>Optimize the loss iteratively to learn parameters (w,b):<ul><li>computing the cost and its gradient </li><li>updating the parameters using gradient descent</li></ul></li><li>Use the learned (w,b) to predict the labels for a given set of examples</font><br></li></ul><h2 id="5-Merge-all-functions-into-a-model"><a href="#5-Merge-all-functions-into-a-model" class="headerlink" title="5 - Merge all functions into a model"></a>5 - Merge all functions into a model</h2><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p><strong>Exercise:</strong> Implement the model function. Use the following notation:<br>    - Y_prediction for your predictions on the test set<br>    - Y_prediction_train for your predictions on the train set<br>    - w, costs, grads for the outputs of optimize()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872[0.94366988 0.86095311 0.88896715 0.93630641 0.74075403 0.52849619 0.03094677 0.85707681 0.88457925 0.67279696 0.26601085 0.4823794 0.74741157 0.78575729 0.00978911 0.9203284  0.02453695 0.84884703 0.2050248  0.03703224 0.92931392 0.11930532 0.01411064 0.7832698 0.58188015 0.66897565 0.75119007 0.01323558 0.03402649 0.99735115 0.21031727 0.78123225 0.6815842  0.46647604 0.66323375 0.03424828 0.08031627 0.76570656 0.34760863 0.06177743 0.6987531  0.4106426 0.6648871  0.02776868 0.93053125 0.46395717 0.23971605 0.9771735 0.66202407 0.10482388][1.96533335e-01 8.97519936e-02 8.90887727e-01 2.05354859e-04 4.10043201e-02 1.13855541e-01 3.58425358e-02 9.20256043e-01 8.11815498e-02 5.09505652e-02 1.43687735e-01 7.77661312e-01 2.37002682e-01 9.26822611e-01 7.20256211e-01 4.54525029e-02 2.88164240e-02 4.96209946e-02 9.53642451e-02 9.27127783e-01 1.46871713e-02 4.42749993e-02 1.99658284e-01 5.10794145e-02 8.71854257e-01 8.54873232e-01 4.43988460e-02 8.41877286e-01 5.57178266e-02 7.39175253e-01 8.73390575e-02 7.61255429e-02 2.01282223e-01 2.02159519e-01 7.95065561e-02 3.69885691e-02 1.14655638e-02 5.90397260e-02 8.36880946e-01 3.33057415e-01 1.98548242e-02 4.46965063e-01 8.23737950e-01 4.13465923e-02 4.61512591e-02 1.21739845e-01 9.76716144e-02 8.07086225e-01 8.93389416e-03 3.73249849e-02 7.53711249e-01 2.47934596e-01 1.47013078e-01 3.93089594e-01 9.02530607e-01 3.94290174e-03 9.38300399e-01 8.14429890e-01 5.51201724e-02 9.56820776e-01 8.35826040e-01 7.75371183e-01 4.97406386e-02 5.05302748e-02 1.68276426e-01 7.39795683e-02 4.23114248e-02 1.80374321e-01 7.36839673e-01 2.36170561e-02 4.78407244e-02 9.72682719e-01 8.87430447e-02 1.40500115e-01 7.39006094e-02 5.87414480e-01 8.55122639e-04 3.51320419e-02 7.21341360e-02 1.59367000e-01 9.18793718e-02 2.76678199e-03 2.16954763e-02 8.75788002e-01 7.48905473e-01 2.61224310e-02 1.31264831e-01 5.58549892e-02 7.96470422e-01 4.31114360e-02 2.46081640e-01 9.28094796e-02 5.13207713e-01 9.23532733e-01 9.11010943e-01 1.56664277e-01 1.40529680e-01 8.72871654e-01 6.33390909e-02 2.04276699e-01 1.50378528e-01 5.42005811e-02 7.16869008e-01 8.93930822e-02 9.68748123e-01 1.16897229e-01 9.65813244e-01 7.63463753e-01 8.45184245e-01 7.94804824e-01 8.77046596e-01 8.92528474e-01 2.33698759e-02 1.08088606e-01 9.41045938e-02 5.06133571e-02 6.14255764e-02 8.74814031e-01 7.14021606e-03 1.49573407e-01 1.38752636e-02 5.75050572e-01 4.74218632e-02 2.67728414e-04 8.16437270e-01 5.25431990e-03 8.27320337e-01 1.63520986e-01 9.19597717e-01 9.11124533e-01 2.96731271e-01 1.37316359e-01 7.56632692e-02 9.51896490e-01 7.13340131e-01 5.62771203e-01 8.46803645e-01 8.81283783e-01 5.80214923e-03 3.24191787e-02 3.66569448e-02 4.24241240e-02 9.02746461e-01 6.95602248e-03 7.28528692e-01 8.04734016e-01 8.48847026e-01 1.97286016e-01 8.73972266e-01 8.56810568e-01 4.60108117e-01 9.98074787e-02 2.67726747e-02 9.16713593e-01 5.70477051e-02 2.34413956e-01 9.17441504e-01 1.43642340e-02 1.48384241e-02 4.18971050e-02 4.81257763e-03 6.74987512e-02 7.96958661e-01 7.94548221e-02 8.88055227e-01 1.63703299e-02 9.64896262e-01 4.74597209e-02 3.78354422e-02 6.75950812e-01 7.60983832e-01 8.91154251e-01 2.15482871e-01 1.80695199e-02 9.46591763e-01 7.71101522e-01 4.14565207e-02 8.02916154e-01 8.02541805e-02 6.89037478e-01 4.92103989e-02 4.87010785e-02 1.89987579e-02 3.71043577e-02 1.73595068e-03 7.71575747e-01 4.06433366e-02 6.60606392e-02 8.34508562e-01 2.27408842e-02 6.17839573e-02 4.56149270e-02 7.64947622e-01 6.19347921e-02 3.55887869e-03 1.03103435e-01 3.83745905e-01 8.77909931e-01 7.72818586e-02 1.79082665e-02 8.09911232e-01 2.02130387e-02 1.89353139e-02 1.83142729e-02 1.94041166e-01 2.01151983e-01 8.48224028e-02 1.61929290e-01 1.82858623e-01]train accuracy: 99.04306220095694 %test accuracy: 70.0 %</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%"> <pre><code>&lt;tr&gt;    &lt;td&gt; **Train Accuracy**  &lt;/td&gt;     &lt;td&gt; 99.04306220095694 % &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;**Test Accuracy** &lt;/td&gt;     &lt;td&gt; 70.0 % &lt;/td&gt;&lt;/tr&gt;</code></pre></table> <p><strong>Comment</strong>: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the <code>index</code> variable) you can look at predictions on pictures of the test set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture that was wrongly classified.</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(test_set_y[<span class="number">0</span>,index]) + <span class="string">", you predicted that it is a \""</span> + classes[int(d[<span class="string">"Y_prediction_test"</span>][<span class="number">0</span>,index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><pre><code>y = 1, you predicted that it is a &quot;cat&quot; picture.</code></pre><center><a href='https://photos.google.com/share/AF1QipOUgHsTlROlQZeelQLlUDt2aOiiW1IzNklWyfOAWfvZY0cOQRFxOWOvkPY0KHVnIA?key=VTR0ZVQ0Sy1TMDdoTjhsOUVWNm5nZjU0Y3NlZm13&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/kJY-tRqpPcdQ7kqF3XJfo4RZ_-pauHBMdujmOlWP6janiuhNawemEADSBrPiqlgRi8vPHLqRVNrwqoxegsWCRDWwy8ntZMiT9BHxlPFfRfa9U4QGr7lNTWB4tNFRzryQuXPGReFn1g=w2400' /></a></center><p>Let’s also plot the cost function and the gradients.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><center><a href='https://photos.google.com/share/AF1QipP9rHmyux7iIhGy_N85xKMcQFohZNYDETOsVAAAvdy-Aun5j_JKS81vPiRDTMa3tQ?key=RnZaVXdSb0dFanFRb1NPUmZnTnZGbWw3TjlZdFh3&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/02H_4u-Q1y8hDW6L6nDzXAX6bE6QqP6WXfK_hrbXFYQ9Zxm4KVDJo9gEo3511OBsuvGzNo9GD6haI8auCtBkFlaEU2UDgCK6ReeVMyGqSg7brcTSQysUZr30K-Y7dTLdRzCVR67cMw=w2400' /></a></center><p><strong>Interpretation</strong>:<br>You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. </p><h2 id="6-Further-analysis-optional-ungraded-exercise"><a href="#6-Further-analysis-optional-ungraded-exercise" class="headerlink" title="6 - Further analysis (optional/ungraded exercise)"></a>6 - Further analysis (optional/ungraded exercise)</h2><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate $\alpha$. </p><h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>:<br>In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>learning rate is: 0.01[0.97125943 0.9155338  0.92079132 0.96358044 0.78924234 0.60411297 0.01179527 0.89814048 0.91522859 0.70264065 0.19380387 0.49537355 0.7927164  0.85423431 0.00298587 0.96199699 0.01234735 0.9107653 0.13661137 0.01424336 0.96894735 0.1033746  0.00579297 0.86081326 0.53811196 0.64950178 0.83272843 0.00426307 0.0131452  0.99947804 0.11468372 0.82182442 0.69611733 0.4991522  0.67231401 0.01728165 0.04136099 0.80069693 0.26832359 0.03958566 0.74731239 0.32116434 0.71871197 0.01205725 0.96879962 0.62310364 0.17737126 0.98960523 0.74697265 0.07284605][1.47839654e-01 5.78008187e-02 9.42385025e-01 4.14849240e-05 2.27209941e-02 7.29254668e-02 2.23704494e-02 9.49717864e-01 5.41724296e-02 2.92729895e-02 6.82412300e-02 8.33370210e-01 1.71420615e-01 9.66879883e-01 8.11537151e-01 2.44343483e-02 7.87634098e-03 2.64027272e-02 5.60720049e-02 9.53130353e-01 5.30865324e-03 3.11020745e-02 1.43606493e-01 1.92650473e-02 9.30132798e-01 8.95291211e-01 2.72790550e-02 9.01480142e-01 2.73987904e-02 8.09041583e-01 6.64266068e-02 5.00479730e-02 1.29245158e-01 1.40274640e-01 6.48179131e-02 1.35261338e-02 4.77693620e-03 2.65922710e-02 8.89771230e-01 2.64826222e-01 1.22921585e-02 6.03229153e-01 8.81822076e-01 1.35079742e-02 2.49595285e-02 6.88961126e-02 5.86046930e-02 8.68932415e-01 5.14520331e-03 1.21099845e-02 8.23403970e-01 1.70985647e-01 9.49977566e-02 3.04227660e-01 9.48091298e-01 8.09204729e-04 9.66640038e-01 8.78319466e-01 3.17284880e-02 9.76165700e-01 8.81584697e-01 8.48145722e-01 2.70795160e-02 2.28390397e-02 1.05295676e-01 4.45165292e-02 1.22858876e-02 1.35813814e-01 8.25867437e-01 9.21552651e-03 2.49353830e-02 9.88067070e-01 5.78381495e-02 8.57292849e-02 4.10128551e-02 5.70507956e-01 2.11603229e-04 1.52264723e-02 6.18390722e-02 1.39187810e-01 6.68993749e-02 4.14281785e-04 1.23347660e-02 9.24789062e-01 8.16880995e-01 9.29503655e-03 8.23770893e-02 2.75905821e-02 8.52215781e-01 2.36580782e-02 1.75344552e-01 6.15499363e-02 6.58017000e-01 9.54697511e-01 9.62775471e-01 1.05372217e-01 9.37239413e-02 9.29062265e-01 2.68654456e-02 1.44668290e-01 9.15662947e-02 2.89260930e-02 8.02603133e-01 6.11847790e-02 9.87937140e-01 5.84677169e-02 9.87171184e-01 8.37167548e-01 8.94717386e-01 8.58260204e-01 9.36232298e-01 9.33067878e-01 8.77279900e-03 5.88387682e-02 5.09517612e-02 2.40626781e-02 3.87480256e-02 9.35343373e-01 2.35202639e-03 8.83972091e-02 4.49639004e-03 6.64404296e-01 1.76677024e-02 2.75426440e-05 8.71728805e-01 2.43292078e-03 8.92351131e-01 9.50411299e-02 9.66495010e-01 9.27285472e-01 2.66413779e-01 8.70883114e-02 5.40743542e-02 9.75155426e-01 8.02323751e-01 6.92965782e-01 9.06287458e-01 9.39900204e-01 1.64790714e-03 1.91364329e-02 1.66925680e-02 1.46846281e-02 9.39237709e-01 2.57925925e-03 8.19134439e-01 8.54311895e-01 9.10765301e-01 1.20452016e-01 9.10603560e-01 9.11977137e-01 3.72174950e-01 6.13527932e-02 1.30882743e-02 9.55225821e-01 4.30680816e-02 1.37970158e-01 9.60868956e-01 8.67705030e-03 5.95741909e-03 2.19466774e-02 1.78308409e-03 2.57658927e-02 8.63787547e-01 3.44218954e-02 9.34152347e-01 9.35483274e-03 9.90908018e-01 1.17832722e-02 2.67756870e-02 7.74546160e-01 8.43831858e-01 9.38847463e-01 1.48599256e-01 4.17198956e-03 9.81043189e-01 8.22764984e-01 1.92120393e-02 8.58870443e-01 5.37478573e-02 7.84878423e-01 3.56080493e-02 2.80545014e-02 1.09777935e-02 1.30396160e-02 3.81067987e-04 8.51025984e-01 2.44476492e-02 4.57657708e-02 8.81871553e-01 1.06481927e-02 2.84032919e-02 1.96773463e-02 8.54577180e-01 3.01055581e-02 1.33843958e-03 7.04152762e-02 3.08344786e-01 9.25167630e-01 4.53183035e-02 9.31980521e-03 8.69872444e-01 4.61339718e-03 4.86286962e-03 7.32772398e-03 1.26009270e-01 1.46124056e-01 4.51019670e-02 1.45139959e-01 1.45971589e-01]train accuracy: 99.52153110047847 %test accuracy: 68.0 %-------------------------------------------------------learning rate is: 0.001[0.74458179 0.63302701 0.70621076 0.7037801  0.5322598  0.43784581 0.1843739  0.71778574 0.73717649 0.59122536 0.39837511 0.44491784 0.63244572 0.53976962 0.09938522 0.7227688  0.12316033 0.58301417 0.28145733 0.16609522 0.61461919 0.14166416 0.0865388  0.4251847 0.67719513 0.61251308 0.46730808 0.11854922 0.21041046 0.8906756 0.42313203 0.56013238 0.60322016 0.37148913 0.57460259 0.11968291 0.24088599 0.65905854 0.4782032  0.14862075 0.4992436  0.61682528 0.4795275  0.16260336 0.70722369 0.23929218 0.36719514 0.87223907 0.45484261 0.19029187][0.34403391 0.18575705 0.63392388 0.00949352 0.185803   0.30979007 0.13544854 0.75931407 0.18856286 0.16653711 0.47903517 0.55094252 0.39894694 0.67613631 0.32941411 0.15120523 0.1515817  0.10868391 0.21533234 0.78261458 0.09236643 0.13102179 0.30209379 0.22018859 0.60467471 0.63089631 0.13786841 0.52162666 0.2229145  0.41807311 0.20928386 0.22354737 0.51863273 0.37446655 0.12619979 0.24763606 0.08217106 0.20570627 0.61668309 0.47341694 0.07578526 0.20272218 0.63694514 0.17332725 0.12774778 0.38987251 0.25716102 0.57589232 0.03660729 0.23627192 0.5058546  0.44851881 0.26882028 0.54506441 0.63427748 0.07593065 0.79389128 0.55848777 0.20399827 0.82950311 0.67551516 0.49340246 0.12825017 0.19483707 0.30405843 0.25239064 0.23849329 0.28306742 0.39562206 0.1338017  0.20953382 0.84559705 0.25983452 0.43347997 0.25869745 0.5275365  0.01851016 0.18226072 0.11686925 0.24360522 0.11457144 0.09711829 0.11403479 0.64158072 0.56492264 0.14249209 0.26621215 0.23562087 0.63347539 0.19718838 0.41665293 0.2560914  0.20511226 0.75854285 0.62700096 0.22437352 0.24158168 0.58986637 0.18250551 0.31168748 0.40230892 0.18766222 0.37363736 0.24954905 0.81540625 0.33905562 0.85287524 0.46460165 0.64873862 0.49476607 0.58689285 0.73160658 0.15974705 0.28192355 0.21969254 0.17213348 0.24140747 0.59506433 0.09843999 0.4664941 0.11789794 0.41615495 0.28828188 0.01549565 0.57657208 0.03491378 0.56433333 0.52342054 0.58263447 0.828261   0.33112864 0.30054751 0.13866344 0.7796039  0.49905825 0.23849455 0.65130553 0.54865883 0.07475945 0.15289783 0.17277205 0.21093974 0.77996081 0.05731401 0.43542011 0.62528802 0.58301417 0.39592429 0.62711359 0.62164606 0.52142034 0.2237536  0.11263677 0.69875451 0.13460421 0.59843604 0.62542932 0.06223702 0.08147044 0.16677973 0.0471795  0.29026597 0.53465382 0.37298272 0.67567251 0.08157721 0.80300364 0.36035876 0.12411481 0.3216639  0.50044148 0.71061923 0.38536321 0.15865892 0.73153708 0.60089581 0.19123039 0.60169201 0.21145324 0.35174627 0.1036799  0.15432715 0.08266478 0.1765554  0.0315303  0.4005418 0.12684962 0.13818573 0.69657105 0.10353719 0.15229696 0.170385 0.39005657 0.21251557 0.03788715 0.32853945 0.47339083 0.62631301 0.22739755 0.0902158  0.56782331 0.17507791 0.20252309 0.09500011 0.34504767 0.39618939 0.25822558 0.24550552 0.30677401]train accuracy: 88.99521531100478 %test accuracy: 64.0 %-------------------------------------------------------learning rate is: 0.0001[0.45098635 0.48539489 0.40959087 0.44864257 0.32818894 0.43729766 0.28884626 0.46438078 0.45494399 0.45491705 0.36938309 0.41863679 0.45816519 0.5031755  0.2842568  0.45155065 0.30672371 0.37824086 0.26505548 0.27737934 0.40677576 0.28781555 0.24304775 0.38397796 0.50642581 0.47047843 0.35358916 0.31561491 0.39430714 0.4603235 0.37998879 0.3764821  0.32056264 0.38693085 0.40764828 0.23150119 0.311659   0.44981144 0.43152263 0.26276732 0.37785575 0.48883282 0.37790798 0.30969512 0.47842906 0.32857529 0.34457076 0.60547775 0.40733226 0.28828383][0.4225819  0.31692389 0.42964509 0.14896683 0.28783033 0.38652698 0.29492571 0.44991522 0.31988018 0.32391139 0.39318147 0.34804173 0.40099138 0.31694856 0.28102266 0.3231201  0.25486297 0.18485428 0.31900054 0.52941528 0.25568417 0.27297382 0.29762542 0.35834172 0.38912252 0.4552143  0.2555983  0.34830216 0.29078565 0.27432926 0.31094887 0.44330557 0.47172673 0.39765449 0.22386371 0.46108148 0.27055987 0.31333951 0.49901097 0.439851   0.23953174 0.29809115 0.42197081 0.28385499 0.2465556  0.40478121 0.35487343 0.45521241 0.1451398  0.37485678 0.36671611 0.37909623 0.30298036 0.40151709 0.40460677 0.24757226 0.50122617 0.38917296 0.3687779  0.50666786 0.52492017 0.37864634 0.24031899 0.30627306 0.35114005 0.37398054 0.43104844 0.31851314 0.37029232 0.29232461 0.37616632 0.52373453 0.32507684 0.48381803 0.39170698 0.38646363 0.17397111 0.31623794 0.24714356 0.35235176 0.17699762 0.33409149 0.3249697  0.49338319 0.413497   0.25575577 0.31724095 0.36456893 0.45018347 0.34698807 0.38500817 0.46053573 0.23091555 0.47268593 0.43291929 0.26828088 0.30258929 0.4164904  0.24453098 0.27111067 0.39272427 0.3043153 0.27984468 0.39085873 0.52409691 0.34966557 0.55950717 0.37739831 0.43697184 0.33557455 0.38439647 0.48607992 0.30731772 0.31028645 0.30037443 0.29400226 0.42714997 0.42909528 0.30432821 0.5227302 0.32144602 0.4066469  0.43683178 0.17768611 0.4354696  0.18662466 0.39558874 0.48819809 0.35425959 0.57279833 0.35098941 0.33429763 0.31014998 0.49175402 0.44154511 0.31203466 0.38776576 0.38352489 0.29611222 0.36104647 0.33824864 0.37198268 0.52831432 0.25732676 0.36743298 0.44902822 0.37824086 0.36790056 0.41246464 0.37833397 0.3418507  0.30119593 0.2592477  0.46753926 0.26777792 0.43134603 0.32491304 0.19700069 0.25937972 0.33143626 0.19820128 0.35468513 0.36334932 0.51823778 0.37235121 0.27650473 0.47271147 0.44760504 0.33240186 0.29967323 0.41157608 0.47817969 0.39048545 0.28309008 0.46350184 0.41099669 0.34508275 0.4323286  0.35065016 0.33976266 0.25459527 0.29233107 0.26976618 0.30004182 0.18212017 0.34254174 0.26213135 0.2674514  0.45817075 0.24356149 0.25227369 0.34588203 0.3783451  0.32762257 0.19831251 0.51451759 0.3792938  0.41417054 0.34795587 0.25521854 0.42313521 0.32557428 0.38342989 0.21943589 0.34909483 0.39399177 0.36128874 0.38042346 0.38929593]train accuracy: 68.42105263157895 %test accuracy: 36.0 %-------------------------------------------------------</code></pre><center><a href='https://photos.google.com/share/AF1QipMIQoAH_HanyZRG6NUD1B5ZQu8lBksMFyYPKySMpVXgKm06otmzS7ikaoGmTSd6sA?key=WF9TdjBCMlh2OFc1UnVFc29rQ2FxTTB6X29lbnJn&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/VISa3bYoaFyoyL4EnFg4OhGXPa_aIgC3M3hDJsOQP-3PK1ICbo9ZT3udg7Jp_nCRj9rxEjREN-wP83WaR6EVAV2dCC1FogwYI91awu3uX9EF0cUh3jApJxQb-0_VKQtBc0Telo5-Kw=w2400' /></a></center><p><b>Interpretation:</b> </p><ul><li>Different learning rates give different costs and thus different predictions results.</li><li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). </li><li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li><li>In deep learning, we usually recommend that you: <ul><li>Choose the learning rate that better minimizes the cost function.</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) </li></ul></li></ul><h2 id="7-Test-with-your-own-image-optional-ungraded-exercise"><a href="#7-Test-with-your-own-image-optional-ungraded-exercise" class="headerlink" title="7 - Test with your own image (optional/ungraded exercise)"></a>7 - Test with your own image (optional/ungraded exercise)</h2><p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:<br>    1. Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub.<br>    2. Add your image to this Jupyter Notebook’s directory, in the “images” folder<br>    3. Change your image’s name in the following code<br>    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## START CODE HERE ## (PUT YOUR IMAGE NAME) </span></span><br><span class="line">my_image = <span class="string">"my_image.jpg"</span>   <span class="comment"># change this to the name of your image file </span></span><br><span class="line"><span class="comment">## END CODE HERE ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We preprocess the image to fit your algorithm.</span></span><br><span class="line">fname = <span class="string">"images/"</span> + my_image</span><br><span class="line">image = np.array(plt.imread(fname))</span><br><span class="line">my_image = skimage.transform.resize(image, output_shape=(num_px,num_px)).reshape((<span class="number">1</span>, num_px*num_px*<span class="number">3</span>)).T</span><br><span class="line">my_predicted_image = predict(d[<span class="string">"w"</span>], d[<span class="string">"b"</span>], my_image)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line">print(<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your algorithm predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><pre><code>/Users/abanihi/opt/miniconda3/envs/pangeo/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in skimage 0.15.  warn(&quot;The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in &quot;/Users/abanihi/opt/miniconda3/envs/pangeo/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;[0.30930831]y = 0.0, your algorithm predicts a &quot;non-cat&quot; picture.</code></pre><center><a href='https://photos.google.com/share/AF1QipPGLzNT1YMUSFC85y62mTofrREZ6t9ghgixEnRTkEHuWJeoXqsukVd_U_KVzhNblQ?key=WHRROHJzc3VCdXp5RS1XcExNeVhlcnZDOWhpcGFR&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/zRQjYeR_0i0Az8BynDY35YelX_s5nomdk0mQAlOjWdhWvSe70Ey4EKZntInJN3WwEb9i9lusqNNqtsBXc15qaEj0eyH8uYlDNxr72DjbeGaqvPjE9iZqmZcTozgW157sVuTlGePc8Q=w2400' /></a></center><p><b><h4>What to remember from this assignment:</h4></b><br><font color='blue'></p><ol><li>Preprocessing the dataset is important.</li><li>You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().</li><li>Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!</li></ol><p>Finally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:<br>    - Play with the learning rate and the number of iterations<br>    - Try different initialization methods and compare the results<br>    - Test other preprocessings (center the data, or divide each row by its standard deviation)<br></font></p><p>Bibliography:</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9zdGF0cy5zdGFja2V4Y2hhbmdlLmNvbS9xdWVzdGlvbnMvMjExNDM2L3doeS1kby13ZS1ub3JtYWxpemUtaW1hZ2VzLWJ5LXN1YnRyYWN0aW5nLXRoZS1kYXRhc2V0cy1pbWFnZS1tZWFuLWFuZC1ub3QtdGhlLWM=">https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c<i class="fa fa-external-link-alt"></i></span></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext version_information</span><br><span class="line">%version_information skimage, numpy, matplotlib, h5py, sklearn</span><br></pre></td></tr></table></figure><table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.6 64bit [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]</td></tr><tr><td>IPython</td><td>7.0.1</td></tr><tr><td>OS</td><td>Darwin 17.7.0 x86_64 i386 64bit</td></tr><tr><td>skimage</td><td>0.14.1</td></tr><tr><td>numpy</td><td>1.15.1</td></tr><tr><td>matplotlib</td><td>3.0.0</td></tr><tr><td>h5py</td><td>2.8.0</td></tr><tr><td>sklearn</td><td>0.20.0</td></tr><tr><td colspan='2'>Mon Oct 15 08:33:43 2018 MDT</td></tr></table><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Logistic Regression with a Neural Network mindset Assignment || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)&lt;br&gt;&lt;b&gt;Note - These are my notes on the second course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;br&gt;All Credits to the teacher &lt;b&gt;andrew ng &lt;/b&gt; :)&lt;/p&gt;
    
    </summary>
    
    
      <category term="new post" scheme="https://massivefile.com/categories/new-post/"/>
    
      <category term="2020" scheme="https://massivefile.com/categories/new-post/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/categories/new-post/2020/Assignment/"/>
    
    
      <category term="new post" scheme="https://massivefile.com/tags/new-post/"/>
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/tags/Assignment/"/>
    
      <category term="Logistic Regression with a Neural Network mindset" scheme="https://massivefile.com/tags/Logistic-Regression-with-a-Neural-Network-mindset/"/>
    
  </entry>
  
  <entry>
    <title>Classic Vim without plugins</title>
    <link href="https://massivefile.com/Vim_no_plugins/"/>
    <id>https://massivefile.com/Vim_no_plugins/</id>
    <published>2020-04-05T14:46:00.000Z</published>
    <updated>2020-04-12T15:49:28.791Z</updated>
    
    <content type="html"><![CDATA[<p>This article Covers everything related to be going with Vim for any beginner or intermediate.<br>I have explained the ins and outs of Vim in briefly.<br>This article will be continuously updated based on experience.</p><a id="more"></a> <h2 id="Four-modes-of-vim"><a href="#Four-modes-of-vim" class="headerlink" title="Four modes of vim"></a>Four modes of vim</h2><ol><li><p>General mode: normal mode. You can move the cursor, delete characters or entire lines, and copy and paste file data. Opening vim is to enter this mode, and the switching between the three modes is also transferred here.</p></li><li><p>Edit mode: In general mode, press any one of <code>i``I``o`` O</code> <code>a`` A</code> <code>r`` R</code> <code>s`` S</code> to enter this mode. You can edit the content of the file and press Esc to return to the normal mode.</p><p> -<code>i`` I</code> is insert (in front of the character under the cursor and at the beginning of the line)<br> -<code>o`` O</code> is open new line (start a new line below the line where the cursor is and start a new line above the line where the cursor is<br> -<code>a`` A</code> is append (after the character under the cursor and at the end of the line where the cursor is)<br> -<code>s`` S</code> is to delete (the character where the cursor is and start inserting and the line where the cursor is and start inserting), that is, substitute replacement.<br> -<code>r`` R</code> is to replace the character under the cursor and become the replacement mode</p></li><li><p>Command line mode: Press <code>:</code> <code>/</code> <code>in normal mode?</code>Any one enters this mode (the meaning of these symbols will be described below). You can find data operations, read, save, replace a lot of characters, leave vim, display line number and other operations, press Esc to return to the general mode.</p></li><li><p>Visual mode: In general mode, press <code>v`` V</code> <code>ctr + v</code> to enter the visual mode, which is equivalent to the normal mode after highlighting the selected text, that is, in this mode, you can arbitrarily select a specific area and be The selected area is highlighted, <code>v</code> selects the unit: one character;<code>V</code> is also called the visible line mode, select the unit: line; <code>ctr + v</code> is also called the visible block mode, select the unit: square ; All three are useful, see below for details.</p></li></ol><h2 id="Mobile"><a href="#Mobile" class="headerlink" title="Mobile"></a>Mobile</h2><p>In normal mode:</p><p><code>w</code> → to the beginning of the next word<code>e</code> → to the end of the next word (words are separated by spaces by default)<br><code>W</code> → to the beginning of the next string<code>E</code> → to the end of the next string (a string refers to a string consisting of numbers, letters, and underscores)<br><code>B</code> → Go to the first character of the previous string. The<code>b</code> → “command moves the cursor to the first character of the previous word.</p><blockquote><p>By default, a word is composed of letters, numbers and underscores<br>If you think words are separated by blank characters, then you need to use uppercase E and W (Chen Hao: Note)</p></blockquote><p><code>0</code> → number zero, to the beginning of the line<br><code>^</code> → Go to the first position of the line that is not a blank character (the so-called blank character is a space, tab, line feed, carriage return, etc.)<br><code>$</code> → Go to the end of the line<br><code>g_</code> → to the last position of the line that is not a blank character<br><code>%</code> → Go to the other of the pair of brackets where the cursor is<br><code>gg</code> → first line<br><code>G</code> → last line<br><code>h`` j</code> <code>k`` l</code> (strongly recommended to use it to move the cursor, but it is not necessary) → you can also use the cursor keys (← ↓ ↑ →). Note: j extends downwards, k extends upward</p><ol><li>** High frequency usage scenario 1 **: Change a variable name in the line first and move the cursor: <code>w</code> and<code>b</code>, <code>W</code> and<code>B</code> (or if the line is too long, use the following Search function) to the target word</li><li>** High frequency usage scenario 2 **: Modify the indent and jump to the beginning of the line<code>^</code></li><li>** High frequency usage scenario 3 **: View the completeness of the function or class or the variable scope <code>%</code></li><li>** High frequency usage scenario 4 **: After splitting the screen, jump to different windows: <code>ctrl + w + (h or j or k or l)</code></li><li>** High-frequency use scene 5 **: Move left (left, top, right) <code>(h, j, k, l)</code></li><li>** High frequency usage scenario 6 **: Delete to the end: <code>d $</code> Delete to the beginning: <code>d ^</code></li></ol><h3 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h3><p>Note: ** mark is to find better **, in normal mode:</p><p><code>mx</code> meaning: mark x, x is the name of mark;<br><code>&#39;x</code> meaning: go to the position of x mark</p><ol><li>** High-frequency usage scenario 1: ** You can see how to define other functions in the function. You want to see how to define them. After you read it, you need to come back. Then mark it first and then jump back.</li></ol><h3 id="Syntax-related-jumps"><a href="#Syntax-related-jumps" class="headerlink" title="Syntax related jumps"></a>Syntax related jumps</h3><p>In normal mode:</p><ol><li><code>gd</code> meaning: go to definition</li><li>Press <code>[</code> and then <code>ctrl + d</code> to jump to #define </li><li>Press <code>[</code> and then <code>ctrl + i</code> to jump to functions, variables and #define </li></ol><p>** Note **: The language support is not very good, you can try the language used</p><h3 id="Quick-page-turning"><a href="#Quick-page-turning" class="headerlink" title="Quick page turning"></a>Quick page turning</h3><p>In normal mode:</p><table><thead><tr><th>Partner 1</th><th>Partner 2</th></tr></thead><tbody><tr><td><code>ctr + d</code> page down</td><td><code>ctr + d</code> page up</td></tr><tr><td><code>ctr + f</code> page forward</td><td><code>ctr + b</code> page back</td></tr></tbody></table><h2 id="Action-operation-instruction"><a href="#Action-operation-instruction" class="headerlink" title="Action operation instruction"></a>Action operation instruction</h2><p>In normal mode:</p><table><thead><tr><th>Partner 1</th><th>Partner 2</th></tr></thead><tbody><tr><td><code>d</code> ** d ** elete a character and copy to clipboard</td><td><code>D</code> has been ** deleted ** from the cursor position to the end of the line</td></tr><tr><td><code>y</code> ** c ** opy to clipboard</td><td><code>Y</code> ** Copy ** one line (= <code>yy</code>)</td></tr><tr><td><code>s</code> ** s ** ubstitue a character</td><td><code>S</code> ** Replace ** the line where the cursor is located</td></tr><tr><td><code>r</code> ** r ** eplace a character</td><td><code>R</code>* Not commonly used *, which means to enter replacement mode</td></tr><tr><td><code>c</code> ** c ** hange a character</td><td><code>C</code> * Not commonly used <em>, which means *</em> modify ** the cursor position until the end of the line, the same effect as <code>S</code> rendering</td></tr><tr><td><code>p</code> ** p ** aste after the cursor</td><td><code>P</code> ** Paste ** before the cursor position (if you paste a whole line, paste to the previous line)</td></tr><tr><td><code>u</code> ** u ** ndo a operation</td><td><code>U</code> one-time ** undo ** all operations on a whole line</td></tr><tr><td><code>x</code> cut a character</td><td><code>X</code> * not commonly used <em>, *</em> cut ** to the left, ie backspace: delete the character to the left of the cursor</td></tr><tr><td>** `** Search the word under the current cursor ** downward, and jump to the next word when found</td><td><code>#</code>** Search the word under the current cursor ** upward, jump to the previous word when found</td></tr><tr><td><code>/ word</code> ** Search the word word ** down to the full text, jump to the first word that matches, if there are multiple, continue to search down and press the n key (in the direction of the original command), up to press the N key.</td><td><code>? word</code> ** Search the word word ** up the full text, jump to the first word that matches, and if there are multiple, continue to search upwards and press the n key (in the direction of the original command), down to press the N key.</td></tr><tr><td><code>a</code> ** a ** ppend after the cursor</td><td><code>A</code> is ** append ** at the end of the line where the cursor is located)</td></tr><tr><td><code>i</code> ** i ** nsert before the cursor</td><td><code>I</code> ** insert ** at the beginning of the line where the cursor is located</td></tr><tr><td><code>o</code> Start a new line below the line under the cursor, open the new world?</td><td><code>O</code> starts a new line above the line where the cursor is located</td></tr><tr><td><code>v</code> enters ** v ** isual mode, used to select areas (can cross lines), used to cooperate with other subsequent operations (addition, deletion, and modification)</td><td><code>v</code> enters visual line mode, used to select some lines To cooperate with other follow-up operations (addition, deletion and modification)</td></tr><tr><td><code>f</code> ** f ** ind a character after the cursor</td><td><code>F</code> ** find a character before the cursor position **</td></tr><tr><td><code>t</code> ** t ** ill a character tx is the same as fx, the difference is to jump to the front of character x</td><td><code>T</code> Tx is the same as Fx, the difference is to jump to the character x</td></tr></tbody></table><h3 id="Formed-separately"><a href="#Formed-separately" class="headerlink" title="Formed separately"></a>Formed separately</h3><p><code>.</code> Repeat the operation just now<br><code>~</code> Convert case</p><ol><li>You can change the case of the first letter of the variable</li><li>You can select a string (variable) in combination with the commands provided below, and then change the case of the entire string (variable). For example: macro definition</li></ol><p><code>=</code> Auto format</p><ol><li>Use <code>==</code> for the current line (double press = twice), or use <code>n ==</code> for multiple lines (n is a natural number) to automatically indent the next n lines from the current line</li><li>Or enter the visual line mode, select some lines and then <code>=</code> for formatting, which is equivalent to the code format in the general IDE.</li><li>Use <code>gg = G</code> to typeset the entire code.</li></ol><h3 id="Undo-and-Redo"><a href="#Undo-and-Redo" class="headerlink" title="Undo and Redo"></a>Undo and Redo</h3><ol><li><code>u</code> undo undoes the operation in the previous step. Commands can be combined. For example,<code>Nu</code> N is any integer, which means undoing the N-step operation. The following is the same.</li><li><code>U</code> restore the current line (that is, undo all operations on the current line at once)</li><li><code>ctr + r</code> control + redo restore the previous operation that was cancelled</li><li><code>CTRL-R</code> back to the previous command</li></ol><h3 id="Text-replacement"><a href="#Text-replacement" class="headerlink" title="Text replacement"></a>Text replacement</h3><p>Enter the replacement command in normal mode: <code>: [range] s / pattern / string / [flags]</code></p><p>-pattern is the string to be replaced, which can be expressed by regexp.<br>-string replaces pattern with string.<br>-[range] has the following values:</p><table><thead><tr><th>[range]</th><th>Meaning</th></tr></thead><tbody><tr><td>None</td><td>The default is the line where the cursor is located</td></tr><tr><td><code>.</code></td><td>Current line of cursor</td></tr><tr><td><code>N</code></td><td>Line N</td></tr><tr><td><code>$</code></td><td>Last line</td></tr><tr><td><code>&#39;a</code></td><td>Mark the line where a (marked with ma before)</td></tr><tr><td><code>. + 1</code></td><td>The line below the current cursor line</td></tr><tr><td><code>$ -1</code></td><td>The penultimate line, you can add or subtract a value to determine the relative line</td></tr><tr><td><code>22,33</code></td><td>Lines 22 to 33</td></tr><tr><td><code>1, $</code></td><td>Line 1 to last line</td></tr><tr><td><code>1, .</code></td><td>line 1 to current line</td></tr><tr><td><code>., $</code></td><td>Current line to last line</td></tr><tr><td><code>&#39;a,&#39; b</code></td><td>The line from the mark a to the line from the mark b (Ma and mb have been used to mark before)</td></tr><tr><td><code>%</code></td><td>All rows (equivalent to 1, $)</td></tr><tr><td><code>? str?</code></td><td>Search up from the current position to find the line of the first str (where str can be any string or regular expression)</td></tr><tr><td><code>/ str /</code></td><td>Search down from the current position to find the line of the first str (where str can be any string or regular expression)</td></tr><tr><td>** Note that all the above methods for range can be used to set the relative offset by + and-operations. **</td><td></td></tr></tbody></table><p>-[flags] has the following values:</p><table><thead><tr><th>flags</th><th>meaning</th></tr></thead><tbody><tr><td><code>g</code></td><td>Replace all matches (global) within the specified range</td></tr><tr><td><code>c</code></td><td>Request user confirmation before replacement</td></tr><tr><td><code>e</code></td><td>Ignore errors during execution</td></tr><tr><td><code>i</code></td><td>ignore case-insensitive</td></tr><tr><td>None</td><td>Only the first match in the specified range is replaced</td></tr></tbody></table><p>** Note: All the above flags can be used in combination. For example, gc means to replace all matching items in the specified range, and the user will be asked to confirm before each replacement. **</p><h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><p>Replace some lines</p><ol><li><code>: 10,20s / from / to / g</code> replaces lines 10 to 20.</li><li><code>: 1, $ s / from / to / g</code> replaces the contents of the first line to the last line (that is, all text)</li><li><code>: 1, .s / from / to / g</code> replaces the content from the first line to the current line.</li><li><code>:., $ S / from / to / g</code> replaces the content from the current line to the last line.</li><li><code>: &#39;a,&#39; bs / from / to / g</code> replaces the line between the marks a and b (including the line where a and b are located), where a and b are the marks previously made with the m command .</li></ol><p>Replace all lines: <code>:% s / from / to / g</code></p><h2 id="Repeat-the-action"><a href="#Repeat-the-action" class="headerlink" title="Repeat the action"></a>Repeat the action</h2><p>In normal mode, any action can be repeated</p><p>Note: N is a number</p><p>-Numbers: <code>Nyy</code> copies N lines down from the current line,<code>Ndd</code> deletes N lines down from the current line, <code>Ngg</code> jumps to the Nth line, and<code>dNw</code> deletes from the current cursor to the first Before N words (does not contain blanks, ie delete N-1 words), <code>yNe</code> copies from the current cursor to the end of the Nth word (note:<code>yy</code> = <code>1yy`` dd</code> = <code>1dd</code>) <code>d $</code> delete to the end of the line<br>-Repeat the previous command: <code>N.</code> (N indicates the number of repetitions)</p><h2 id="Block-selection"><a href="#Block-selection" class="headerlink" title="Block selection"></a>Block selection</h2><p>Note: The brackets are optional</p><p>In normal mode: <code>[ctr +] v + (h or j or k or l)</code></p><ol><li>** High-frequency use scenario 1 *<em>: <code>[ctr +] v</code> Select some line headers and then press<code>=</code>Effect: *</em> Code format is automatically adjusted **</li><li>** High frequency usage scenario 2 *<em>: <code>[ctr +] v</code> After selecting the head of some lines, press<code>I</code>, then press the comment symbol (eg: <code>//</code>) and finally press <code>ESC</code> : All the selected lines are commented *</em> Multi-line quick comment **</li><li>** High frequency usage scenario 3 *<em>: <code>[ctr +] v</code> After selecting some line headers, then press<code>A</code>, then press the contents of the comment and finally press <code>ESC</code> (for example:<code>// This is a test Code</code>) Effect: All the lines at the end of the selected lines are commented on.//This is the test code` *</em> Multi-line quick comment **</li><li>** High-frequency use scenario 4 *<em>: <code>[ctr +] v</code> select some line header comments (for example:<code>//</code>), then press<code>d</code> and finally press <code>ESC</code> effect: selected All these lines are commented out. *</em> Multi-line quick delete comments ** </li><li>** High-frequency use scenario 5 **: After selecting certain blocks, press [ctr +] v` and then press the above-mentioned action buttons to achieve regional operation</li></ol><h2 id="Powerful-combination"><a href="#Powerful-combination" class="headerlink" title="Powerful combination"></a>Powerful combination</h2><h3 id="Operate-a-word-under-the-cursor"><a href="#Operate-a-word-under-the-cursor" class="headerlink" title="Operate a word under the cursor"></a>Operate a word under the cursor</h3><p>In normal mode:</p><p>Action + Move [+ Number of Repeats]<br>The combination has already been used a lot in the past, continue here:</p><table><thead><tr><th>Action Operation Command + Range</th><th>Effect</th></tr></thead><tbody><tr><td>cw or c1 or c1w</td><td>change from current cursor to word end</td></tr><tr><td>caw</td><td>change whole word including current cursor</td></tr><tr><td>dw or d1 or d1w</td><td>delete from current cursor to word end</td></tr><tr><td>daw</td><td>delete whole word including current cursor</td></tr><tr><td>yw or y1 or y1w</td><td>copy from current cursor to word end</td></tr><tr><td>yaw</td><td>copy whole word including current cursor</td></tr><tr><td>d / word</td><td>delete forward until the former character of the next ‘word’</td></tr><tr><td>d? word</td><td>delete backward until the former character of the last ‘word’</td></tr></tbody></table><table><thead><tr><th>Action Operation Command + Range</th><th>Effect</th></tr></thead><tbody><tr><td>dtc</td><td>delete until before the next ‘c’</td></tr><tr><td>dfc</td><td>delete until after the next ‘c’</td></tr><tr><td>Scope + Action Operation Instructions</td><td>Effects</td></tr><tr><td>———————</td><td>———-</td></tr><tr><td><code>bve</code> or<code>BvE</code> + c / d / y</td><td>manipulate a variable or string</td></tr></tbody></table><p>The above table are high-frequency use scenarios</p><h2 id="Autocomplete"><a href="#Autocomplete" class="headerlink" title="Autocomplete"></a>Autocomplete</h2><p>Press directly in insert mode: the most commonly used completion</p><p><code>`ctrl + n  ctrl + p</code> `</p><p>Smart completion</p><p><code>`ctrl + x // Enter completion mode</code> `</p><p>-Complete line completion <code>CTRL-X`` CTRL-L</code><br>-** Complete based on keywords in the current file ** <code>CTRL-X`` CTRL-N</code><br>-** Complete according to dictionary ** <code>CTRL-X`` CTRL-K</code><br>-Completion of <code>CTRL-X`` CTRL-T</code> according to thesaurus<br>-** Complete according to keywords in header file ** <code>CTRL-X`` CTRL-I</code><br>-Completion of <code>CTRL-X`` CTRL-]</code>according to tags<br>-** Complete file name ** <code>CTRL-X`` CTRL-F</code><br>-Completion of macro definition <code>CTRL-X`` CTRL-D</code><br>-Completion of vim command <code>CTRL-X`` CTRL-V</code><br>-User-defined completion method <code>CTRL-X`` CTRL-U</code><br>-** Spelling suggestions ** <code>CTRL-X`` CTRL-S</code> // For example: an English word</p><h2 id="Collapse"><a href="#Collapse" class="headerlink" title="Collapse"></a>Collapse</h2><p>In normal mode:</p><p><code>`zo (fold + open)zi (fold + indent)zc (fold + close)</code> `</p><h2 id="Split-screen"><a href="#Split-screen" class="headerlink" title="Split screen"></a>Split screen</h2><p>** Split command **, in normal mode, enter</p><ol><li><code>vs</code> (Description: Vertically split the screen vertically)</li><li><code>sp</code> (Note: split horizontally splits the screen, which is the default splitting method)</li></ol><p>** Screens jump to each other **</p><ol><li><code>ctr + w</code> and then press<code>h</code> or <code>j</code> or<code>k</code> or <code>l</code></li><li>Explanation: <code>h</code>: left,<code>j</code>: down, <code>k</code>: up,<code>l</code>: right</li></ol><p>** Resize the split window **</p><ol><li><code>ctrl + w</code> before pressing<code>+</code>or<code>-</code> or <code>=</code>, of course press a number before pressing <code>+</code> or <code>-</code> or<code>=</code>to change the window height,<code>=</code>is equal The meaning of points. .</li><li>In normal mode, enter <code>: resize -N</code> or<code>: resize + N</code> to explicitly specify that the window is reduced or increased by N lines</li><li><code>ctrl + w</code> before pressing<code>&lt;</code>or<code>&gt;</code>or<code>=</code>, of course press a number before pressing<code>&lt;</code>or<code>&gt;</code>or<code>=</code>to change the window width,<code>=</code>is equal The meaning of points.</li><li>Sometimes preview large files and feel that the split screen is too small, <code>ctrl + w</code> +<code>T</code> moves the current window to a new tab.</li></ol><h2 id="tab-window"><a href="#tab-window" class="headerlink" title="tab window"></a>tab window</h2><p>Vim has added the function of multi-tab switching since vim7, which is equivalent to multiple windows. Although the previous version also has a multi-file editing function, it is not as convenient as this. Usage in normal mode:</p><p>-<code>: tabnew</code> [++ opt option] [＋ cmd] The file creates a new tab for the specified file<br>-<code>: tabc</code> closes the current tab or<code>: q</code><br>-<code>: tabo</code> close ** other ** tab<br>-<code>: tabs</code> view ** all open ** tabs<br>-<code>: tabp</code> previous tab window<br>-<code>: tabn</code> next tab window</p><p> <code>gt</code>,<code>gT</code> can switch directly between tabs. There are many other commands,: help table.</p><h2 id="table-of-Contents"><a href="#table-of-Contents" class="headerlink" title="table of Contents"></a>table of Contents</h2><p>In normal mode:</p><ol><li><code>: Te</code> displays the current directory in the form of a tab window, then you can switch directories and open a file</li><li><code>:! Ls</code> This is how vim invokes shell commands<code>:! Ls + shell_command</code>, but it does not display the current directory in the form of a tab window.</li></ol><h2 id="Content-operation-of-paired-symbols"><a href="#Content-operation-of-paired-symbols" class="headerlink" title="Content operation of paired symbols"></a>Content operation of paired symbols</h2><p>The following commands can operate on the content within punctuation:</p><ol><li><code>ci&#39;`` ci &quot;</code> <code>ci (</code> <code>ci [</code> <code>ci {</code> <code>ci &lt;</code> respectively change the text content of these paired punctuation marks </li><li><code>di&#39;``di&quot;</code>di (<code>or</code>dib<code>di [</code> di {<code>or</code> diB<code></code>di &lt;` delete the text content of these paired punctuation marks respectively </li><li><code>yi&#39;`` yi &quot;</code> <code>yi (</code> <code>yi [</code> <code>yi {</code> <code>yi &lt;</code> separately copy the text content of these paired punctuation marks </li><li><code>vi&#39;`` vi &quot;</code> <code>vi (</code> <code>vi [</code> <code>vi {</code> <code>vi &lt;</code> respectively select the text content of these paired punctuation marks</li><li><code>cit`` dit</code> <code>yit`` vit</code> separately operate the content between a pair of tags, editing html is very useful</li></ol><p>** In addition, if you change the above <code>i</code> to<code>a</code>, you can operate the paired punctuation and the content of the paired punctuation at the same time **, for example:</p><p>For example, the text to be operated: 111 “222” 333, move the cursor to any character of “222” and enter the command </p><p>-di “, the text will become: 111” “333<br>-If you enter the command da “, the text will become: 111333</p><h2 id="Clipboard"><a href="#Clipboard" class="headerlink" title="Clipboard"></a>Clipboard</h2><h3 id="1-Simple-copy-and-paste"><a href="#1-Simple-copy-and-paste" class="headerlink" title="1. Simple copy and paste"></a>1. Simple copy and paste</h3><p>vim provides 12 clipboards, their names are vim and there are 11 pasteboards, which are 0, 1, 2, …, 9, a, “. If you open the system clipboard, there will be two more + And *. Use the: reg command to view the contents of each pasteboard.</p><p>In vim, simply use <code>y</code> and just copy it to the clipboard of<code>&quot;</code>, and the same thing as paste with <code>p</code> is also the content of this clipboard.</p><h3 id="2-Copy-and-paste-to-the-specified-clipboard"><a href="#2-Copy-and-paste-to-the-specified-clipboard" class="headerlink" title="2. Copy and paste to the specified clipboard"></a>2. Copy and paste to the specified clipboard</h3><p>To copy the content of vim to a pasteboard, enter the normal mode, select the content to be copied, then press <code>&quot; Ny</code> to complete the copy, where N is the pasteboard number (note that you click the double quotes and then press the pasteboard number Finally, press y), for example, to copy the content to the pasteboard a, select the content and press “ay”.</p><p>To paste the contents of a vim pasteboard, you need to exit the editing mode, press <code>&quot; Np</code> in the normal mode, where N is the pasteboard number. For example, you can press <code>&quot; 5p</code> to paste the contents of pasteboard No. If you paste it in, you can also press <code>&quot; + p</code> to paste the content in the system global pasteboard.</p><h3 id="3-System-Clipboard"><a href="#3-System-Clipboard" class="headerlink" title="3. System Clipboard"></a>3. System Clipboard</h3><p>To view the clipboard supported by vim, enter <code>: reg</code> in normal mode</p><p>How should it be used to interact with the system clipboard? When you encounter problems, the first thing you are looking for is the help document. The Clipboard is the Clipboard. View help via <code>: h clipboard</code></p><p>The asterisk * and plus sign + pasteboard are system pasteboards. Under Windows, the * and + clipboards are the same. For X11 systems, * the clipboard stores the selected or highlighted content, + the clipboard stores the copied or cut content. Open the clipboard option, you can access + clipboard; open xterm_clipboard, you can access * clipboard. * One function of the clipboard is that the content selected in one window of vim can be taken out in another window of vim.</p><p>** Copy to system clipboard **</p><p>example:</p><p><code>&quot; * y</code> <code>&quot; + y</code> <code>&quot; + Nyy</code> copy N lines to the system clipboard</p><p>Explanation:</p><table><thead><tr><th>Command</th><th>Meaning</th></tr></thead><tbody><tr><td>{Visual} “+ y</td><td>copy the selected text into the system clipboard</td></tr><tr><td>“+ y {motion}</td><td>copy the text specified by {motion} into the system clipboard</td></tr><tr><td>: [range] yank +</td><td>copy the text specified by [range] into the system clipboard</td></tr></tbody></table><p>** Cut to system clipboard **</p><p>example:</p><p>“+ dd</p><p>** Paste from system clipboard to vim **</p><p>In normal mode:</p><ol><li><code>&quot; * p</code></li><li><code>&quot; + p</code></li><li><code>: put +</code> Meaning: Ex command puts contents of system clipboard on a new line</li></ol><p>In insert mode:</p><p><code>&lt;Cr&gt; +</code> Meaning: From insert mode (or commandline mode)</p><p>“+ p is better than the Ctrl-v command, it can handle the pasting of large blocks of text faster and more reliably, and can also avoid the accumulation of automatic indentation at the beginning of each line when pasting a large amount of text, because Ctrl-v The system caches stream processing, processing the pasted text line by line.</p><h2 id="vim-encoding"><a href="#vim-encoding" class="headerlink" title="vim encoding"></a>vim encoding</h2><p>Vim can edit all kinds of character encoding files very well, which of course includes popular Unicode encoding methods such as UCS-2 and UTF-8.</p><p>Four character encoding options, encoding, fileencoding, fileencodings, termencoding (for possible values ​​of these options, please refer to Vim online help: help encoding-names, their meanings are as follows:</p><p>-** encoding **: The character encoding used internally by Vim</p><p>Including Vim’s buffer (buffer), menu text, message text, etc. The default is to choose according to your locale. The user manual recommends changing its value only in .vimrc. In fact, it seems that it only makes sense to change its value in .vimrc. You can use another encoding to edit and save the file. For example, the encoding of your vim is utf-8, and the edited file uses cp936 encoding. Vim will automatically convert the read file into utf-8 (vim can read) Understand the way), and when you write a file, it will automatically switch back to cp936 (the file save encoding).</p><p>-** fileencoding **: The character encoding of the currently edited file in Vim</p><p>When Vim saves the file, it will also save the file in this character encoding (regardless of whether it is a new file).</p><p>-** fileencodings **: Vim will automatically detect encoding settings</p><p>When starting, it will detect the character encoding method of the file to be opened one by one according to the character encoding method it lists, and set fileencoding to the character encoding method finally detected. Therefore, it is best to put the Unicode encoding method at the front of this list and the Latin encoding method latin1 at the end.</p><p>-** termencoding **: The character encoding method of the terminal (or Console window of Windows) that Vim works on</p><p>If the term where vim is located is the same as the vim encoding, you do not need to set it. If it is not the case, you can use vim’s termencoding option to automatically convert to term encoding. This option is not valid for gVim in our common GUI mode under Windows, and for Console mode Vim is the code page of the Windows console, and Usually we don’t need to change it. Okay, after explaining this bunch of parameters that are easy to confuse novices, let’s take a look at how Vim’s multi-character encoding support works.</p><ol><li>Start Vim, and set the character encoding of buffer, menu text, and message according to the encoding value set in .vimrc.</li><li>Read the file to be edited, and detect the file encoding method one by one according to the character encoding methods listed in fileencodings. And set fileencoding to be detected, it seems to be the correct (Note 1) character encoding.</li><li>Compare the values ​​of fileencoding and encoding. If they are different, call iconv to convert the file content to the character encoding method described by encoding, and put the converted content into the buffer developed for this file. This file. Note that to complete this step, you need to call the external iconv.dll (Note 2). You need to ensure that this file exists in $ VIMRUNTIME or other directories listed in the PATH environment variable.</li><li>When saving the file after editing, compare the values ​​of fileencoding and encoding again. If different, call iconv again to convert the text in the buffer to be saved to the character encoding described by fileencoding and save it to the specified file. Similarly, this requires calling iconv.dll. Because Unicode can contain characters of almost all languages, and UTF-8 encoding of Unicode is a very cost-effective encoding (space consumption is less than UCS-2), so the value of encoding is recommended. Set to utf-8. Another reason for this is that when encoding is set to utf-8, Vim automatically detects the encoding of the file to be more accurate (perhaps this reason is the main reason;). For the files we edit in Chinese Windows, in order to take into account the compatibility with other software, the file encoding is still set to GB2312 / GBK, so fileencoding is recommended to be set to Chinese (chinese is an individual name, which means gb2312 in Unix and Windows. cp936, which is the code page of GBK).</li></ol><p>For fedora, vim settings are generally placed in the / etc / vimrc file, however, it is recommended not to modify it. You can modify the ~ / .vimrc file (which does not exist by default, you can create a new one yourself) and write the desired settings.</p><p>My .vimrc file is as follows:</p><p><code>`: set encoding = utf-8: set fileencodings = ucs-bom, utf-8, cp936: set fileencoding = gb2312: set termencoding = utf-8</code> `</p><p>Among them, the fileencoding configuration can set utf-8, but my mp3 does not seem to support utf-8 encoding, so simply, I set it to gb2312. Now it’s done, no matter whether it is in vi or mp3, it can display .txt files without garbled characters.</p><h2 id="Personal-configuration"><a href="#Personal-configuration" class="headerlink" title="Personal configuration"></a>Personal configuration</h2><p>The configuration during my use of ** without plugin ** is very short. It is written in the vim configuration file. Vimrc. The configuration is configured using ** vim script **. It has its own set of syntax. For details, please click [ vim Script] (<span class="exturl" data-url="aHR0cHM6Ly93d3cudzNjc2Nob29sLmNuL3ZpbS9uY2t4MXB1MC5odG1s">https://www.w3cschool.cn/vim/nckx1pu0.html<i class="fa fa-external-link-alt"></i></span>)</p><p><code>`vimset number; display numberset mouse = a; setting smart mouseset hlsearch; high light searchset tabstop = 4; setting tab width 4 lettersset shiftwidth = 4; setting new line incident widthset noexpandtab; tab doesn&#39;t expand to space; set list; display manipulator, example: \ n \ t \ r ......set encoding = utf-8set fileencodings = ucs-bom, utf-8, cp936set fileencoding = gb2312set termencoding = utf-8</code> `</p><h2 id="Forward-and-backward-function"><a href="#Forward-and-backward-function" class="headerlink" title="Forward and backward function"></a>Forward and backward function</h2><p>Popular text editors usually have forward and backward functions, and can move back and forth between the positions that have been viewed in the file (associated with the browser), use <code>Ctrl-O</code> in vim to perform backward, use<code>Ctrl-I</code>Execute forward, related help:<code>: help CTRL-O</code> <code>: help CTRL-I``: help jump-motions</code></p><h2 id="vim-compare-files"><a href="#vim-compare-files" class="headerlink" title="vim compare files"></a>vim compare files</h2><h3 id="Starting-method"><a href="#Starting-method" class="headerlink" title="Starting method"></a>Starting method</h3><p>First ensure that the diff command is available in the system. Vim’s diff mode is dependent on the diff command.</p><code>`shell vimdiff file1 file2 [file3 [file4]]</code> <code>or``</code>shell<br>vim -d file1 file2 [file3 [file4]]<br><code>`The window is more localized in the current tab. You cannot see the difference between a window and the windows in other tabs. In this way, multiple groups of comparison windows can be opened at the same time, and each group of differences is in a separate tab. Vim will open a window for each file and use ** vertical split ** just like the `-O` parameter. If you want to ** horizontal split **, add `-o` parameter:</code> <code>shellvimdiff -o file1 file2 [file3 [file4]]``</code><br>If you are already in Vim, you can enter the comparison mode in three ways, only one is introduced:<br><code>`: diffs [plit] {filename}</code> <code>Open a new window for {filename}. The current and newly opened windows will have the same parameters as &quot;vimdiff&quot;. To split the window vertically, add</code>: vertical<code>in front. E.g:<dl><dt>``</code></dt><dd>vert diffsplit another_filename<br>`` `</dd></dl><h3 id="Jump-to-the-difference"><a href="#Jump-to-the-difference" class="headerlink" title="Jump to the difference"></a>Jump to the difference</h3><p>There are two commands that can be used to jump to the location of the difference text:</p><ol><li><code>[c</code> jumps backwards to the beginning of the last change. Count the prefix so that it repeats the corresponding times.</li><li><code>] c</code> jumps forward to the beginning of the next change. Count the prefix so that it repeats the corresponding times.<br> If there is no change to which the cursor can jump, an error will be generated.</li></ol><h3 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h3><p>The purpose of comparison is to merge the differences and directly use the following built-in commands or troublesome methods: manually copy from one window to another.</p><dl><dt>`` `<br>: [range] diffg [et] [bufspec]<br>                Use another buffer to modify the current buffer to eliminate differences. Unless there is only another<br>                For a buffer in comparison mode, [bufspec] must exist and specify that buffer.<br>                If [bufspec] specifies the current buffer, it is an empty action. [range] can refer to the following.</dt><dd>[range] diffpu [t] [bufspec]<br>                Use the current buffer to modify another buffer, eliminating differences.<br>[count] do Same as “: diffget”, but without scope. “o” means “obtain” (not available<br>                “dg” because that might be the beginning of “dgg”! ).<br>dp Same as “: diffput”, but no range. Note that it does not apply to the visual mode.<br>                The given [count] is used as the [bufspec] parameter of “: diffput”.</dd></dl><p>When [range] is not given, only the difference text at or above the current cursor position will be affected.<br>When [range] is specified, Vim tries to change only the lines it specifies. However, this is not always effective when there are deleted rows.<br>The parameter [bufspec] can be the serial number of the buffer, matching the pattern of the buffer name or part of the buffer name.<br>E.g:<br>        : diffget uses another buffer to enter compare mode<br>        : diffget 3 uses buffer 3<br>        : diffget v2 uses the buffer whose name matches “v2” and enters the comparison mode (for example, “file.c.v2”)<br>`` `</p><h3 id="Update-comparison-and-undo-changes"><a href="#Update-comparison-and-undo-changes" class="headerlink" title="Update comparison and undo changes"></a>Update comparison and undo changes</h3><p>Compare the contents based on the buffer. Therefore, if you have made changes after loading the file, these changes will also participate in the comparison. However, you may want to use <code>: diffupdate [!]</code> From time to time. Because not all changes can be automatically updated. When <code>!</code> Is included, Vim checks whether the file has been changed externally and needs to be reloaded. Prompt for each changed file.</p><p>If you want to undo the modification, you can enter the normal mode directly and press <code>u</code> as you normally do in vim editing, but you must be careful to move the cursor to the file window that needs to be undoed.</p><h3 id="Expand-and-view-context"><a href="#Expand-and-view-context" class="headerlink" title="Expand and view context"></a>Expand and view context</h3><p>When comparing and merging files, it is often necessary to combine context to determine the final action to be taken. By default, Vimdiff will display the six lines of text above and below the differences for reference. Other identical lines of text are automatically folded. If you want to modify the default number of context lines, you can set it like this:</p><p> <code>`: set diffopt = context: 3</code> `</p><h3 id="Exit-of-multiple-files"><a href="#Exit-of-multiple-files" class="headerlink" title="Exit of multiple files"></a>Exit of multiple files</h3><p>After comparing and merging to a conclusion, you can use the following commands to operate on multiple files at the same time.</p><p>For example, quit at the same time: <code>: qa (quit all)</code>  </p><p>If you want to save all files: <code>: wa (write all)</code></p><p>Or a merge command of the two, save all files, and then exit: <code>: wqa (write, then quit all)</code></p><p>If you do not want to save the result of any operation when you quit: <code>: qa! (Force to quit all)</code></p><h3 id="vimdiff-For-details-please-refer-to"><a href="#vimdiff-For-details-please-refer-to" class="headerlink" title="vimdiff For details, please refer to"></a>vimdiff For details, please refer to</h3><ol><li><code>: help diff</code> under vim</li><li>[vimdiff doc] (<span class="exturl" data-url="aHR0cHM6Ly92aW1jZG9jLnNvdXJjZWZvcmdlLm5ldC9kb2MvZGlmZi5odG1s">https://vimcdoc.sourceforge.net/doc/diff.html<i class="fa fa-external-link-alt"></i></span>)</li></ol><h2 id="vim-command-line-save-leave-and-other-commands"><a href="#vim-command-line-save-leave-and-other-commands" class="headerlink" title="vim command line save, leave and other commands:"></a>vim command line save, leave and other commands:</h2><ol><li><code>: w</code> Write the edited data to the hard disk file.</li><li><code>: w!</code> If the file attribute is “read-only”, write to the file forcibly. But the ability to write is also dependent on the file permissions of the file.</li><li>Leave after <code>: q</code>. If it is “: wq!”, It will be forced to save and leave.</li><li><code>: w [file name]</code> Save the edited data as another file.</li><li><code>: r [file name]</code> Read the content of another file in the edited data and add it to the line behind the cursor.</li><li><code>: n1, n2 w [file name]</code> Save the contents of lines n1 to n2 to another file.</li><li><code>:! Command</code> Temporarily leave vi to display the result of executing command in command line mode.</li><li><code>ZZ</code> If the file has not been changed, leave directly; if it has been changed, leave after saving.</li><li><code>set num / nonum</code> displays / cancels the line number.</li></ol><h2 id="Macro-of-VIM"><a href="#Macro-of-VIM" class="headerlink" title="Macro of VIM"></a>Macro of VIM</h2><p>The use of macros is very powerful, go to [vim, the use of macros] (<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnNpbmEuY29tLmNuL3MvYmxvZ182OWU1ZDg0MDAxMDJ3MXoxLmh0bWw=">https://blog.sina.com.cn/s/blog_69e5d8400102w1z1.html<i class="fa fa-external-link-alt"></i></span>)</p><h2 id="Full-version-command"><a href="#Full-version-command" class="headerlink" title="Full version command"></a>Full version command</h2><p>This article only provides high-frequency scenes accumulated during personal use. For the full version, please click [here] (<span class="exturl" data-url="aHR0cHM6Ly9xNmdtOGZvbXcuYmt0LmNsb3VkZG4uY29tL2dpdHBhZ2UvdmltL3ZpbV9jb21tYW5kLnBuZw==">https://q6gm8fomw.bkt.clouddn.com/gitpage/vim/vim_command.png<i class="fa fa-external-link-alt"></i></span>), or consult the vim manual</p><h2 id="Play-games-to-make-perfect"><a href="#Play-games-to-make-perfect" class="headerlink" title="Play games to make perfect"></a>Play games to make perfect</h2><p>** Use advance and retreat **, so multi-use is king. I recommend a game here: use the keyboard to control the character adventure game, familiar with VIM commands during the game: [vim-adventures] (https: // vim -adventures.com/)</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="** Reference **"></a>** Reference **</h2><ol><li>[Official document] (<span class="exturl" data-url="aHR0cHM6Ly92aW0uc291cmNlZm9yZ2UuaW8vZG9jcy5waHA=">https://vim.sourceforge.io/docs.php<i class="fa fa-external-link-alt"></i></span>)</li><li>[vim doc] (<span class="exturl" data-url="aHR0cHM6Ly92aW1jZG9jLnNvdXJjZWZvcmdlLm5ldC9kb2MvaGVscC5odG1s">https://vimcdoc.sourceforge.net/doc/help.html<i class="fa fa-external-link-alt"></i></span>) Chinese</li><li>[freewater blog] (<span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vZnJlZXdhdGVyL2FyY2hpdmUvMjAxMS8wOC8yNi8yMTU0NjAyLmh0bWw=">https://www.cnblogs.com/freewater/archive/2011/08/26/2154602.html<i class="fa fa-external-link-alt"></i></span>)</li><li>[Thinking In Linux] (<span class="exturl" data-url="aHR0cHM6Ly93d3cubGludXhzb25nLm9yZy8yMDEwLzA5L3ZpbS1xdWljay1zZWxlY3QtY29weS1kZWxldGUv">https://www.linuxsong.org/2010/09/vim-quick-select-copy-delete/<i class="fa fa-external-link-alt"></i></span>)</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article Covers everything related to be going with Vim for any beginner or intermediate.&lt;br&gt;I have explained the ins and outs of Vim in briefly.&lt;br&gt;This article will be continuously updated based on experience.&lt;/p&gt;
    
    </summary>
    
    
      <category term="IDE" scheme="https://massivefile.com/categories/IDE/"/>
    
    
      <category term="linux" scheme="https://massivefile.com/tags/linux/"/>
    
      <category term="IDE" scheme="https://massivefile.com/tags/IDE/"/>
    
  </entry>
  
  <entry>
    <title>Classic Excerpt-Normal Random Variables</title>
    <link href="https://massivefile.com/NormalRandomVariables%20/"/>
    <id>https://massivefile.com/NormalRandomVariables%20/</id>
    <published>2017-08-27T14:46:00.000Z</published>
    <updated>2020-04-12T06:23:02.415Z</updated>
    
    <content type="html"><![CDATA[<p><b>The full text is taken from [Introduction to probability, 2nd Edition]</b><br>3.3 normal random variables</p><a id="more"></a><h2 id="Normal-random-variables"><a href="#Normal-random-variables" class="headerlink" title="Normal random variables"></a>Normal random variables</h2><p>If the probability density of a continuous random variable $ X $ has the following form, then the random variable is called normal or Gaussian.<br>$$<br>f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}<br>$$<br>Among them, $ u $ and $ \ sigma $ are two parameters of the density function, and $ \ sigma $ must also be a positive number. It can be proved that $ f_X (x) $ satisfies the normalization condition of the following probability density function (see the exercises on theorems in this chapter):<br>$$<br>\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx=1<br>$$<br>The figure below is the density function and distribution function of the normal distribution $ (\mu=1 \text{ and } \sigma^2=1) $.</p><center><a href='https://photos.google.com/share/AF1QipOQ9XBst8wbXLscf7F_YCqEW1oyqWeUQAij0FXw1BNHtwEMXIrKQ26Bi1ZLk6Cpow?key=cFViaFdDb3oxcVBudDJoQmZFOTlfSWtHYk05aEJR&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/Eyz-hNy4QsZEFVYqFafTXj8GDlJcZi_B509b8xlUXqU2o2lIhh_-CMyKGzR0Hgt5DETP3fI0jvwQZQpCel8XwX0jDN84KijUeCCfKrg-VcZcSZaw8Lsj8W2215kHIlJJQyow3OALQw=w2400' /></a> [Figure : [A_normal_PDF_and_CDF_with_u = 1_and_sigmal ^ 2 = 1]</center><br><br> <p>It can be seen from the figure that the probability density function of a normal random variable is a symmetrical bell curve with respect to the mean $ \ mu $. When $ x $ leaves $ \ mu $, the term $ e ^ {\ frac {-(x- \ mu) ^ 2} {2 \ sigma ^ 2}} $ in the expression of the probability density function quickly decline. In the figure, the probability density function is very close to $ 0 $ outside the interval $ [-1,3] $.</p><p>The mean and variance of a normal random variable can be given by the following formula:<br>$$<br>E[X]=\mu,\quad var(X)=\sigma^2<br>$$<br>Since the probability density function of $ X $ is symmetric with respect to $ \ mu $, its mean can only be $ \ mu $. As for the formula of variance, one sentence is defined as:<br>$$<br>var(X)=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}(x-\mu)^2e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx<br>$$<br>Replacing the integral in the formula as an integral variable $ y = \ frac { ( x - \ mu ) } { \ sigma } $ and the distributed integral yields:</p><center><a href='https://photos.google.com/share/AF1QipOD7l3rtTBfwCMk9n1wZzl4SjWirtPE1pyhKEug4BaTn5L8dujwQtoPfMN5QY8Egg?key=NlJ3UEZpbGNhSFZyZjVXc3MxdVJBbjVEaHN6R3pn&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/QVoXyim2mP0uEHramWNPMdpBBeEZg0bGZlxv0rYx6lUN-gqlYQbVyabn0YaSTyHSA5MFkjr2oL6wQFtsjubdNslVQjE-wLKb1AAXNjDDxJ8b9MN6krAgNl1DDQSvPDD78_Y8IWRSoA=w2400' /></a></center><p>The last equation above is due to<br>$$<br>\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{-\frac{y^2}{2}}dy=1<br>$$<br>This formula happens to be the normalization condition of the probability density function of the normal random variable when $ \ mu = 0 $ and $ \ sigma ^ 2 = 1 $. The problem is proved in the problem 14 of this chapter. The screenshot is as follows:</p><center><<a href='https://photos.google.com/share/AF1QipNBs4d13V1vfL98XCuW9mFcqqT8AxG0xDVUsqHZ1f-KJgKcY-k65fgHMay1btur-Q?key=Qmp0TG5JbGxJbDdqemp1eExZdmtUeG05UnpISXJ3&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/BF16RgTjIo7M1vinN5aNbhRk-e6IXwSNQVjOOstZteK3fbb4K5RMw5c_Bgvi80qSwtOg6zgKbsmScyC1DnlC19-TPG_ovK67xrY1PhXl0EMFxtrzU94y9LqvzD_xC8jCR-xR5drviw=w2400' /></a> [Figure : the_normal_PDF_satisfies_the_normalization_property]</center><br><br> Normal random variables have several important properties. The following properties are particularly important and will be discussed in Chapter 4This is demonstrated in the first section of on Random Variables.<h3 id="The-normality-of-random-variables-remains-unchanged-under-linear-transformation"><a href="#The-normality-of-random-variables-remains-unchanged-under-linear-transformation" class="headerlink" title="The normality of random variables remains unchanged under linear transformation"></a>The normality of random variables remains unchanged under linear transformation</h3><p>Let $ X $ be a normal random variable, the mean of which is $ \ mu $ and the variance is $ \ sigma ^ 2 $. If $ a \ ne 0 $ and $ b $ are two constants, then the random variable<br>$$<br>Y = aX + b<br>$$<br>It is still a normal random variable, and its mean and variance are given by the following formula:<br>$$<br>E[Y]=a\mu+b,\quad var(Y)=a^2\sigma^2<br>$$</p><h2 id="Standard-normal-random-variables"><a href="#Standard-normal-random-variables" class="headerlink" title="Standard normal random variables"></a>Standard normal random variables</h2><p>Suppose the expectation of the normal random variable $ Y $ is $ 0 $ and the variance is $ 1 $, then $ Y $ is called the standard normal random variable. Let $ \ Phi $ be its CDF:<br>$$<br>\Phi(y)=P(Y\le y)=P(Y&lt; y)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}e^{\frac{-t^2}{2}}dt<br>$$<br>It is usually listed as a table-the standard normal cumulative distribution table (see the table below), which is an important tool for calculating the probability of normal random variables. Each item of the standard normal table provides the value of $ \Phi(y)=P(Y\le y) $, where $ Y $ is a normal random variable, and in this table $ y\in [0,4.09] $. How to use this table? For example, to find the value of $ \Phi(1.71) $, look at the row where $ 1.7 $ is located and the column where $ 0.01 $ is located, and you get $ \Phi(1.71)=0.95637 $.</p><p>Note that the following table only lists the value of $ \Phi  ( y ) $ when  y&gt; 0, the symmetry of the probability density function of the standard normal random variable can be used, and $ \Phi ( y ) $ The value is derived. E.g:</p><center>Φ (−0.5) = P(Y ≤ −0.5) = P(Y ≥ 0.5) = 1−P(Y < 0.5)<br>= 1− Φ (0.5) = 1 − 0.69146 = 0.30854 <br><br> ∀ y> 0 , Φ(−y)= 1 − Φ (y)</center><br><br><table><thead><tr><th align="center">y</th><th>+0.00</th><th>+0.01</th><th>+0.02</th><th>+0.03</th><th>+0.04</th><th>+0.05</th><th>+0.06</th><th>+0.07</th><th>+0.08</th><th>+0.09</th></tr></thead><tbody><tr><td align="center">0.0</td><td>0.50000</td><td>0.50399</td><td>0.50798</td><td>0.51197</td><td>0.51595</td><td>0.51994</td><td>0.52392</td><td>0.52790</td><td>0.53188</td><td>0.53586</td></tr><tr><td align="center">0.1</td><td>0.53983</td><td>0.54380</td><td>0.54776</td><td>0.55172</td><td>0.55567</td><td>0.55966</td><td>0.56360</td><td>0.56749</td><td>0.57142</td><td></td></tr><tr><td align="center">0.2</td><td>0.57926</td><td>0.58317</td><td>0.58706</td><td>0.59095</td><td>0.59483</td><td>0.59871</td><td>0.60257</td><td>0.60642</td><td>0.61026</td><td></td></tr><tr><td align="center">0.3</td><td>0.61791</td><td>0.62172</td><td>0.62552</td><td>0.62930</td><td>0.63307</td><td>0.63683</td><td>0.64058</td><td>0.64431</td><td>0.64803</td><td>0.65173</td></tr><tr><td align="center">0.4</td><td>0.65542</td><td>0.65910</td><td>0.66276</td><td>0.66640</td><td>0.67003</td><td>0.67364</td><td>0.67724</td><td>0.68082</td><td>0.68439</td><td></td></tr><tr><td align="center">0.5</td><td>0.69146</td><td>0.69497</td><td>0.69847</td><td>0.70194</td><td>0.70540</td><td>0.70884</td><td>0.71226</td><td>0.71566</td><td>0.71904</td><td>0.72240</td></tr><tr><td align="center">0.6</td><td>0.72575</td><td>0.72907</td><td>0.73237</td><td>0.73565</td><td>0.73891</td><td>0.74215</td><td>0.74537</td><td>0.74857</td><td>0.75175</td><td></td></tr><tr><td align="center">0.7</td><td>0.75804</td><td>0.76115</td><td>0.76424</td><td>0.76730</td><td>0.77035</td><td>0.77337</td><td>0.77637</td><td>0.77935</td><td>0.78230</td><td>0.78524</td></tr><tr><td align="center">0.8</td><td>0.78814</td><td>0.79103</td><td>0.79389</td><td>0.79673</td><td>0.79955</td><td>0.80234</td><td>0.80511</td><td>0.80785</td><td>0.81057</td><td>0.81327</td></tr><tr><td align="center">0.9</td><td>0.81594</td><td>0.81859</td><td>0.82121</td><td>0.82381</td><td>0.82639</td><td>0.82894</td><td>0.83147</td><td>0.83398</td><td>0.83646</td><td>0.83891</td></tr><tr><td align="center">1.0</td><td>0.84134</td><td>0.84375</td><td>0.84614</td><td>0.84849</td><td>0.85083</td><td>0.85314</td><td>0.85543</td><td>0.85769</td><td>0.85993</td><td></td></tr><tr><td align="center">1.1</td><td>0.86433</td><td>0.86650</td><td>0.86864</td><td>0.87076</td><td>0.87286</td><td>0.87493</td><td>​​0.87698</td><td>0.87900</td><td>0.88100</td><td></td></tr><tr><td align="center">1.2</td><td>0.88493</td><td>​​0.88686</td><td>0.88877</td><td>0.89065</td><td>0.89251</td><td>0.89435</td><td>0.89617</td><td>0.89796</td><td>0.89973</td><td></td></tr><tr><td align="center">1.3</td><td>0.90320</td><td>0.90490</td><td>0.90658</td><td>0.90824</td><td>0.90988</td><td>0.91149</td><td>0.91308</td><td>0.91466</td><td>0.91621</td><td>0.91774</td></tr><tr><td align="center">1.4</td><td>0.91924</td><td>0.92073</td><td>0.92220</td><td>0.92364</td><td>0.92507</td><td>0.92647</td><td>0.92785</td><td>0.92922</td><td>0.93056</td><td>0.93189</td></tr><tr><td align="center">1.5</td><td>0.93319</td><td>0.93448</td><td>0.93574</td><td>0.93699</td><td>0.93822</td><td>0.93943</td><td>0.94062</td><td>0.94179</td><td>0.94295</td><td>0.94408</td></tr><tr><td align="center">1.6</td><td>0.94520</td><td>0.94630</td><td>0.94738</td><td>0.94845</td><td>0.94950</td><td>0.95053</td><td>0.95154</td><td>0.95254</td><td>0.95352</td><td>0.95449</td></tr><tr><td align="center">1.7</td><td>0.95543</td><td>0.95637</td><td>0.95728</td><td>0.95818</td><td>0.95907</td><td>0.95994</td><td>0.96080</td><td>0.96164</td><td>0.96246</td><td>0.96327</td></tr><tr><td align="center">1.8</td><td>0.96407</td><td>0.96485</td><td>0.96562</td><td>0.96638</td><td>0.96712</td><td>0.96784</td><td>0.96856</td><td>0.96926</td><td>0.96995</td><td>0.97062</td></tr><tr><td align="center">0.99128</td><td>0.97193</td><td>0.97257</td><td>0.97320</td><td>0.97381</td><td>0.97441</td><td>0.97500</td><td>0.97558</td><td>0.97615</td><td>0.97670</td><td></td></tr><tr><td align="center">2.0</td><td>0.97725</td><td>0.97778</td><td>0.97831</td><td>0.97882</td><td>0.97932</td><td>0.97982</td><td>0.98030</td><td>0.98077</td><td>0.98124</td><td>0.98169</td></tr><tr><td align="center">2.1</td><td>0.98214</td><td>0.98257</td><td>0.98300</td><td>0.98341</td><td>0.98382</td><td>0.98422</td><td>0.98461</td><td>0.98500</td><td>0.98537</td><td></td></tr><tr><td align="center">2.2</td><td>0.98610</td><td>0.98645</td><td>0.98679</td><td>0.98713</td><td>0.98745</td><td>0.98778</td><td>0.98809</td><td>0.98840</td><td>0.98870</td><td></td></tr><tr><td align="center">2.3</td><td>0.98928</td><td>0.98956</td><td>0.98983</td><td>0.99010</td><td>0.99036</td><td>0.99061</td><td>0.99086</td><td>0.99111</td><td>0.99134</td><td>0.99158</td></tr><tr><td align="center">2.4</td><td>0.99180</td><td>0.99202</td><td>0.99224</td><td>0.99245</td><td>0.99266</td><td>0.99286</td><td>0.99305</td><td>0.99324</td><td>0.99343</td><td>0.99361</td></tr><tr><td align="center">2.5</td><td>0.99379</td><td>0.99396</td><td>0.99413</td><td>0.99430</td><td>0.99446</td><td>0.99461</td><td>0.99477</td><td>0.99492</td><td>0.99506</td><td>0.99520</td></tr><tr><td align="center">2.6</td><td>0.99534</td><td>0.99547</td><td>0.99560</td><td>0.99573</td><td>0.99585</td><td>0.99598</td><td>0.99609</td><td>0.99621</td><td>0.99632</td><td>0.99643</td></tr><tr><td align="center">2.7</td><td>0.99653</td><td>0.99664</td><td>0.99674</td><td>0.99683</td><td>0.99693</td><td>0.99702</td><td>0.99711</td><td>0.99720</td><td>0.99728</td><td>0.99736</td></tr><tr><td align="center">2.8</td><td>0.99744</td><td>0.99752</td><td>0.99760</td><td>0.99767</td><td>0.99774</td><td>0.99781</td><td>0.99788</td><td>0.99795</td><td>0.99801</td><td>0.99807</td></tr><tr><td align="center">2.9</td><td>0.99813</td><td>0.99819</td><td>0.99825</td><td>0.99831</td><td>0.99836</td><td>0.99841</td><td>0.99846</td><td>0.99851</td><td>0.99856</td><td>0.99861</td></tr><tr><td align="center">3.0</td><td>0.99865</td><td>0.99869</td><td>0.99874</td><td>0.99878</td><td>0.99882</td><td>0.99886</td><td>0.99889</td><td>0.99893</td><td>0.99896</td><td>0.99900</td></tr><tr><td align="center">3.1</td><td>0.99903</td><td>0.99906</td><td>0.99910</td><td>0.99913</td><td>0.99916</td><td>0.99918</td><td>0.99921</td><td>0.99924</td><td>0.99926</td><td>0.99929</td></tr><tr><td align="center">3.2</td><td>0.99931</td><td>0.99934</td><td>0.99936</td><td>0.99938</td><td>0.99940</td><td>0.99942</td><td>0.99944</td><td>0.99946</td><td>0.99948</td><td>0.99950</td></tr><tr><td align="center">3.3</td><td>0.99952</td><td>0.99953</td><td>0.99955</td><td>0.99957</td><td>0.99958</td><td>0.99960</td><td>0.99961</td><td>0.99962</td><td>0.99964</td><td>0.99965</td></tr><tr><td align="center">3.4</td><td>0.99966</td><td>0.99968</td><td>0.99969</td><td>0.99970</td><td>0.99971</td><td>0.99972</td><td>0.99973</td><td>0.99974</td><td>0.99975</td><td>0.99976</td></tr><tr><td align="center">3.5</td><td>0.99977</td><td>0.99978</td><td>0.99978</td><td>0.99979</td><td>0.99980</td><td>0.99981</td><td>0.99981</td><td>0.99982</td><td>0.99983</td><td>0.99983</td></tr><tr><td align="center">3.6</td><td>0.99984</td><td>0.99985</td><td>0.99985</td><td>0.99986</td><td>0.99986</td><td>0.99987</td><td>0.99987</td><td>0.99988</td><td>0.99988</td><td>0.99989</td></tr><tr><td align="center">3.7</td><td>0.99989</td><td>0.99990</td><td>0.99990</td><td>0.99990</td><td>0.99991</td><td>0.99991</td><td>0.99992</td><td>0.99992</td><td>0.99992</td><td>0.99992</td></tr><tr><td align="center">3.8</td><td>0.99993</td><td>0.99993</td><td>0.99993</td><td>0.99994</td><td>0.99994</td><td>0.99994</td><td>0.99994</td><td>0.99995</td><td>0.99995</td><td>0.99995</td></tr><tr><td align="center">3.9</td><td>0.99995</td><td>0.99995</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99997</td><td>0.99997</td></tr><tr><td align="center">4.0</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99998</td><td>0.99998</td><td>0.99998</td><td>0.99998</td></tr></tbody></table><p>Now use $ X $ to represent a normal random variable with a mean of $ \ mu $ and a variance of $ \ sigma ^ 2 $. Standardize $ X $ (“standardize”) by defining a new random variable $ Y $:<br>$$<br>Y=\frac{X-\mu}{\sigma}<br>$$<br>Because $ Y $ is a linear function of $ X $, $ Y $ is also a normal random variable. and<br>$$<br>E[Y]=\frac{E[X]-u}{\sigma}=0,\quad var(Y)=\frac{var(X)}{\sigma^2}=1<br>$$<br>Therefore, $ Y $ is a standard normal random variable. This fact allows us to redefine the event represented by $ X $ with $ Y $, and then use the standard normal table to calculate.</p><h4 id="Example-of-using-normal-distribution-table"><a href="#Example-of-using-normal-distribution-table" class="headerlink" title="Example of using normal distribution table"></a>Example of using normal distribution table</h4><p>The annual snowfall in a certain area is a normal random variable, the expectation is $ \ mu = 60 $ inches, and the standard deviation is $ \ sigma = 20 $. What is the probability that the snowfall will be at least $ 80 $ inches this year?</p><p>Let $ X $ be the annual snowfall, so that<br>$$<br>Y=\frac{X-\mu}{\sigma}=\frac{X-60}{20}<br>$$<br>Obviously $ Y $ is a standard normal random variable.<br>$$<br>P(X\ge 80)=P(\frac{X-60}{20} \ge \frac{80-60}{20})=P(Y\ge \frac{80-60}{20})=P(Y\ge 1)=1-\Phi(1)<br>$$<br>Where $ \ Phi $ is the standard normal cumulative distribution function. Obtained by querying the above table: $ \Phi(1)=0.84134 $, so<br>$$<br>P(X\ge 80)=1-\Phi(1)=0.15866<br>$$<br>Promoting the method in this example, we get the following:</p><h3 id="Calculation-of-the-cumulative-distribution-function-of-normal-random-variables"><a href="#Calculation-of-the-cumulative-distribution-function-of-normal-random-variables" class="headerlink" title="Calculation of the cumulative distribution function of normal random variables"></a>Calculation of the cumulative distribution function of normal random variables</h3><p>For a normal random variable $ X $ with a mean of $ \ mu $ and a variance of $ \ sigma ^ 2 $, use the following steps:</p><ol><li><p>Normalized $ X $: First subtract $ \ mu $ and then divide by $ \ sigma $ to obtain the standard random variable $ Y $.</p></li><li><p>Read the cumulative distribution function value from the standard normal table:<br> $$<br> P(X\le x)=P(\frac{X-\mu}{\sigma}\le \frac{x-\mu}{\sigma})=P(Y\le \frac{x-\mu}{\sigma})=\Phi(\frac{x-\mu}{\sigma})<br> $$<br> Normal random variables are often used in signal processing and communication engineering to model noise and signal distortion.</p></li></ol><h4 id="Example-3-8-Signal-detection"><a href="#Example-3-8-Signal-detection" class="headerlink" title="Example 3.8 Signal detection"></a>Example 3.8 Signal detection</h4><p>Binary information is transmitted with the signal $ s $. This information is either $ -1 $ and $ + 1 $. The signal will be accompanied by some noise during the channel transmission. The noise satisfies the normal distribution with a mean value of $ \ mu = 0 $ and a variance of $ \ sigma ^ 2 $. The receiver will receive a signal mixed with noise, if the received value is less than $ 0 $, then the signal is considered to be $ -1 $, if the received value is greater than $ 0 $, then the received signal is considered to be $ + 1 $. How big is the error of this judgment method?</p><p>The error only appears in the following two cases:</p><ol><li>The actual transmitted signal is $ -1 $, but the value of the noise variable $ N $ is at least $ 1 $, so $ s + N = -1 + N \ ge 0 $. </li><li>The actual transmitted signal is $ + 1 $, but the value of the noise variable $ N $ is less than $ -1 $. Therefore $ s + N = 1 + N &lt;0 $.<center><a href='https://photos.google.com/share/AF1QipOEgCcSYNlyL9HGGNtdY7J9a96EtPuB84nBGpJBrszrzoeDHPWZm9Pv6wbwdVW7Rg?key=ZlNEZFJjV0pUOFBNbDZWTE1kcE4tS01STFVVVmZn&source=ctrlq.org' target="_blank" rel="noopener"><img src='https://lh3.googleusercontent.com/26H7cHnyGWMkXDDwy3YHMmzla_X3yZAsbrmZnMNhZCVDD9qWInCvmgFj87feKbHyctBzaaSS8JI7_krDHoWXoTdhZAHqkdYB1NS_c_zrmznaASmeeu-773VILwZYUyTP4rFB6CYWUw=w2400' /></a>[Figure_3.11_The_signal_detection]</center> <br><br></li></ol><p>Therefore, the probability of error in this judgment method in case 1 is:</p><p>P ( N ≥ 1 ) = 1 − P ( N &lt; 1 ) = 1 − P  ( N &lt; 1 ) = 1 − P ( N − Μ / σ &lt; 1 − μ ) / σ )</p><p>= 1 - Φ( 1 − μ ) / σ)</p><p>= 1 - Φ( 1 / σ)</p><p>The probability of an error in the second case is obtained according to the symmetry of the normal distribution as in the previous case. $ \Phi (\frac {1} {\sigma}) $ can be obtained from the normal distribution table. For example, for $ \sigma = 1 $, $ \Phi (\frac {1} {\ sigma}) = \Phi (1) = 0.84134 $, so the probability of error is $ 0.15864 $.</p><p>Normal random variables play an important role in a wide range of probabilistic models. The reason is that normal random variables can well simulate the superposition effect of many independent factors in physics, engineering and statistics. Mathematically, the key fact is that the distribution of the sum of a large number of independent and identically distributed random variables (not necessarily normal) obey the normal distribution, and this fact has nothing to do with the specific distribution of each sum. This fact is the famous central limit theorem, which will be explained in detail in Chapter 5 of this book.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;The full text is taken from [Introduction to probability, 2nd Edition]&lt;/b&gt;&lt;br&gt;3.3 normal random variables&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning Maths" scheme="https://massivefile.com/categories/Machine-Learning-Maths/"/>
    
    
      <category term="probability" scheme="https://massivefile.com/tags/probability/"/>
    
  </entry>
  
</feed>
