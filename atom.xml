<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="https://www.w3.org/2005/Atom">
  <title>Machine Learning</title>
  
  <subtitle>Data Science</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://massivefile.com/"/>
  <updated>2020-04-14T19:33:56.028Z</updated>
  <id>https://massivefile.com/</id>
  
  <author>
    <name>Karan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Quiz 4 || Neural Networks and Deep Learning (Week 4)</title>
    <link href="https://massivefile.com/quiz4/"/>
    <id>https://massivefile.com/quiz4/</id>
    <published>2020-04-14T18:30:00.000Z</published>
    <updated>2020-04-14T19:33:56.028Z</updated>
    
    <content type="html"><![CDATA[<p>title: Quiz 4|| Deeplearnig.ai (Course - 1 Week - 4) Neural Networks and Deep Learning (Week 4)<br><b>Note - These are my notes on the first course of DeepLearning.ai by AndrewNg</b></p><a id="more"></a><iframe src="https://drive.google.com/file/d/1jnPSkG8QQoT6PwWJtDDojc4_X4ivoARi/preview" width="640" height="480"></iframe><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;title: Quiz 4|| Deeplearnig.ai (Course - 1 Week - 4) Neural Networks and Deep Learning (Week 4)&lt;br&gt;&lt;b&gt;Note - These are my notes on the first course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="2020" scheme="https://massivefile.com/categories/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/categories/2020/Quiz/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/tags/Quiz/"/>
    
  </entry>
  
  <entry>
    <title>Planar data classification with one hidden layer || Neural Networks and Deep Learning(Week 3)</title>
    <link href="https://massivefile.com/Planar_data_classification_with_one_hidden_layer/"/>
    <id>https://massivefile.com/Planar_data_classification_with_one_hidden_layer/</id>
    <published>2020-04-14T00:56:53.000Z</published>
    <updated>2020-04-14T19:29:28.333Z</updated>
    
    <content type="html"><![CDATA[<p>Planar data classification with one hidden layer || Deeplearning.ai(Course - 1 Week - 3)|| Neural Networks and Deep(Week 3)<br><b>Note - These are my notes on the second course of DeepLearning.ai by AndrewNg</b><br>All Credits to the teacher <b>andrew ng </b> :)</p><a id="more"></a><h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1> <p><div class="lev1 toc-item"><a href="#Planar-data-classification-with-one-hidden-layer" data-toc-modified-id="Planar-data-classification-with-one-hidden-layer-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Planar data classification with one hidden layer</a></div><div class="lev2 toc-item"><a href="#1---Packages" data-toc-modified-id="1---Packages-11"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>1 - Packages</a></div><div class="lev2 toc-item"><a href="#2---Dataset" data-toc-modified-id="2---Dataset-12"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>2 - Dataset</a></div><div class="lev2 toc-item"><a href="#3---Simple-Logistic-Regression" data-toc-modified-id="3---Simple-Logistic-Regression-13"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>3 - Simple Logistic Regression</a></div><div class="lev2 toc-item"><a href="#4---Neural-Network-model" data-toc-modified-id="4---Neural-Network-model-14"><span class="toc-item-num">1.4&nbsp;&nbsp;</span>4 - Neural Network model</a></div><div class="lev3 toc-item"><a href="#4.1---Defining-the-neural-network-structure" data-toc-modified-id="4.1---Defining-the-neural-network-structure-141"><span class="toc-item-num">1.4.1&nbsp;&nbsp;</span>4.1 - Defining the neural network structure</a></div><div class="lev3 toc-item"><a href="#4.2---Initialize-the-model's-parameters" data-toc-modified-id="4.2---Initialize-the-model's-parameters-142"><span class="toc-item-num">1.4.2&nbsp;&nbsp;</span>4.2 - Initialize the model's parameters</a></div><div class="lev3 toc-item"><a href="#4.3---The-Loop" data-toc-modified-id="4.3---The-Loop-143"><span class="toc-item-num">1.4.3&nbsp;&nbsp;</span>4.3 - The Loop</a></div><div class="lev3 toc-item"><a href="#4.4---Integrate-parts-4.1,-4.2-and-4.3-in-nn_model()" data-toc-modified-id="4.4---Integrate-parts-4.1,-4.2-and-4.3-in-nn_model()-144"><span class="toc-item-num">1.4.4&nbsp;&nbsp;</span>4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()</a></div><div class="lev3 toc-item"><a href="#4.5-Predictions" data-toc-modified-id="4.5-Predictions-145"><span class="toc-item-num">1.4.5&nbsp;&nbsp;</span>4.5 Predictions</a></div><div class="lev3 toc-item"><a href="#4.6---Tuning-hidden-layer-size-(optional/ungraded-exercise)" data-toc-modified-id="4.6---Tuning-hidden-layer-size-(optional/ungraded-exercise)-146"><span class="toc-item-num">1.4.6&nbsp;&nbsp;</span>4.6 - Tuning hidden layer size (optional/ungraded exercise)</a></div><div class="lev2 toc-item"><a href="#5)-Performance-on-other-datasets" data-toc-modified-id="5)-Performance-on-other-datasets-15"><span class="toc-item-num">1.5&nbsp;&nbsp;</span>5) Performance on other datasets</a></div><h1 id="Planar-data-classification-with-one-hidden-layer"><a href="#Planar-data-classification-with-one-hidden-layer" class="headerlink" title="Planar data classification with one hidden layer"></a>Planar data classification with one hidden layer</h1><p>Welcome to your week 3 programming assignment. It’s time to build your first neural network, which will have a hidden layer. You will see a big difference between this model and the one you implemented using logistic regression. </p><p><strong>You will learn how to:</strong></p><ul><li>Implement a 2-class classification neural network with a single hidden layer</li><li>Use units with a non-linear activation function, such as tanh </li><li>Compute the cross entropy loss </li><li>Implement forward and backward propagation</li></ul><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>Let’s first import all the packages that you will need during this assignment.</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly93d3cubnVtcHkub3Jn">numpy<i class="fa fa-external-link-alt"></i></span> is the fundamental package for scientific computing with Python.</li><li><span class="exturl" data-url="aHR0cHM6Ly9zY2lraXQtbGVhcm4ub3JnL3N0YWJsZS8=">sklearn<i class="fa fa-external-link-alt"></i></span> provides simple and efficient tools for data mining and data analysis. </li><li><span class="exturl" data-url="aHR0cHM6Ly9tYXRwbG90bGliLm9yZw==">matplotlib<i class="fa fa-external-link-alt"></i></span> is a library for plotting graphs in Python.</li><li>testCases provides some test examples to assess the correctness of your functions</li><li>planar_utils provide various useful functions used in this assignment</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Package imports</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>) <span class="comment"># set a seed so that the results are consistent</span></span><br></pre></td></tr></table></figure><h2 id="2-Dataset"><a href="#2-Dataset" class="headerlink" title="2 - Dataset"></a>2 - Dataset</h2><p>First, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables <code>X</code> and <code>Y</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, Y = load_planar_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape, Y.shape</span><br></pre></td></tr></table></figure><pre><code>((2, 400), (1, 400))</code></pre><p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Visualize the data:</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y[<span class="number">0</span>, :], s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br></pre></td></tr></table></figure><p>You have:<br>    - a numpy-array (matrix) X that contains your features (x1, x2)<br>    - a numpy-array (vector) Y that contains your labels (red:0, blue:1).</p><p>Lets first get a better sense of what our data is like. </p><p><strong>Exercise</strong>: How many training examples do you have? In addition, what is the <code>shape</code> of the variables <code>X</code> and <code>Y</code>? </p><p><strong>Hint</strong>: How do you get the shape of a numpy array? <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2MvbnVtcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9udW1weS5uZGFycmF5LnNoYXBlLmh0bWw=">(help)<i class="fa fa-external-link-alt"></i></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">shape_X = X.shape</span><br><span class="line">shape_Y = Y.shape</span><br><span class="line">m =  Y.size <span class="comment"># training set size</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of X is: '</span> + str(shape_X))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'The shape of Y is: '</span> + str(shape_Y))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'I have m = %d training examples!'</span> % (m))</span><br></pre></td></tr></table></figure><pre><code>The shape of X is: (2, 400)The shape of Y is: (1, 400)I have m = 400 training examples!</code></pre><p><strong>Expected Output</strong>:</p><table style="width:20%">  <tr>    <td>**shape of X**</td>    <td> (2, 400) </td>   </tr>  <tr>    <td>**shape of Y**</td>    <td>(1, 400) </td>   </tr><pre><code>&lt;tr&gt;&lt;td&gt;**m**&lt;/td&gt;&lt;td&gt; 400 &lt;/td&gt; </code></pre>  </tr></table><h2 id="3-Simple-Logistic-Regression"><a href="#3-Simple-Logistic-Regression" class="headerlink" title="3 - Simple Logistic Regression"></a>3 - Simple Logistic Regression</h2><p>Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train the logistic regression classifier</span></span><br><span class="line">clf = sklearn.linear_model.LogisticRegressionCV(cv=<span class="number">5</span>);</span><br><span class="line">clf.fit(X.T, Y.T.ravel());</span><br></pre></td></tr></table></figure><p>You can now plot the decision boundary of these models. Run the code below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary for logistic regression</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: clf.predict(x), X, Y)</span><br><span class="line">plt.title(<span class="string">"Logistic Regression"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">LR_predictions = clf.predict(X.T)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy of logistic regression: %d '</span> % float((np.dot(Y,LR_predictions) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-LR_predictions))/float(Y.size)*<span class="number">100</span>) +</span><br><span class="line">       <span class="string">'% '</span> + <span class="string">"(percentage of correctly labelled datapoints)"</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)</code></pre><p><strong>Expected Output</strong>:</p><table style="width:20%">  <tr>    <td>**Accuracy**</td>    <td> 47% </td>   </tr></table><p><strong>Interpretation</strong>: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now! </p><h2 id="4-Neural-Network-model"><a href="#4-Neural-Network-model" class="headerlink" title="4 - Neural Network model"></a>4 - Neural Network model</h2><p>Logistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer.</p><p><strong>Here is our model</strong>:<br><strong>Mathematically</strong>:</p><p>For one example $x^{(i)}$:<br>$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1}$$<br>$$a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}$$<br>$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}$$<br>$$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}$$<br>$$y^{(i)}_{prediction} = \begin{cases} 1 &amp; \mbox{if } a^{<a href="i">2</a>} &gt; 0.5 \ 0 &amp; \mbox{otherwise } \end{cases}\tag{5}$$</p><p>Given the predictions on all the examples, you can also compute the cost $J$ as follows:<br>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}$$</p><p><strong>Reminder</strong>: The general methodology to build a Neural Network is to:<br>    1. Define the neural network structure ( # of input units,  # of hidden units, etc).<br>    2. Initialize the model’s parameters<br>    3. Loop:<br>        - Implement forward propagation<br>        - Compute loss<br>        - Implement backward propagation to get the gradients<br>        - Update parameters (gradient descent)</p><p>You often build helper functions to compute steps 1-3 and then merge them into one function we call <code>nn_model()</code>. Once you’ve built <code>nn_model()</code> and learnt the right parameters, you can make predictions on new data.</p><h3 id="4-1-Defining-the-neural-network-structure"><a href="#4-1-Defining-the-neural-network-structure" class="headerlink" title="4.1 - Defining the neural network structure"></a>4.1 - Defining the neural network structure</h3><p><strong>Exercise</strong>: Define three variables:<br>    - n_x: the size of the input layer<br>    - n_h: the size of the hidden layer (set this to 4)<br>    - n_y: the size of the output layer</p><p><strong>Hint</strong>: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: layer_sizes</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = layer_sizes_test_case()</span><br><span class="line">(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)</span><br><span class="line">print(<span class="string">"The size of the input layer is: n_x = "</span> + str(n_x))</span><br><span class="line">print(<span class="string">"The size of the hidden layer is: n_h = "</span> + str(n_h))</span><br><span class="line">print(<span class="string">"The size of the output layer is: n_y = "</span> + str(n_y))</span><br></pre></td></tr></table></figure><pre><code>The size of the input layer is: n_x = 5The size of the hidden layer is: n_h = 4The size of the output layer is: n_y = 2</code></pre><p><strong>Expected Output</strong> (these are not the sizes you will use for your network, they are just used to assess the function you’ve just coded).</p><table style="width:20%">  <tr>    <td>**n_x**</td>    <td> 5 </td>   </tr><pre><code>&lt;tr&gt;&lt;td&gt;**n_h**&lt;/td&gt;&lt;td&gt; 4 &lt;/td&gt; </code></pre>  </tr><pre><code>&lt;tr&gt;&lt;td&gt;**n_y**&lt;/td&gt;&lt;td&gt; 2 &lt;/td&gt; </code></pre>  </tr></table><h3 id="4-2-Initialize-the-model’s-parameters"><a href="#4-2-Initialize-the-model’s-parameters" class="headerlink" title="4.2 - Initialize the model’s parameters"></a>4.2 - Initialize the model’s parameters</h3><p><strong>Exercise</strong>: Implement the function <code>initialize_parameters()</code>.</p><p><strong>Instructions</strong>:</p><ul><li>Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed.</li><li>You will initialize the weights matrices with random values. <ul><li>Use: <code>np.random.randn(a,b) * 0.01</code> to randomly initialize a matrix of shape (a,b).</li></ul></li><li>You will initialize the bias vectors as zeros. <ul><li>Use: <code>np.zeros((a,b))</code> to initialize a matrix of shape (a,b) with zeros.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">2</span>) <span class="comment"># we set up a seed so that your output matches ours although the initialization is random.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h, n_x) * <span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h) * <span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_x, n_h, n_y = initialize_parameters_test_case()</span><br><span class="line"></span><br><span class="line">parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[-0.00416758 -0.00056267] [-0.02136196  0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]]b1 = [[0.] [0.] [0.] [0.]]W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]b2 = [[0.]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:90%">  <tr>    <td>**W1**</td>    <td> [[-0.00416758 -0.00056267] [-0.02136196  0.01640271] [-0.01793436 -0.00841747] [ 0.00502881 -0.01245288]] </td>   </tr>  <tr>    <td>**b1**</td>    <td> [[ 0.] [ 0.] [ 0.] [ 0.]] </td>   </tr>  <tr>    <td>**W2**</td>    <td> [[-0.01057952 -0.00909008  0.00551454  0.02292208]]</td>   </tr>  <tr>    <td>**b2**</td>    <td> [[ 0.]] </td>   </tr></table><h3 id="4-3-The-Loop"><a href="#4-3-The-Loop" class="headerlink" title="4.3 - The Loop"></a>4.3 - The Loop</h3><p><strong>Question</strong>: Implement <code>forward_propagation()</code>.</p><p><strong>Instructions</strong>:</p><ul><li>Look above at the mathematical representation of your classifier.</li><li>You can use the function <code>sigmoid()</code>. It is built-in (imported) in the notebook.</li><li>You can use the function <code>np.tanh()</code>. It is part of the numpy library.</li><li>The steps you have to implement are:<ol><li>Retrieve each parameter from the dictionary “parameters” (which is the output of <code>initialize_parameters()</code>) by using <code>parameters[&quot;..&quot;]</code>.</li><li>Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).</li></ol></li><li>Values needed in the backpropagation are stored in “<code>cache</code>“. The <code>cache</code> will be given as an input to the backpropagation function.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">#print(W1.shape, b1.shape, W2.shape, b2.shape)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_assess, parameters = forward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">A2, cache = forward_propagation(X_assess, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: we use the mean here just to make sure that your output matches ours. </span></span><br><span class="line">print(np.mean(cache[<span class="string">'Z1'</span>]) ,np.mean(cache[<span class="string">'A1'</span>]),np.mean(cache[<span class="string">'Z2'</span>]),np.mean(cache[<span class="string">'A2'</span>]))</span><br></pre></td></tr></table></figure><pre><code>-0.0004997557777419913 -0.0004969633532317802 0.0004381874509591466 0.500109546852431</code></pre><p><strong>Expected Output</strong>:</p><table style="width:55%">  <tr>    <td> -0.000499755777742 -0.000496963353232 0.000438187450959 0.500109546852 </td>   </tr></table><p>Now that you have computed $A^{[2]}$ (in the Python variable “<code>A2</code>“), which contains $a^{<a href="i">2</a>}$ for every example, you can compute the cost function as follows:</p><p>$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}$$</p><p><strong>Exercise</strong>: Implement <code>compute_cost()</code> to compute the value of the cost $J$.</p><p><strong>Instructions</strong>:</p><ul><li>There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented<br>$- \sum\limits_{i=0}^{m}  y^{(i)}\log(a^{<a href="i">2</a>})$:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logprobs = np.multiply(np.log(A2),Y)</span><br><span class="line">cost = - np.sum(logprobs)                <span class="comment"># no need to use a for loop!</span></span><br></pre></td></tr></table></figure></li></ul><p>(you can use either <code>np.multiply()</code> and then <code>np.sum()</code> or directly <code>np.dot()</code>).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve W1 and W2 from parameters</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2), Y) + np.multiply((<span class="number">1</span>-Y), np.log(<span class="number">1</span>-A2))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(logprobs) </span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A2, Y_assess, parameters = compute_cost_test_case()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"cost = "</span> + str(compute_cost(A2, Y_assess, parameters)))</span><br></pre></td></tr></table></figure><pre><code>cost = 0.6929198937761265</code></pre><p><strong>Expected Output</strong>:</p><table style="width:20%">  <tr>    <td>**cost**</td>    <td> 0.692919893776 </td>   </tr></table><p>Using the cache computed during forward propagation, you can now implement backward propagation.</p><p><strong>Question</strong>: Implement the function <code>backward_propagation()</code>.</p><p><strong>Instructions</strong>:<br>Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.  </p><!--$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$- Note that $*$ denotes elementwise multiplication.- The notation you will use is common in deep learning coding:    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$!--><ul><li>Tips:<ul><li>To compute dZ1 you’ll need to compute $g^{[1]’}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]’}(z) = 1-a^2$. So you can compute<br>$g^{[1]’}(Z^{[1]})$ using <code>(1 - np.power(A1, 2))</code>.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: backward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">    dZ2= A2 - Y</span><br><span class="line">    dW2 = (<span class="number">1</span>/m) * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = (<span class="number">1</span>/m) * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T, dZ2), (<span class="number">1</span> - np.power(A1, <span class="number">2</span>)))</span><br><span class="line">    dW1 = (<span class="number">1</span>/m) * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = (<span class="number">1</span>/m) * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, cache, X_assess, Y_assess = backward_propagation_test_case()</span><br><span class="line"></span><br><span class="line">grads = backward_propagation(parameters, cache, X_assess, Y_assess)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW1 = "</span>+ str(grads[<span class="string">"dW1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db1 = "</span>+ str(grads[<span class="string">"db1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dW2 = "</span>+ str(grads[<span class="string">"dW2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db2 = "</span>+ str(grads[<span class="string">"db2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>dW1 = [[ 0.01018708 -0.00708701] [ 0.00873447 -0.0060768 ] [-0.00530847  0.00369379] [-0.02206365  0.01535126]]db1 = [[-0.00069728] [-0.00060606] [ 0.000364  ] [ 0.00151207]]dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]db2 = [[0.06589489]]</code></pre><p><strong>Expected output</strong>:</p><table style="width:80%">  <tr>    <td>**dW1**</td>    <td> [[ 0.01018708 -0.00708701] [ 0.00873447 -0.0060768 ] [-0.00530847  0.00369379] [-0.02206365  0.01535126]] </td>   </tr>  <tr>    <td>**db1**</td>    <td>  [[-0.00069728] [-0.00060606] [ 0.000364  ] [ 0.00151207]] </td>   </tr>  <tr>    <td>**dW2**</td>    <td> [[ 0.00363613  0.03153604  0.01162914 -0.01318316]] </td>   </tr>  <tr>    <td>**db2**</td>    <td> [[ 0.06589489]] </td>   </tr></table>  <p><strong>Question</strong>: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p><p><strong>General gradient descent rule</strong>: $ \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$ where $\alpha$ is the learning rate and $\theta$ represents a parameter.</p><p><strong>Illustration</strong>: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: update_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = W1 - learning_rate * dW1</span><br><span class="line">    b1 = b1 - learning_rate * db1</span><br><span class="line">    W2 = W2 - learning_rate * dW2</span><br><span class="line">    b2 = b2 - learning_rate * db2</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[-0.00643025  0.01936718] [-0.02410458  0.03978052] [-0.01653973 -0.02096177] [ 0.01046864 -0.05990141]]b1 = [[-1.02420756e-06] [ 1.27373948e-05] [ 8.32996807e-07] [-3.20136836e-06]]W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]b2 = [[0.00010457]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:80%">  <tr>    <td>**W1**</td>    <td> [[-0.00643025  0.01936718] [-0.02410458  0.03978052] [-0.01653973 -0.02096177] [ 0.01046864 -0.05990141]]</td>   </tr>  <tr>    <td>**b1**</td>    <td> [[ -1.02420756e-06] [  1.27373948e-05] [  8.32996807e-07] [ -3.20136836e-06]]</td>   </tr>  <tr>    <td>**W2**</td>    <td> [[-0.01041081 -0.04463285  0.01758031  0.04747113]] </td>   </tr>  <tr>    <td>**b2**</td>    <td> [[ 0.00010457]] </td>   </tr></table>  <h3 id="4-4-Integrate-parts-4-1-4-2-and-4-3-in-nn-model"><a href="#4-4-Integrate-parts-4-1-4-2-and-4-3-in-nn-model" class="headerlink" title="4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()"></a>4.4 - Integrate parts 4.1, 4.2 and 4.3 in nn_model()</h3><p><strong>Question</strong>: Build your neural network model in <code>nn_model()</code>.</p><p><strong>Instructions</strong>: The neural network model has to use the previous functions in the right order.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: nn_model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">         </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_assess, Y_assess = nn_model_test_case()</span><br><span class="line"></span><br><span class="line">parameters = nn_model(X_assess, Y_assess, <span class="number">4</span>, num_iterations=<span class="number">10000</span>, print_cost=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"W1 = "</span> + str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line">print(<span class="string">"b1 = "</span> + str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line">print(<span class="string">"W2 = "</span> + str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line">print(<span class="string">"b2 = "</span> + str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>/Users/abanihi/opt/miniconda3/envs/pangeo/lib/python3.6/site-packages/ipykernel/__main__.py:26: RuntimeWarning: divide by zero encountered in log/Users/abanihi/devel/personal/deep-learning-specialization-coursera/01-Neural-Networks-and-Deep-Learning/week3/Programming-Assignments/planar_utils.py:34: RuntimeWarning: overflow encountered in exp  s = 1/(1+np.exp(-x))W1 = [[-4.18491249  5.33220652] [-7.52991891  1.24304481] [-4.19257906  5.32654053] [ 7.52989842 -1.24305585]]b1 = [[ 2.32930709] [ 3.79453736] [ 2.33008777] [-3.79456804]]W2 = [[-6033.83651818 -6008.12961806 -6033.10073783  6008.06596049]]b2 = [[-52.66626424]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:90%">  <tr>    <td>**W1**</td>    <td> [[-4.18494056  5.33220609] [-7.52989382  1.24306181] [-4.1929459   5.32632331] [ 7.52983719 -1.24309422]]</td>   </tr>  <tr>    <td>**b1**</td>    <td> [[ 2.32926819] [ 3.79458998] [ 2.33002577] [-3.79468846]]</td>   </tr>  <tr>    <td>**W2**</td>    <td> [[-6033.83672146 -6008.12980822 -6033.10095287  6008.06637269]] </td>   </tr>  <tr>    <td>**b2**</td>    <td> [[-52.66607724]] </td>   </tr></table>  <h3 id="4-5-Predictions"><a href="#4-5-Predictions" class="headerlink" title="4.5 Predictions"></a>4.5 Predictions</h3><p><strong>Question</strong>: Use your model to predict by building predict().<br>Use forward propagation to predict results.</p><p><strong>Reminder</strong>: predictions = $y_{prediction} = \mathbb 1 \textfalse = \begin{cases}<br>      1 &amp; \text{if}\ activation &gt; 0.5 \<br>      0 &amp; \text{otherwise}<br>    \end{cases}$  </p><p>As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: <code>X_new = (X &gt; threshold)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)  <span class="comment"># Vectorized</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">parameters, X_assess = predict_test_case()</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X_assess)</span><br><span class="line">print(<span class="string">"predictions mean = "</span> + str(np.mean(predictions)))</span><br></pre></td></tr></table></figure><pre><code>predictions mean = 0.6666666666666666</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%">  <tr>    <td>**predictions mean**</td>    <td> 0.666666666667 </td>   </tr></table><p>It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build a model with a n_h-dimensional hidden layer</span></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693048Cost after iteration 1000: 0.288083Cost after iteration 2000: 0.254385Cost after iteration 3000: 0.233864Cost after iteration 4000: 0.226792Cost after iteration 5000: 0.222644Cost after iteration 6000: 0.219731Cost after iteration 7000: 0.217504Cost after iteration 8000: 0.219456Cost after iteration 9000: 0.218558Text(0.5, 1.0, &apos;Decision Boundary for hidden layer size 4&apos;)</code></pre><p><strong>Expected Output</strong>:</p><table style="width:40%">  <tr>    <td>**Cost after iteration 9000**</td>    <td> 0.218607 </td>   </tr></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print accuracy</span></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><pre><code>Accuracy: 90%</code></pre><p><strong>Expected Output</strong>: </p><table style="width:15%">  <tr>    <td>**Accuracy**</td>    <td> 90% </td>   </tr></table><p>Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. </p><p>Now, let’s try out several hidden layer sizes.</p><h3 id="4-6-Tuning-hidden-layer-size-optional-ungraded-exercise"><a href="#4-6-Tuning-hidden-layer-size-optional-ungraded-exercise" class="headerlink" title="4.6 - Tuning hidden layer size (optional/ungraded exercise)"></a>4.6 - Tuning hidden layer size (optional/ungraded exercise)</h3><p>Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This may take about 2 minutes to run</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><pre><code>Accuracy for 1 hidden units: 67.5 %Accuracy for 2 hidden units: 67.25 %Accuracy for 3 hidden units: 90.75 %Accuracy for 4 hidden units: 90.5 %Accuracy for 5 hidden units: 91.25 %Accuracy for 20 hidden units: 90.5 %Accuracy for 50 hidden units: 90.75 %</code></pre><p><strong>Interpretation</strong>:</p><ul><li>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. </li><li>The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to  fits the data well without also incurring noticable overfitting.</li><li>You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. </li></ul><p><strong>Optional questions</strong>:</p><p><strong>Note</strong>: Remember to submit the assignment but clicking the blue “Submit Assignment” button at the upper-right. </p><p>Some optional/ungraded questions that you can explore if you wish: </p><ul><li>What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?</li><li>Play with the learning_rate. What happens?</li><li>What if we change the dataset? (See part 5 below!)</li></ul><font color='blue'>**You've learnt to:**- Build a complete neural network with a hidden layer- Make a good use of a non-linear unit- Implemented forward propagation and backpropagation, and trained a neural network- See the impact of varying the hidden layer size, including overfitting.</font>Nice work! <h2 id="5-Performance-on-other-datasets"><a href="#5-Performance-on-other-datasets" class="headerlink" title="5) Performance on other datasets"></a>5) Performance on other datasets</h2><p>If you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Datasets</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line"></span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### (choose your dataset)</span></span><br><span class="line">dataset = <span class="string">"noisy_moons"</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=Y.ravel(), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br></pre></td></tr></table></figure><p>Congrats on finishing this Programming Assignment!</p><p>Reference:</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9zY3MucnllcnNvbi5jYS9+YWhhcmxleS9uZXVyYWwtbmV0d29ya3Mv">https://scs.ryerson.ca/~aharley/neural-networks/<i class="fa fa-external-link-alt"></i></span></li><li><span class="exturl" data-url="aHR0cHM6Ly9jczIzMW4uZ2l0aHViLmlvL25ldXJhbC1uZXR3b3Jrcy1jYXNlLXN0dWR5Lw==">https://cs231n.github.io/neural-networks-case-study/<i class="fa fa-external-link-alt"></i></span></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext version_information</span><br><span class="line">%version_information numpy, matplotlib, sklearn</span><br></pre></td></tr></table></figure><table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.6 64bit [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]</td></tr><tr><td>IPython</td><td>7.0.1</td></tr><tr><td>OS</td><td>Darwin 17.7.0 x86_64 i386 64bit</td></tr><tr><td>numpy</td><td>1.15.1</td></tr><tr><td>matplotlib</td><td>3.0.0</td></tr><tr><td>sklearn</td><td>0.20.0</td></tr><tr><td colspan='2'>Sun Oct 14 20:51:09 2018 MDT</td></tr></table>]]></content>
    
    <summary type="html">
    
      Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn&#39;s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset.The dataset is not linearly separable, so logistic regression doesn&#39;t perform well. Hopefully a neural network will do better. Let&#39;s try this now!
    
    </summary>
    
    
      <category term="new post" scheme="https://massivefile.com/categories/new-post/"/>
    
      <category term="2020" scheme="https://massivefile.com/categories/new-post/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/categories/new-post/2020/Assignment/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/tags/Assignment/"/>
    
      <category term="Planar data classification with one hidden layer" scheme="https://massivefile.com/tags/Planar-data-classification-with-one-hidden-layer/"/>
    
  </entry>
  
  <entry>
    <title>Quiz 3 || Neural Networks and Deep Learning (Week 3)</title>
    <link href="https://massivefile.com/quiz3/"/>
    <id>https://massivefile.com/quiz3/</id>
    <published>2020-04-11T18:30:00.000Z</published>
    <updated>2020-04-14T19:28:39.605Z</updated>
    
    <content type="html"><![CDATA[<p>title: Quiz 3|| Deeplearnig.ai (Course - 1 Week - 3) Neural Networks and Deep Learning (Week 3)<br><b>Note - These are my notes on the first course of DeepLearning.ai by AndrewNg</b></p><a id="more"></a><iframe src="https://drive.google.com/file/d/1w3Kl4Ow8xStBNb3L3YbN52eOTVZmxM2S/preview" width="680" height="480"></iframe><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;title: Quiz 3|| Deeplearnig.ai (Course - 1 Week - 3) Neural Networks and Deep Learning (Week 3)&lt;br&gt;&lt;b&gt;Note - These are my notes on the first course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="2020" scheme="https://massivefile.com/categories/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/categories/2020/Quiz/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/tags/Quiz/"/>
    
  </entry>
  
  <entry>
    <title>Logistic Regression with a Neural Network mindset || Neural Networks and Deep Learning (Week 2)</title>
    <link href="https://massivefile.com/Logistic_Regression_with_a_Neural_Network_mindset/"/>
    <id>https://massivefile.com/Logistic_Regression_with_a_Neural_Network_mindset/</id>
    <published>2020-04-10T18:30:00.000Z</published>
    <updated>2020-04-13T14:46:39.787Z</updated>
    
    <content type="html"><![CDATA[<p>Logistic Regression with a Neural Network mindset Assignment || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)<br><b>Note - These are my notes on the second course of DeepLearning.ai by AndrewNg</b><br>All Credits to the teacher <b>andrew ng </b> :)</p><a id="more"></a><h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1> <p><div class="lev1 toc-item"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" data-toc-modified-id="Logistic-Regression-with-a-Neural-Network-mindset-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Logistic Regression with a Neural Network mindset</a></div><div class="lev2 toc-item"><a href="#1---Packages" data-toc-modified-id="1---Packages-11"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>1 - Packages</a></div><div class="lev1 toc-item"><a href="#load-lr_utils.py" data-toc-modified-id="load-lr_utils.py-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>load lr_utils.py</a></div><div class="lev2 toc-item"><a href="#2---Overview-of-the-Problem-set" data-toc-modified-id="2---Overview-of-the-Problem-set-21"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>2 - Overview of the Problem set</a></div><div class="lev2 toc-item"><a href="#3---General-Architecture-of-the-learning-algorithm" data-toc-modified-id="3---General-Architecture-of-the-learning-algorithm-22"><span class="toc-item-num">2.2&nbsp;&nbsp;</span>3 - General Architecture of the learning algorithm</a></div><div class="lev2 toc-item"><a href="#4---Building-the-parts-of-our-algorithm" data-toc-modified-id="4---Building-the-parts-of-our-algorithm-23"><span class="toc-item-num">2.3&nbsp;&nbsp;</span>4 - Building the parts of our algorithm</a></div><div class="lev3 toc-item"><a href="#4.1---Helper-functions" data-toc-modified-id="4.1---Helper-functions-231"><span class="toc-item-num">2.3.1&nbsp;&nbsp;</span>4.1 - Helper functions</a></div><div class="lev3 toc-item"><a href="#4.2---Initializing-parameters" data-toc-modified-id="4.2---Initializing-parameters-232"><span class="toc-item-num">2.3.2&nbsp;&nbsp;</span>4.2 - Initializing parameters</a></div><div class="lev3 toc-item"><a href="#4.3---Forward-and-Backward-propagation" data-toc-modified-id="4.3---Forward-and-Backward-propagation-233"><span class="toc-item-num">2.3.3&nbsp;&nbsp;</span>4.3 - Forward and Backward propagation</a></div><div class="lev3 toc-item"><a href="#d)-Optimization" data-toc-modified-id="d)-Optimization-234"><span class="toc-item-num">2.3.4&nbsp;&nbsp;</span>d) Optimization</a></div><div class="lev2 toc-item"><a href="#5---Merge-all-functions-into-a-model" data-toc-modified-id="5---Merge-all-functions-into-a-model-24"><span class="toc-item-num">2.4&nbsp;&nbsp;</span>5 - Merge all functions into a model</a></div><div class="lev2 toc-item"><a href="#6---Further-analysis-(optional/ungraded-exercise)" data-toc-modified-id="6---Further-analysis-(optional/ungraded-exercise)-25"><span class="toc-item-num">2.5&nbsp;&nbsp;</span>6 - Further analysis (optional/ungraded exercise)</a></div><div class="lev4 toc-item"><a href="#Choice-of-learning-rate" data-toc-modified-id="Choice-of-learning-rate-2501"><span class="toc-item-num">2.5.0.1&nbsp;&nbsp;</span>Choice of learning rate</a></div><div class="lev2 toc-item"><a href="#7---Test-with-your-own-image-(optional/ungraded-exercise)" data-toc-modified-id="7---Test-with-your-own-image-(optional/ungraded-exercise)-26"><span class="toc-item-num">2.6&nbsp;&nbsp;</span>7 - Test with your own image (optional/ungraded exercise)</a></div><h1 id="Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Logistic Regression with a Neural Network mindset"></a>Logistic Regression with a Neural Network mindset</h1><p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and so will also hone your intuitions about deep learning.</p><p><strong>Instructions:</strong></p><ul><li>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</li></ul><p><strong>You will learn to:</strong></p><ul><li>Build the general architecture of a learning algorithm, including:<ul><li>Initializing parameters</li><li>Calculating the cost function and its gradient</li><li>Using an optimization algorithm (gradient descent) </li></ul></li><li>Gather all three functions above into a main model function, in the right order.</li></ul><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>First, let’s run the cell below to import all the packages that you will need during this assignment. </p><ul><li><span class="exturl" data-url="aHR0cHM6Ly93d3cubnVtcHkub3Jn">numpy<i class="fa fa-external-link-alt"></i></span> is the fundamental package for scientific computing with Python.</li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cuaDVweS5vcmc=">h5py<i class="fa fa-external-link-alt"></i></span> is a common package to interact with a dataset that is stored on an H5 file.</li><li><span class="exturl" data-url="aHR0cHM6Ly9tYXRwbG90bGliLm9yZw==">matplotlib<i class="fa fa-external-link-alt"></i></span> is a famous library to plot graphs in Python.</li><li><span class="exturl" data-url="aHR0cHM6Ly93d3cucHl0aG9ud2FyZS5jb20vcHJvZHVjdHMvcGlsLw==">PIL<i class="fa fa-external-link-alt"></i></span> and <span class="exturl" data-url="aHR0cHM6Ly93d3cuc2NpcHkub3JnLw==">scipy<i class="fa fa-external-link-alt"></i></span> are used here to test your model with your own picture at the end.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">import</span> skimage</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="load-lr-utils-py"><a href="#load-lr-utils-py" class="headerlink" title="load lr_utils.py"></a>load lr_utils.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    train_dataset = h5py.File(<span class="string">'datasets/train_catvnoncat.h5'</span>, <span class="string">"r"</span>)</span><br><span class="line">    train_set_x_orig = np.array(train_dataset[<span class="string">"train_set_x"</span>][:]) <span class="comment"># your train set features</span></span><br><span class="line">    train_set_y_orig = np.array(train_dataset[<span class="string">"train_set_y"</span>][:]) <span class="comment"># your train set labels</span></span><br><span class="line"></span><br><span class="line">    test_dataset = h5py.File(<span class="string">'datasets/test_catvnoncat.h5'</span>, <span class="string">"r"</span>)</span><br><span class="line">    test_set_x_orig = np.array(test_dataset[<span class="string">"test_set_x"</span>][:]) <span class="comment"># your test set features</span></span><br><span class="line">    test_set_y_orig = np.array(test_dataset[<span class="string">"test_set_y"</span>][:]) <span class="comment"># your test set labels</span></span><br><span class="line"></span><br><span class="line">    classes = np.array(test_dataset[<span class="string">"list_classes"</span>][:]) <span class="comment"># the list of classes</span></span><br><span class="line">    </span><br><span class="line">    train_set_y_orig = train_set_y_orig.reshape((<span class="number">1</span>, train_set_y_orig.shape[<span class="number">0</span>]))</span><br><span class="line">    test_set_y_orig = test_set_y_orig.reshape((<span class="number">1</span>, test_set_y_orig.shape[<span class="number">0</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes</span><br></pre></td></tr></table></figure><h2 id="2-Overview-of-the-Problem-set"><a href="#2-Overview-of-the-Problem-set" class="headerlink" title="2 - Overview of the Problem set"></a>2 - Overview of the Problem set</h2><p><strong>Problem Statement</strong>: You are given a dataset (“data.h5”) containing:<br>    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)<br>    - a test set of m_test images labeled as cat or non-cat<br>    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).</p><p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p><p>Let’s get more familiar with the dataset. Load the data by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (cat/non-cat)</span></span><br><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p><p>Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the <code>index</code> value and re-run to see other images. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">25</span></span><br><span class="line">example = train_set_x_orig[index]</span><br><span class="line">plt.imshow(train_set_x_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(train_set_y[:, index]) + <span class="string">", it's a '"</span> + classes[np.squeeze(train_set_y[:, index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"' picture."</span>)</span><br></pre></td></tr></table></figure><center><img src='https://lh3.googleusercontent.com/p7ly3mtv7h5Y3fl4JtiFmtb-i_iAerowOHSG4EXhu3SESUPMjmZhwIjaXxWp9GOqejyWazhp-maQ9NsyVNsruLAErIu1N7cZjBWTN5NnV43bukiGrcBmGm_k4WqWKoLdbAoYUyNgPw=w2400' /></center><p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. </p><p><strong>Exercise:</strong> Find the values for:<br>    - m_train (number of training examples)<br>    - m_test (number of test examples)<br>    - num_px (= height = width of a training image)<br>Remember that <code>train_set_x_orig</code> is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access <code>m_train</code> by writing <code>train_set_x_orig.shape[0]</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_set_x_orig.shape[<span class="number">2</span>]</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: m_train = "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: m_test = "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Height/Width of each image: num_px = "</span> + str(num_px))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x shape: "</span> + str(train_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x shape: "</span> + str(test_set_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br></pre></td></tr></table></figure><pre><code>Number of training examples: m_train = 209Number of testing examples: m_test = 50Height/Width of each image: num_px = 64Each image is of size: (64, 64, 3)train_set_x shape: (209, 64, 64, 3)train_set_y shape: (1, 209)test_set_x shape: (50, 64, 64, 3)test_set_y shape: (1, 50)</code></pre><p><strong>Expected Output for m_train, m_test and num_px</strong>: </p><table style="width:15%">  <tr>    <td>**m_train**</td>    <td> 209 </td>   </tr>  <tr>    <td>**m_test**</td>    <td> 50 </td>   </tr>  <tr>    <td>**num_px**</td>    <td> 64 </td>   </tr></table><p>For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $<em>$ num_px $</em>$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.</p><p><strong>Exercise:</strong> Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $<em>$ num_px $</em>$ 3, 1).</p><p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$<em>$c$</em>$d, a) is to use: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[<span class="number">0</span>], <span class="number">-1</span>).T      <span class="comment"># X.T is the transpose of X</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test examples</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_x_flatten shape: "</span> + str(train_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_set_y shape: "</span> + str(train_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_x_flatten shape: "</span> + str(test_set_x_flatten.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_set_y shape: "</span> + str(test_set_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sanity check after reshaping: "</span> + str(train_set_x_flatten[<span class="number">0</span>:<span class="number">5</span>,<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>train_set_x_flatten shape: (12288, 209)train_set_y shape: (1, 209)test_set_x_flatten shape: (12288, 50)test_set_y shape: (1, 50)sanity check after reshaping: [17 31 56 22 33]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:35%">  <tr>    <td>**train_set_x_flatten shape**</td>    <td> (12288, 209)</td>   </tr>  <tr>    <td>**train_set_y shape**</td>    <td>(1, 209)</td>   </tr>  <tr>    <td>**test_set_x_flatten shape**</td>    <td>(12288, 50)</td>   </tr>  <tr>    <td>**test_set_y shape**</td>    <td>(1, 50)</td>   </tr>  <tr>  <td>**sanity check after reshaping**</td>  <td>[17 31 56 22 33]</td>   </tr></table><p>To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.</p><p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p><!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> <p>Let’s standardize our dataset.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x = train_set_x_flatten/<span class="number">255.</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255.</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(train_set_x)</span><br></pre></td></tr></table></figure><pre><code>12288</code></pre><p><b><h4>What you need to remember:</h4></b><br><font color='blue'></p><p>Common steps for pre-processing a new dataset are:</p><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)</li><li>“Standardize” the data</font></li></ul><h2 id="3-General-Architecture-of-the-learning-algorithm"><a href="#3-General-Architecture-of-the-learning-algorithm" class="headerlink" title="3 - General Architecture of the learning algorithm"></a>3 - General Architecture of the learning algorithm</h2><p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p><p><strong>Mathematical expression of the algorithm</strong>:</p><p>For one example $x^{(i)}$:<br>$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$<br>$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$<br>$$ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} ) * $$<br>$$ \ *log(1-a^{(i)})\tag{3}$$</p><p>The cost is then computed by summing over all training examples:<br>$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$</p><p><strong>Key steps</strong>:<br>In this exercise, you will carry out the following steps:<br>    - Initialize the parameters of the model<br>    - Learn the parameters for the model by minimizing the cost<br>    - Use the learned parameters to make predictions (on the test set)<br>    - Analyse the results and conclude</p><h2 id="4-Building-the-parts-of-our-algorithm"><a href="#4-Building-the-parts-of-our-algorithm" class="headerlink" title="4 - Building the parts of our algorithm"></a>4 - Building the parts of our algorithm</h2><p>The main steps for building a Neural Network are:</p><ol><li>Define the model structure (such as number of input features) </li><li>Initialize the model’s parameters</li><li>Loop:<ul><li>Calculate current loss (forward propagation)</li><li>Calculate current gradient (backward propagation)</li><li>Update parameters (gradient descent)</li></ul></li></ol><p>You often build 1-3 separately and integrate them into one function we call <code>model()</code>.</p><h3 id="4-1-Helper-functions"><a href="#4-1-Helper-functions" class="headerlink" title="4.1 - Helper functions"></a>4.1 - Helper functions</h3><p><strong>Exercise</strong>: Using your code from “Python Basics”, implement <code>sigmoid()</code>. As you’ve seen in the figure above, you need to compute $sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use np.exp().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1.</span> / ( <span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid([0, 2]) = "</span> + str(sigmoid(np.array([<span class="number">0</span>,<span class="number">2</span>]))))</span><br></pre></td></tr></table></figure><pre><code>sigmoid([0, 2]) = [0.5        0.88079708]</code></pre><p><strong>Expected Output</strong>: </p><table>  <tr>    <td>**sigmoid([0, 2])**</td>    <td> [ 0.5         0.88079708]</td>   </tr></table><h3 id="4-2-Initializing-parameters"><a href="#4-2-Initializing-parameters" class="headerlink" title="4.2 - Initializing parameters"></a>4.2 - Initializing parameters</h3><p><strong>Exercise:</strong> Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_with_zeros</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    w = np.zeros(shape=(dim, <span class="number">1</span>), dtype=np.float32)</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dim = <span class="number">2</span></span><br><span class="line">w, b = initialize_with_zeros(dim)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(w))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(b))</span><br></pre></td></tr></table></figure><pre><code>w = [[0.] [0.]]b = 0</code></pre><p><strong>Expected Output</strong>: </p><table style="width:15%">    <tr>        <td>  ** w **  </td>        <td> [[ 0.] [ 0.]] </td>    </tr>    <tr>        <td>  ** b **  </td>        <td> 0 </td>    </tr></table><p>For image inputs, w will be of shape (num_px $\times$ num_px $\times$ 3, 1).</p><h3 id="4-3-Forward-and-Backward-propagation"><a href="#4-3-Forward-and-Backward-propagation" class="headerlink" title="4.3 - Forward and Backward propagation"></a>4.3 - Forward and Backward propagation</h3><p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p><p><strong>Exercise:</strong> Implement a function <code>propagate()</code> that computes the cost function and its gradient.</p><p><strong>Hints</strong>:</p><p>Forward Propagation:</p><ul><li>You get X</li><li>You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, …, a^{(m-1)}, a^{(m)})$</li><li>You calculate the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$</li></ul><p>Here are the two formulas you will be using: </p><p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$<br>$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: propagate</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)              <span class="comment"># compute activation</span></span><br><span class="line">    cost = (<span class="number">-1.</span> / m) * np.sum((Y*np.log(A) + (<span class="number">1</span> - Y)*np.log(<span class="number">1</span>-A)), axis=<span class="number">1</span>)     <span class="comment"># compute cost</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dw = (<span class="number">1.</span>/m)*np.dot(X,((A-Y).T))</span><br><span class="line">    db = (<span class="number">1.</span>/m)*np.sum(A-Y, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w, b, X, Y = np.array([[<span class="number">1</span>],[<span class="number">2</span>]]), <span class="number">2</span>, np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]), np.array([[<span class="number">1</span>,<span class="number">0</span>]])</span><br><span class="line">grads, cost = propagate(w, b, X, Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"cost = "</span> + str(cost))</span><br></pre></td></tr></table></figure><pre><code>dw = [[0.99993216] [1.99980262]]db = [0.49993523]cost = 6.000064773192205</code></pre><p><strong>Expected Output</strong>:</p><table style="width:50%">    <tr>        <td>  ** dw **  </td>        <td> [[ 0.99993216] [ 1.99980262]]</td>    </tr>    <tr>        <td>  ** db **  </td>        <td> 0.499935230625 </td>    </tr>    <tr>        <td>  ** cost **  </td>        <td> 6.000064773192205</td>    </tr></table><h3 id="d-Optimization"><a href="#d-Optimization" class="headerlink" title="d) Optimization"></a>d) Optimization</h3><ul><li>You have initialized your parameters.</li><li>You are also able to compute a cost function and its gradient.</li><li>Now, you want to update the parameters using gradient descent.</li></ul><p><strong>Exercise:</strong> Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $ \theta = \theta - \alpha \text{ } d\theta$, where $\alpha$ is the learning rate.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        grads, cost = propagate(w=w, b=b, X=X, Y=Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b -  learning_rate*db</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">params, grads, costs = optimize(w, b, X, Y, num_iterations= <span class="number">100</span>, learning_rate = <span class="number">0.009</span>, print_cost = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"w = "</span> + str(params[<span class="string">"w"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b = "</span> + str(params[<span class="string">"b"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dw = "</span> + str(grads[<span class="string">"dw"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"db = "</span> + str(grads[<span class="string">"db"</span>]))</span><br></pre></td></tr></table></figure><pre><code>w = [[0.1124579 ] [0.23106775]]b = [1.55930492]dw = [[0.90158428] [1.76250842]]db = [0.43046207]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%">    <tr>       <td> **w** </td>       <td>[[ 0.1124579 ] [ 0.23106775]] </td>    </tr><pre><code>&lt;tr&gt;   &lt;td&gt; **b** &lt;/td&gt;   &lt;td&gt; 1.55930492484 &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;   &lt;td&gt; **dw** &lt;/td&gt;   &lt;td&gt; [[ 0.90158428]</code></pre><p> [ 1.76250842]] </td><br>    </tr><br>    <tr><br>       <td> <strong>db</strong> </td><br>       <td> 0.430462071679 </td><br>    </tr></p></table><p><strong>Exercise:</strong> The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the <code>predict()</code> function. There is two steps to computing predictions:</p><ol><li><p>Calculate $\hat{Y} = A = \sigma(w^T X + b)$</p></li><li><p>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code>Y_prediction</code>. If you wish, you can use an <code>if</code>/<code>else</code> statement in a <code>for</code> loop (though there is also a way to vectorize this). </p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: predict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    [print(x) <span class="keyword">for</span> x <span class="keyword">in</span> A]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>, i] &gt;= <span class="number">0.5</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>, i] = <span class="number">0</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"predictions = "</span> + str(predict(w, b, X)))</span><br></pre></td></tr></table></figure><pre><code>[0.99987661 0.99999386]predictions = [[1. 1.]]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:30%">    <tr>         <td>             **predictions**         </td>          <td>            [[ 1.  1.]]         </td>     </tr></table><p><b><h4>What to remember </h4></b><br><font color='blue'><br>You’ve implemented several functions that:</p><ul><li>Initialize (w,b)</li><li>Optimize the loss iteratively to learn parameters (w,b):<ul><li>computing the cost and its gradient </li><li>updating the parameters using gradient descent</li></ul></li><li>Use the learned (w,b) to predict the labels for a given set of examples</font><br></li></ul><h2 id="5-Merge-all-functions-into-a-model"><a href="#5-Merge-all-functions-into-a-model" class="headerlink" title="5 - Merge all functions into a model"></a>5 - Merge all functions into a model</h2><p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p><p><strong>Exercise:</strong> Implement the model function. Use the following notation:<br>    - Y_prediction for your predictions on the test set<br>    - Y_prediction_train for your predictions on the train set<br>    - w, costs, grads for the outputs of optimize()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693147Cost after iteration 100: 0.584508Cost after iteration 200: 0.466949Cost after iteration 300: 0.376007Cost after iteration 400: 0.331463Cost after iteration 500: 0.303273Cost after iteration 600: 0.279880Cost after iteration 700: 0.260042Cost after iteration 800: 0.242941Cost after iteration 900: 0.228004Cost after iteration 1000: 0.214820Cost after iteration 1100: 0.203078Cost after iteration 1200: 0.192544Cost after iteration 1300: 0.183033Cost after iteration 1400: 0.174399Cost after iteration 1500: 0.166521Cost after iteration 1600: 0.159305Cost after iteration 1700: 0.152667Cost after iteration 1800: 0.146542Cost after iteration 1900: 0.140872[0.94366988 0.86095311 0.88896715 0.93630641 0.74075403 0.52849619 0.03094677 0.85707681 0.88457925 0.67279696 0.26601085 0.4823794 0.74741157 0.78575729 0.00978911 0.9203284  0.02453695 0.84884703 0.2050248  0.03703224 0.92931392 0.11930532 0.01411064 0.7832698 0.58188015 0.66897565 0.75119007 0.01323558 0.03402649 0.99735115 0.21031727 0.78123225 0.6815842  0.46647604 0.66323375 0.03424828 0.08031627 0.76570656 0.34760863 0.06177743 0.6987531  0.4106426 0.6648871  0.02776868 0.93053125 0.46395717 0.23971605 0.9771735 0.66202407 0.10482388][1.96533335e-01 8.97519936e-02 8.90887727e-01 2.05354859e-04 4.10043201e-02 1.13855541e-01 3.58425358e-02 9.20256043e-01 8.11815498e-02 5.09505652e-02 1.43687735e-01 7.77661312e-01 2.37002682e-01 9.26822611e-01 7.20256211e-01 4.54525029e-02 2.88164240e-02 4.96209946e-02 9.53642451e-02 9.27127783e-01 1.46871713e-02 4.42749993e-02 1.99658284e-01 5.10794145e-02 8.71854257e-01 8.54873232e-01 4.43988460e-02 8.41877286e-01 5.57178266e-02 7.39175253e-01 8.73390575e-02 7.61255429e-02 2.01282223e-01 2.02159519e-01 7.95065561e-02 3.69885691e-02 1.14655638e-02 5.90397260e-02 8.36880946e-01 3.33057415e-01 1.98548242e-02 4.46965063e-01 8.23737950e-01 4.13465923e-02 4.61512591e-02 1.21739845e-01 9.76716144e-02 8.07086225e-01 8.93389416e-03 3.73249849e-02 7.53711249e-01 2.47934596e-01 1.47013078e-01 3.93089594e-01 9.02530607e-01 3.94290174e-03 9.38300399e-01 8.14429890e-01 5.51201724e-02 9.56820776e-01 8.35826040e-01 7.75371183e-01 4.97406386e-02 5.05302748e-02 1.68276426e-01 7.39795683e-02 4.23114248e-02 1.80374321e-01 7.36839673e-01 2.36170561e-02 4.78407244e-02 9.72682719e-01 8.87430447e-02 1.40500115e-01 7.39006094e-02 5.87414480e-01 8.55122639e-04 3.51320419e-02 7.21341360e-02 1.59367000e-01 9.18793718e-02 2.76678199e-03 2.16954763e-02 8.75788002e-01 7.48905473e-01 2.61224310e-02 1.31264831e-01 5.58549892e-02 7.96470422e-01 4.31114360e-02 2.46081640e-01 9.28094796e-02 5.13207713e-01 9.23532733e-01 9.11010943e-01 1.56664277e-01 1.40529680e-01 8.72871654e-01 6.33390909e-02 2.04276699e-01 1.50378528e-01 5.42005811e-02 7.16869008e-01 8.93930822e-02 9.68748123e-01 1.16897229e-01 9.65813244e-01 7.63463753e-01 8.45184245e-01 7.94804824e-01 8.77046596e-01 8.92528474e-01 2.33698759e-02 1.08088606e-01 9.41045938e-02 5.06133571e-02 6.14255764e-02 8.74814031e-01 7.14021606e-03 1.49573407e-01 1.38752636e-02 5.75050572e-01 4.74218632e-02 2.67728414e-04 8.16437270e-01 5.25431990e-03 8.27320337e-01 1.63520986e-01 9.19597717e-01 9.11124533e-01 2.96731271e-01 1.37316359e-01 7.56632692e-02 9.51896490e-01 7.13340131e-01 5.62771203e-01 8.46803645e-01 8.81283783e-01 5.80214923e-03 3.24191787e-02 3.66569448e-02 4.24241240e-02 9.02746461e-01 6.95602248e-03 7.28528692e-01 8.04734016e-01 8.48847026e-01 1.97286016e-01 8.73972266e-01 8.56810568e-01 4.60108117e-01 9.98074787e-02 2.67726747e-02 9.16713593e-01 5.70477051e-02 2.34413956e-01 9.17441504e-01 1.43642340e-02 1.48384241e-02 4.18971050e-02 4.81257763e-03 6.74987512e-02 7.96958661e-01 7.94548221e-02 8.88055227e-01 1.63703299e-02 9.64896262e-01 4.74597209e-02 3.78354422e-02 6.75950812e-01 7.60983832e-01 8.91154251e-01 2.15482871e-01 1.80695199e-02 9.46591763e-01 7.71101522e-01 4.14565207e-02 8.02916154e-01 8.02541805e-02 6.89037478e-01 4.92103989e-02 4.87010785e-02 1.89987579e-02 3.71043577e-02 1.73595068e-03 7.71575747e-01 4.06433366e-02 6.60606392e-02 8.34508562e-01 2.27408842e-02 6.17839573e-02 4.56149270e-02 7.64947622e-01 6.19347921e-02 3.55887869e-03 1.03103435e-01 3.83745905e-01 8.77909931e-01 7.72818586e-02 1.79082665e-02 8.09911232e-01 2.02130387e-02 1.89353139e-02 1.83142729e-02 1.94041166e-01 2.01151983e-01 8.48224028e-02 1.61929290e-01 1.82858623e-01]train accuracy: 99.04306220095694 %test accuracy: 70.0 %</code></pre><p><strong>Expected Output</strong>: </p><table style="width:40%"> <pre><code>&lt;tr&gt;    &lt;td&gt; **Train Accuracy**  &lt;/td&gt;     &lt;td&gt; 99.04306220095694 % &lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;    &lt;td&gt;**Test Accuracy** &lt;/td&gt;     &lt;td&gt; 70.0 % &lt;/td&gt;&lt;/tr&gt;</code></pre></table> <p><strong>Comment</strong>: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p><p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the <code>index</code> variable) you can look at predictions on pictures of the test set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture that was wrongly classified.</span></span><br><span class="line">index = <span class="number">1</span></span><br><span class="line">plt.imshow(test_set_x[:,index].reshape((num_px, num_px, <span class="number">3</span>)))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(test_set_y[<span class="number">0</span>,index]) + <span class="string">", you predicted that it is a \""</span> + classes[int(d[<span class="string">"Y_prediction_test"</span>][<span class="number">0</span>,index])].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><pre><code>y = 1, you predicted that it is a &quot;cat&quot; picture.</code></pre><center><img src='https://lh3.googleusercontent.com/kJY-tRqpPcdQ7kqF3XJfo4RZ_-pauHBMdujmOlWP6janiuhNawemEADSBrPiqlgRi8vPHLqRVNrwqoxegsWCRDWwy8ntZMiT9BHxlPFfRfa9U4QGr7lNTWB4tNFRzryQuXPGReFn1g=w2400' /></center><p>Let’s also plot the cost function and the gradients.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot learning curve (with costs)</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><center><img src='https://lh3.googleusercontent.com/02H_4u-Q1y8hDW6L6nDzXAX6bE6QqP6WXfK_hrbXFYQ9Zxm4KVDJo9gEo3511OBsuvGzNo9GD6haI8auCtBkFlaEU2UDgCK6ReeVMyGqSg7brcTSQysUZr30K-Y7dTLdRzCVR67cMw=w2400' /></center><p><strong>Interpretation</strong>:<br>You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. </p><h2 id="6-Further-analysis-optional-ungraded-exercise"><a href="#6-Further-analysis-optional-ungraded-exercise" class="headerlink" title="6 - Further analysis (optional/ungraded exercise)"></a>6 - Further analysis (optional/ungraded exercise)</h2><p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate $\alpha$. </p><h4 id="Choice-of-learning-rate"><a href="#Choice-of-learning-rate" class="headerlink" title="Choice of learning rate"></a>Choice of learning rate</h4><p><strong>Reminder</strong>:<br>In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $\alpha$  determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p><p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code>learning_rates</code> variable to contain, and see what happens. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = [<span class="number">0.01</span>, <span class="number">0.001</span>, <span class="number">0.0001</span>]</span><br><span class="line">models = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"learning rate is: "</span> + str(i))</span><br><span class="line">    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">1500</span>, learning_rate = i, print_cost = <span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">'\n'</span> + <span class="string">"-------------------------------------------------------"</span> + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    plt.plot(np.squeeze(models[str(i)][<span class="string">"costs"</span>]), label= str(models[str(i)][<span class="string">"learning_rate"</span>]))</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations'</span>)</span><br><span class="line"></span><br><span class="line">legend = plt.legend(loc=<span class="string">'upper center'</span>, shadow=<span class="literal">True</span>)</span><br><span class="line">frame = legend.get_frame()</span><br><span class="line">frame.set_facecolor(<span class="string">'0.90'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>learning rate is: 0.01[0.97125943 0.9155338  0.92079132 0.96358044 0.78924234 0.60411297 0.01179527 0.89814048 0.91522859 0.70264065 0.19380387 0.49537355 0.7927164  0.85423431 0.00298587 0.96199699 0.01234735 0.9107653 0.13661137 0.01424336 0.96894735 0.1033746  0.00579297 0.86081326 0.53811196 0.64950178 0.83272843 0.00426307 0.0131452  0.99947804 0.11468372 0.82182442 0.69611733 0.4991522  0.67231401 0.01728165 0.04136099 0.80069693 0.26832359 0.03958566 0.74731239 0.32116434 0.71871197 0.01205725 0.96879962 0.62310364 0.17737126 0.98960523 0.74697265 0.07284605][1.47839654e-01 5.78008187e-02 9.42385025e-01 4.14849240e-05 2.27209941e-02 7.29254668e-02 2.23704494e-02 9.49717864e-01 5.41724296e-02 2.92729895e-02 6.82412300e-02 8.33370210e-01 1.71420615e-01 9.66879883e-01 8.11537151e-01 2.44343483e-02 7.87634098e-03 2.64027272e-02 5.60720049e-02 9.53130353e-01 5.30865324e-03 3.11020745e-02 1.43606493e-01 1.92650473e-02 9.30132798e-01 8.95291211e-01 2.72790550e-02 9.01480142e-01 2.73987904e-02 8.09041583e-01 6.64266068e-02 5.00479730e-02 1.29245158e-01 1.40274640e-01 6.48179131e-02 1.35261338e-02 4.77693620e-03 2.65922710e-02 8.89771230e-01 2.64826222e-01 1.22921585e-02 6.03229153e-01 8.81822076e-01 1.35079742e-02 2.49595285e-02 6.88961126e-02 5.86046930e-02 8.68932415e-01 5.14520331e-03 1.21099845e-02 8.23403970e-01 1.70985647e-01 9.49977566e-02 3.04227660e-01 9.48091298e-01 8.09204729e-04 9.66640038e-01 8.78319466e-01 3.17284880e-02 9.76165700e-01 8.81584697e-01 8.48145722e-01 2.70795160e-02 2.28390397e-02 1.05295676e-01 4.45165292e-02 1.22858876e-02 1.35813814e-01 8.25867437e-01 9.21552651e-03 2.49353830e-02 9.88067070e-01 5.78381495e-02 8.57292849e-02 4.10128551e-02 5.70507956e-01 2.11603229e-04 1.52264723e-02 6.18390722e-02 1.39187810e-01 6.68993749e-02 4.14281785e-04 1.23347660e-02 9.24789062e-01 8.16880995e-01 9.29503655e-03 8.23770893e-02 2.75905821e-02 8.52215781e-01 2.36580782e-02 1.75344552e-01 6.15499363e-02 6.58017000e-01 9.54697511e-01 9.62775471e-01 1.05372217e-01 9.37239413e-02 9.29062265e-01 2.68654456e-02 1.44668290e-01 9.15662947e-02 2.89260930e-02 8.02603133e-01 6.11847790e-02 9.87937140e-01 5.84677169e-02 9.87171184e-01 8.37167548e-01 8.94717386e-01 8.58260204e-01 9.36232298e-01 9.33067878e-01 8.77279900e-03 5.88387682e-02 5.09517612e-02 2.40626781e-02 3.87480256e-02 9.35343373e-01 2.35202639e-03 8.83972091e-02 4.49639004e-03 6.64404296e-01 1.76677024e-02 2.75426440e-05 8.71728805e-01 2.43292078e-03 8.92351131e-01 9.50411299e-02 9.66495010e-01 9.27285472e-01 2.66413779e-01 8.70883114e-02 5.40743542e-02 9.75155426e-01 8.02323751e-01 6.92965782e-01 9.06287458e-01 9.39900204e-01 1.64790714e-03 1.91364329e-02 1.66925680e-02 1.46846281e-02 9.39237709e-01 2.57925925e-03 8.19134439e-01 8.54311895e-01 9.10765301e-01 1.20452016e-01 9.10603560e-01 9.11977137e-01 3.72174950e-01 6.13527932e-02 1.30882743e-02 9.55225821e-01 4.30680816e-02 1.37970158e-01 9.60868956e-01 8.67705030e-03 5.95741909e-03 2.19466774e-02 1.78308409e-03 2.57658927e-02 8.63787547e-01 3.44218954e-02 9.34152347e-01 9.35483274e-03 9.90908018e-01 1.17832722e-02 2.67756870e-02 7.74546160e-01 8.43831858e-01 9.38847463e-01 1.48599256e-01 4.17198956e-03 9.81043189e-01 8.22764984e-01 1.92120393e-02 8.58870443e-01 5.37478573e-02 7.84878423e-01 3.56080493e-02 2.80545014e-02 1.09777935e-02 1.30396160e-02 3.81067987e-04 8.51025984e-01 2.44476492e-02 4.57657708e-02 8.81871553e-01 1.06481927e-02 2.84032919e-02 1.96773463e-02 8.54577180e-01 3.01055581e-02 1.33843958e-03 7.04152762e-02 3.08344786e-01 9.25167630e-01 4.53183035e-02 9.31980521e-03 8.69872444e-01 4.61339718e-03 4.86286962e-03 7.32772398e-03 1.26009270e-01 1.46124056e-01 4.51019670e-02 1.45139959e-01 1.45971589e-01]train accuracy: 99.52153110047847 %test accuracy: 68.0 %-------------------------------------------------------learning rate is: 0.001[0.74458179 0.63302701 0.70621076 0.7037801  0.5322598  0.43784581 0.1843739  0.71778574 0.73717649 0.59122536 0.39837511 0.44491784 0.63244572 0.53976962 0.09938522 0.7227688  0.12316033 0.58301417 0.28145733 0.16609522 0.61461919 0.14166416 0.0865388  0.4251847 0.67719513 0.61251308 0.46730808 0.11854922 0.21041046 0.8906756 0.42313203 0.56013238 0.60322016 0.37148913 0.57460259 0.11968291 0.24088599 0.65905854 0.4782032  0.14862075 0.4992436  0.61682528 0.4795275  0.16260336 0.70722369 0.23929218 0.36719514 0.87223907 0.45484261 0.19029187][0.34403391 0.18575705 0.63392388 0.00949352 0.185803   0.30979007 0.13544854 0.75931407 0.18856286 0.16653711 0.47903517 0.55094252 0.39894694 0.67613631 0.32941411 0.15120523 0.1515817  0.10868391 0.21533234 0.78261458 0.09236643 0.13102179 0.30209379 0.22018859 0.60467471 0.63089631 0.13786841 0.52162666 0.2229145  0.41807311 0.20928386 0.22354737 0.51863273 0.37446655 0.12619979 0.24763606 0.08217106 0.20570627 0.61668309 0.47341694 0.07578526 0.20272218 0.63694514 0.17332725 0.12774778 0.38987251 0.25716102 0.57589232 0.03660729 0.23627192 0.5058546  0.44851881 0.26882028 0.54506441 0.63427748 0.07593065 0.79389128 0.55848777 0.20399827 0.82950311 0.67551516 0.49340246 0.12825017 0.19483707 0.30405843 0.25239064 0.23849329 0.28306742 0.39562206 0.1338017  0.20953382 0.84559705 0.25983452 0.43347997 0.25869745 0.5275365  0.01851016 0.18226072 0.11686925 0.24360522 0.11457144 0.09711829 0.11403479 0.64158072 0.56492264 0.14249209 0.26621215 0.23562087 0.63347539 0.19718838 0.41665293 0.2560914  0.20511226 0.75854285 0.62700096 0.22437352 0.24158168 0.58986637 0.18250551 0.31168748 0.40230892 0.18766222 0.37363736 0.24954905 0.81540625 0.33905562 0.85287524 0.46460165 0.64873862 0.49476607 0.58689285 0.73160658 0.15974705 0.28192355 0.21969254 0.17213348 0.24140747 0.59506433 0.09843999 0.4664941 0.11789794 0.41615495 0.28828188 0.01549565 0.57657208 0.03491378 0.56433333 0.52342054 0.58263447 0.828261   0.33112864 0.30054751 0.13866344 0.7796039  0.49905825 0.23849455 0.65130553 0.54865883 0.07475945 0.15289783 0.17277205 0.21093974 0.77996081 0.05731401 0.43542011 0.62528802 0.58301417 0.39592429 0.62711359 0.62164606 0.52142034 0.2237536  0.11263677 0.69875451 0.13460421 0.59843604 0.62542932 0.06223702 0.08147044 0.16677973 0.0471795  0.29026597 0.53465382 0.37298272 0.67567251 0.08157721 0.80300364 0.36035876 0.12411481 0.3216639  0.50044148 0.71061923 0.38536321 0.15865892 0.73153708 0.60089581 0.19123039 0.60169201 0.21145324 0.35174627 0.1036799  0.15432715 0.08266478 0.1765554  0.0315303  0.4005418 0.12684962 0.13818573 0.69657105 0.10353719 0.15229696 0.170385 0.39005657 0.21251557 0.03788715 0.32853945 0.47339083 0.62631301 0.22739755 0.0902158  0.56782331 0.17507791 0.20252309 0.09500011 0.34504767 0.39618939 0.25822558 0.24550552 0.30677401]train accuracy: 88.99521531100478 %test accuracy: 64.0 %-------------------------------------------------------learning rate is: 0.0001[0.45098635 0.48539489 0.40959087 0.44864257 0.32818894 0.43729766 0.28884626 0.46438078 0.45494399 0.45491705 0.36938309 0.41863679 0.45816519 0.5031755  0.2842568  0.45155065 0.30672371 0.37824086 0.26505548 0.27737934 0.40677576 0.28781555 0.24304775 0.38397796 0.50642581 0.47047843 0.35358916 0.31561491 0.39430714 0.4603235 0.37998879 0.3764821  0.32056264 0.38693085 0.40764828 0.23150119 0.311659   0.44981144 0.43152263 0.26276732 0.37785575 0.48883282 0.37790798 0.30969512 0.47842906 0.32857529 0.34457076 0.60547775 0.40733226 0.28828383][0.4225819  0.31692389 0.42964509 0.14896683 0.28783033 0.38652698 0.29492571 0.44991522 0.31988018 0.32391139 0.39318147 0.34804173 0.40099138 0.31694856 0.28102266 0.3231201  0.25486297 0.18485428 0.31900054 0.52941528 0.25568417 0.27297382 0.29762542 0.35834172 0.38912252 0.4552143  0.2555983  0.34830216 0.29078565 0.27432926 0.31094887 0.44330557 0.47172673 0.39765449 0.22386371 0.46108148 0.27055987 0.31333951 0.49901097 0.439851   0.23953174 0.29809115 0.42197081 0.28385499 0.2465556  0.40478121 0.35487343 0.45521241 0.1451398  0.37485678 0.36671611 0.37909623 0.30298036 0.40151709 0.40460677 0.24757226 0.50122617 0.38917296 0.3687779  0.50666786 0.52492017 0.37864634 0.24031899 0.30627306 0.35114005 0.37398054 0.43104844 0.31851314 0.37029232 0.29232461 0.37616632 0.52373453 0.32507684 0.48381803 0.39170698 0.38646363 0.17397111 0.31623794 0.24714356 0.35235176 0.17699762 0.33409149 0.3249697  0.49338319 0.413497   0.25575577 0.31724095 0.36456893 0.45018347 0.34698807 0.38500817 0.46053573 0.23091555 0.47268593 0.43291929 0.26828088 0.30258929 0.4164904  0.24453098 0.27111067 0.39272427 0.3043153 0.27984468 0.39085873 0.52409691 0.34966557 0.55950717 0.37739831 0.43697184 0.33557455 0.38439647 0.48607992 0.30731772 0.31028645 0.30037443 0.29400226 0.42714997 0.42909528 0.30432821 0.5227302 0.32144602 0.4066469  0.43683178 0.17768611 0.4354696  0.18662466 0.39558874 0.48819809 0.35425959 0.57279833 0.35098941 0.33429763 0.31014998 0.49175402 0.44154511 0.31203466 0.38776576 0.38352489 0.29611222 0.36104647 0.33824864 0.37198268 0.52831432 0.25732676 0.36743298 0.44902822 0.37824086 0.36790056 0.41246464 0.37833397 0.3418507  0.30119593 0.2592477  0.46753926 0.26777792 0.43134603 0.32491304 0.19700069 0.25937972 0.33143626 0.19820128 0.35468513 0.36334932 0.51823778 0.37235121 0.27650473 0.47271147 0.44760504 0.33240186 0.29967323 0.41157608 0.47817969 0.39048545 0.28309008 0.46350184 0.41099669 0.34508275 0.4323286  0.35065016 0.33976266 0.25459527 0.29233107 0.26976618 0.30004182 0.18212017 0.34254174 0.26213135 0.2674514  0.45817075 0.24356149 0.25227369 0.34588203 0.3783451  0.32762257 0.19831251 0.51451759 0.3792938  0.41417054 0.34795587 0.25521854 0.42313521 0.32557428 0.38342989 0.21943589 0.34909483 0.39399177 0.36128874 0.38042346 0.38929593]train accuracy: 68.42105263157895 %test accuracy: 36.0 %-------------------------------------------------------</code></pre><center><img src='https://lh3.googleusercontent.com/VISa3bYoaFyoyL4EnFg4OhGXPa_aIgC3M3hDJsOQP-3PK1ICbo9ZT3udg7Jp_nCRj9rxEjREN-wP83WaR6EVAV2dCC1FogwYI91awu3uX9EF0cUh3jApJxQb-0_VKQtBc0Telo5-Kw=w2400' /></center><p><b>Interpretation:</b> </p><ul><li>Different learning rates give different costs and thus different predictions results.</li><li>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). </li><li>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</li><li>In deep learning, we usually recommend that you: <ul><li>Choose the learning rate that better minimizes the cost function.</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) </li></ul></li></ul><h2 id="7-Test-with-your-own-image-optional-ungraded-exercise"><a href="#7-Test-with-your-own-image-optional-ungraded-exercise" class="headerlink" title="7 - Test with your own image (optional/ungraded exercise)"></a>7 - Test with your own image (optional/ungraded exercise)</h2><p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:<br>    1. Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub.<br>    2. Add your image to this Jupyter Notebook’s directory, in the “images” folder<br>    3. Change your image’s name in the following code<br>    4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## START CODE HERE ## (PUT YOUR IMAGE NAME) </span></span><br><span class="line">my_image = <span class="string">"my_image.jpg"</span>   <span class="comment"># change this to the name of your image file </span></span><br><span class="line"><span class="comment">## END CODE HERE ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We preprocess the image to fit your algorithm.</span></span><br><span class="line">fname = <span class="string">"images/"</span> + my_image</span><br><span class="line">image = np.array(plt.imread(fname))</span><br><span class="line">my_image = skimage.transform.resize(image, output_shape=(num_px,num_px)).reshape((<span class="number">1</span>, num_px*num_px*<span class="number">3</span>)).T</span><br><span class="line">my_predicted_image = predict(d[<span class="string">"w"</span>], d[<span class="string">"b"</span>], my_image)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line">print(<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your algorithm predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><pre><code>/Users/abanihi/opt/miniconda3/envs/pangeo/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in skimage 0.15.  warn(&quot;The default mode, &apos;constant&apos;, will be changed to &apos;reflect&apos; in &quot;/Users/abanihi/opt/miniconda3/envs/pangeo/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.  warn(&quot;Anti-aliasing will be enabled by default in skimage 0.15 to &quot;[0.30930831]y = 0.0, your algorithm predicts a &quot;non-cat&quot; picture.</code></pre><center><img src='https://lh3.googleusercontent.com/zRQjYeR_0i0Az8BynDY35YelX_s5nomdk0mQAlOjWdhWvSe70Ey4EKZntInJN3WwEb9i9lusqNNqtsBXc15qaEj0eyH8uYlDNxr72DjbeGaqvPjE9iZqmZcTozgW157sVuTlGePc8Q=w2400' /></center><p><b><h4>What to remember from this assignment:</h4></b><br><font color='blue'></p><ol><li>Preprocessing the dataset is important.</li><li>You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().</li><li>Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!</li></ol><p>Finally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:<br>    - Play with the learning rate and the number of iterations<br>    - Try different initialization methods and compare the results<br>    - Test other preprocessings (center the data, or divide each row by its standard deviation)<br></font></p><p>Bibliography:</p><ul><li><span class="exturl" data-url="aHR0cHM6Ly9zdGF0cy5zdGFja2V4Y2hhbmdlLmNvbS9xdWVzdGlvbnMvMjExNDM2L3doeS1kby13ZS1ub3JtYWxpemUtaW1hZ2VzLWJ5LXN1YnRyYWN0aW5nLXRoZS1kYXRhc2V0cy1pbWFnZS1tZWFuLWFuZC1ub3QtdGhlLWM=">https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c<i class="fa fa-external-link-alt"></i></span></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext version_information</span><br><span class="line">%version_information skimage, numpy, matplotlib, h5py, sklearn</span><br></pre></td></tr></table></figure><table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.6 64bit [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]</td></tr><tr><td>IPython</td><td>7.0.1</td></tr><tr><td>OS</td><td>Darwin 17.7.0 x86_64 i386 64bit</td></tr><tr><td>skimage</td><td>0.14.1</td></tr><tr><td>numpy</td><td>1.15.1</td></tr><tr><td>matplotlib</td><td>3.0.0</td></tr><tr><td>h5py</td><td>2.8.0</td></tr><tr><td>sklearn</td><td>0.20.0</td></tr><tr><td colspan='2'>Mon Oct 15 08:33:43 2018 MDT</td></tr></table><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Logistic Regression with a Neural Network mindset Assignment || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)&lt;br&gt;&lt;b&gt;Note - These are my notes on the second course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;br&gt;All Credits to the teacher &lt;b&gt;andrew ng &lt;/b&gt; :)&lt;/p&gt;
    
    </summary>
    
    
      <category term="2020" scheme="https://massivefile.com/categories/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/categories/2020/Assignment/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/tags/Assignment/"/>
    
      <category term="Logistic Regression with a Neural Network mindset" scheme="https://massivefile.com/tags/Logistic-Regression-with-a-Neural-Network-mindset/"/>
    
  </entry>
  
  <entry>
    <title>Vectorization || Neural Networks and Deep Learning (Week 2)</title>
    <link href="https://massivefile.com/Vectorization/"/>
    <id>https://massivefile.com/Vectorization/</id>
    <published>2020-04-10T18:30:00.000Z</published>
    <updated>2020-04-13T14:46:52.128Z</updated>
    
    <content type="html"><![CDATA[<p>Vectorization || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)<br><b>Note - These are my notes on the second course of DeepLearning.ai by AndrewNg</b><br>All Credits to the teacher <b>andrew ng </b> :)</p><a id="more"></a><h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1> <p><div class="lev1 toc-item"><a href="#Vectorization" data-toc-modified-id="Vectorization-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Vectorization</a></div><div class="lev1 toc-item"><a href="#Vectorizing-Logistic-Regression" data-toc-modified-id="Vectorizing-Logistic-Regression-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Vectorizing Logistic Regression</a></div><div class="lev1 toc-item"><a href="#Vectorizing-Logistic-Regression's-Gradient-Output" data-toc-modified-id="Vectorizing-Logistic-Regression's-Gradient-Output-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Vectorizing Logistic Regression's Gradient Output</a></div><div class="lev1 toc-item"><a href="#A-note-on-python/numpy-vectors" data-toc-modified-id="A-note-on-python/numpy-vectors-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>A note on python/numpy vectors</a></div><h1 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h1><p><img src="https://i.imgur.com/yz60bns.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%time c = np.dot(a, b)</span><br></pre></td></tr></table></figure><pre><code>CPU times: user 1.62 ms, sys: 727 µs, total: 2.34 msWall time: 1.06 ms</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span><span class="params">()</span>:</span></span><br><span class="line">    c = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">        c += a[i] * b[i]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%time loop()</span><br></pre></td></tr></table></figure><pre><code>CPU times: user 316 ms, sys: 2.56 ms, total: 319 msWall time: 317 ms</code></pre><p><img src="https://i.imgur.com/p94lp3F.png" alt=""></p><p><img src="https://i.imgur.com/62FCVzX.png" alt=""></p><p><img src="https://i.imgur.com/lQnsLDn.png" alt=""></p><h1 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h1><p><img src="https://i.imgur.com/3NO1maQ.png" alt=""></p><h1 id="Vectorizing-Logistic-Regression’s-Gradient-Output"><a href="#Vectorizing-Logistic-Regression’s-Gradient-Output" class="headerlink" title="Vectorizing Logistic Regression’s Gradient Output"></a>Vectorizing Logistic Regression’s Gradient Output</h1><p><img src="https://i.imgur.com/5HLkUab.png" alt=""></p><h1 id="A-note-on-python-numpy-vectors"><a href="#A-note-on-python-numpy-vectors" class="headerlink" title="A note on python/numpy vectors"></a>A note on python/numpy vectors</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><pre><code>array([-0.91796822, -0.53903443,  1.00289266,  0.22272871, -0.35617949])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.shape</span><br></pre></td></tr></table></figure><pre><code>(5,)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.T</span><br></pre></td></tr></table></figure><pre><code>array([-0.91796822, -0.53903443,  1.00289266,  0.22272871, -0.35617949])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.dot(a, a.T)</span><br></pre></td></tr></table></figure><pre><code>2.3154893533786054</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">a</span><br></pre></td></tr></table></figure><pre><code>array([[-1.26834861],       [-0.254855  ],       [-1.37786229],       [ 0.18718574],       [-1.31341244]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.shape</span><br></pre></td></tr></table></figure><pre><code>(5, 1)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.T</span><br></pre></td></tr></table></figure><pre><code>array([[-1.26834861, -0.254855  , -1.37786229,  0.18718574, -1.31341244]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.dot(a, a.T)</span><br></pre></td></tr></table></figure><pre><code>array([[ 1.60870819,  0.32324498,  1.74760972, -0.23741677,  1.66586484],       [ 0.32324498,  0.06495107,  0.35115509, -0.04770522,  0.33472972],       [ 1.74760972,  0.35115509,  1.8985045 , -0.25791618,  1.80970147],       [-0.23741677, -0.04770522, -0.25791618,  0.0350385 , -0.24585208],       [ 1.66586484,  0.33472972,  1.80970147, -0.24585208,  1.72505223]])</code></pre><p><img src="https://i.imgur.com/RWHkYsw.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext version_information</span><br><span class="line">%version_information numpy</span><br></pre></td></tr></table></figure><table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.6 64bit [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]</td></tr><tr><td>IPython</td><td>7.0.1</td></tr><tr><td>OS</td><td>Darwin 17.7.0 x86_64 i386 64bit</td></tr><tr><td>numpy</td><td>1.15.1</td></tr><tr><td colspan='2'>Sun Oct 14 19:41:16 2018 MDT</td></tr></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Vectorization || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)&lt;br&gt;&lt;b&gt;Note - These are my notes on the second course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;br&gt;All Credits to the teacher &lt;b&gt;andrew ng &lt;/b&gt; :)&lt;/p&gt;
    
    </summary>
    
    
      <category term="2020" scheme="https://massivefile.com/categories/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/categories/2020/Assignment/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/tags/Assignment/"/>
    
      <category term="Vectorization" scheme="https://massivefile.com/tags/Vectorization/"/>
    
  </entry>
  
  <entry>
    <title>Python Basics With Numpy || Neural Networks and Deep Learning(Week 2)</title>
    <link href="https://massivefile.com/Python_Basics_With_Numpy/"/>
    <id>https://massivefile.com/Python_Basics_With_Numpy/</id>
    <published>2020-04-09T18:30:00.000Z</published>
    <updated>2020-04-13T16:20:05.364Z</updated>
    
    <content type="html"><![CDATA[<p>Python Basics With Numpy || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)<br><b>Note - These are my notes on the second course of DeepLearning.ai by AndrewNg</b><br>All Credits to the teacher <b>andrew ng </b> :)</p><a id="more"></a><h1 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h1> <p><div class="lev1 toc-item"><a href="#Python-Basics-with-Numpy-(optional-assignment)" data-toc-modified-id="Python-Basics-with-Numpy-(optional-assignment)-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Python Basics with Numpy (optional assignment)</a></div><div class="lev2 toc-item"><a href="#About-iPython-Notebooks" data-toc-modified-id="About-iPython-Notebooks-11"><span class="toc-item-num">1.1&nbsp;&nbsp;</span>About iPython Notebooks</a></div><div class="lev2 toc-item"><a href="#1---Building-basic-functions-with-numpy" data-toc-modified-id="1---Building-basic-functions-with-numpy-12"><span class="toc-item-num">1.2&nbsp;&nbsp;</span>1 - Building basic functions with numpy</a></div><div class="lev3 toc-item"><a href="#1.1---sigmoid-function,-np.exp()" data-toc-modified-id="1.1---sigmoid-function,-np.exp()-121"><span class="toc-item-num">1.2.1&nbsp;&nbsp;</span>1.1 - sigmoid function, np.exp()</a></div><div class="lev3 toc-item"><a href="#1.2---Sigmoid-gradient" data-toc-modified-id="1.2---Sigmoid-gradient-122"><span class="toc-item-num">1.2.2&nbsp;&nbsp;</span>1.2 - Sigmoid gradient</a></div><div class="lev3 toc-item"><a href="#1.3---Reshaping-arrays" data-toc-modified-id="1.3---Reshaping-arrays-123"><span class="toc-item-num">1.2.3&nbsp;&nbsp;</span>1.3 - Reshaping arrays</a></div><div class="lev3 toc-item"><a href="#1.4---Normalizing-rows" data-toc-modified-id="1.4---Normalizing-rows-124"><span class="toc-item-num">1.2.4&nbsp;&nbsp;</span>1.4 - Normalizing rows</a></div><div class="lev3 toc-item"><a href="#1.5---Broadcasting-and-the-softmax-function" data-toc-modified-id="1.5---Broadcasting-and-the-softmax-function-125"><span class="toc-item-num">1.2.5&nbsp;&nbsp;</span>1.5 - Broadcasting and the softmax function</a></div><div class="lev2 toc-item"><a href="#2)-Vectorization" data-toc-modified-id="2)-Vectorization-13"><span class="toc-item-num">1.3&nbsp;&nbsp;</span>2) Vectorization</a></div><div class="lev3 toc-item"><a href="#2.1-Implement-the-L1-and-L2-loss-functions" data-toc-modified-id="2.1-Implement-the-L1-and-L2-loss-functions-131"><span class="toc-item-num">1.3.1&nbsp;&nbsp;</span>2.1 Implement the L1 and L2 loss functions</a></div><h1 id="Python-Basics-with-Numpy-optional-assignment"><a href="#Python-Basics-with-Numpy-optional-assignment" class="headerlink" title="Python Basics with Numpy (optional assignment)"></a>Python Basics with Numpy (optional assignment)</h1><p>Welcome to your first assignment. This exercise gives you a brief introduction to Python. Even if you’ve used Python before, this will help familiarize you with functions we’ll need.  </p><p><strong>Instructions:</strong></p><ul><li>You will be using Python 3.</li><li>Avoid using for-loops and while-loops, unless you are explicitly told to do so.</li><li>Do not modify the (GRADED FUNCTION [function name]) comment in some cells. Your work would not be graded if you change this. Each cell containing that comment should only contain one function.</li><li>After coding your function, run the cell right below it to check if your result is correct.</li></ul><p><strong>After this assignment you will:</strong></p><ul><li>Be able to use iPython Notebooks</li><li>Be able to use numpy functions and numpy matrix/vector operations</li><li>Understand the concept of “broadcasting”</li><li>Be able to vectorize code</li></ul><p>Let’s get started!</p><h2 id="About-iPython-Notebooks"><a href="#About-iPython-Notebooks" class="headerlink" title="About iPython Notebooks"></a>About iPython Notebooks</h2><p>iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments. After writing your code, you can run the cell by either pressing “SHIFT”+”ENTER” or by clicking on “Run Cell” (denoted by a play symbol) in the upper bar of the notebook. </p><p>We will often specify “(≈ X lines of code)” in the comments to tell you about how much code you need to write. It is just a rough estimate, so don’t feel bad if your code is longer or shorter.</p><p><strong>Exercise</strong>: Set test to <code>&quot;Hello World&quot;</code> in the cell below to print “Hello World” and run the two cells below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">test = <span class="string">"Hello World"</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (<span class="string">"test: "</span> + test)</span><br></pre></td></tr></table></figure><pre><code>test: Hello World</code></pre><p><strong>Expected output</strong>:<br>test: Hello World</p><font color='blue'>**What you need to remember**:- Run your cells using SHIFT+ENTER (or "Run cell")- Write code in the designated areas using Python 3 only- Do not modify the code outside of the designated areas</font><h3> 1 - Building basic functions with numpy </h3><p>Numpy is the main package for scientific computing in Python. It is maintained by a large community (<span class="exturl" data-url="aHR0cHM6Ly93d3cubnVtcHkub3Jn">https://www.numpy.org<i class="fa fa-external-link-alt"></i></span>). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape. You will need to know how to use these functions for future assignments.</p><h3 id="1-1-sigmoid-function-np-exp"><a href="#1-1-sigmoid-function-np-exp" class="headerlink" title="1.1 - sigmoid function, np.exp()"></a>1.1 - sigmoid function, np.exp()</h3><p>Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp().</p><p><strong>Exercise</strong>: Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function.</p><p><strong>Reminder</strong>:<br>$sigmoid(x) = \frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.</p><p>To refer to a function belonging to a specific package you could call it using package_name.function(). Run the code below to see an example with math.exp().</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: basic_sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">basic_sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute sigmoid of x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + math.exp(-x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">basic_sigmoid(<span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>0.9525741268224334</code></pre><p><strong>Expected Output</strong>: </p><table style = "width:40%">    <tr>    <td>** basic_sigmoid(3) **</td>         <td>0.9525741268224334 </td>     </tr></table><p>Actually, we rarely use the “math” library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### One reason why we use "numpy" instead of "math" in Deep Learning ###</span></span><br><span class="line"><span class="comment"># x = [1, 2, 3]</span></span><br><span class="line"><span class="comment"># basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector.</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-5-8ccefa5bf989&gt; in &lt;module&gt;      1 ### One reason why we use &quot;numpy&quot; instead of &quot;math&quot; in Deep Learning ###      2 x = [1, 2, 3]----&gt; 3 basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector.&lt;ipython-input-3-6bbe580769a3&gt; in basic_sigmoid(x)     15      16     ### START CODE HERE ### (≈ 1 line of code)---&gt; 17     s = 1 / (1 + math.exp(-x))     18     ### END CODE HERE ###     19 TypeError: bad operand type for unary -: &apos;list&apos;</code></pre><p>In fact, if $ x = (x_1, x_2, …, x_n)$ is a row vector then $np.exp(x)$ will apply the exponential function to every element of x. The output will thus be: $np.exp(x) = (e^{x_1}, e^{x_2}, …, e^{x_n})$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># example of np.exp</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(np.exp(x)) <span class="comment"># result is (exp(1), exp(2), exp(3))</span></span><br></pre></td></tr></table></figure><pre><code>[ 2.71828183  7.3890561  20.08553692]</code></pre><p>Furthermore, if x is a vector, then a Python operation such as $s = x + 3$ or $s = \frac{1}{x}$ will output s as a vector of the same size as x.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example of vector operation</span></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (x + <span class="number">3</span>)</span><br></pre></td></tr></table></figure><pre><code>[4 5 6]</code></pre><p>Any time you need more info on a numpy function, we encourage you to look at <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2MvbnVtcHktMS4xMC4xL3JlZmVyZW5jZS9nZW5lcmF0ZWQvbnVtcHkuZXhwLmh0bWw=">the official documentation<i class="fa fa-external-link-alt"></i></span>. </p><p>You can also create a new cell in the notebook and write <code>np.exp?</code> (for example) to get quick access to the documentation.</p><p><strong>Exercise</strong>: Implement the sigmoid function using numpy. </p><p><strong>Instructions</strong>: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices…) are called numpy arrays. You don’t need to know more for now.<br>$$<br>\text{For } x \in \mathbb{R}^n \<br>\text{,     } sigmoid(x) = $$<br>sigmoid<br>$$<br>\begin{pmatrix}<br>    x_1  \<br>    x_2  \<br>    …  \<br>    x_n  \<br>\end{pmatrix} =<br>\begin{pmatrix}<br>    \frac{1}{1+e^{-x_1}}  \<br>    \frac{1}{1+e^{-x_2}}  \<br>    …  \<br>    \frac{1}{1+e^{-x_n}}  \<br>\end{pmatrix}\tag{1} $$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># this means you can access numpy functions by writing np.function() instead of numpy.function()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1.</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">sigmoid(x)</span><br></pre></td></tr></table></figure><pre><code>array([0.73105858, 0.88079708, 0.95257413])</code></pre><p><strong>Expected Output</strong>: </p><table>    <tr>         <td> **sigmoid([1,2,3])**</td>         <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td>     </tr></table> <h3 id="1-2-Sigmoid-gradient"><a href="#1-2-Sigmoid-gradient" class="headerlink" title="1.2 - Sigmoid gradient"></a>1.2 - Sigmoid gradient</h3><p>As you’ve seen in lecture, you will need to compute gradients to optimize loss functions using backpropagation. Let’s code your first gradient function.</p><p><strong>Exercise</strong>: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid_derivative(x) = \sigma’(x) = $$<br>$$ \sigma(x) (1 - \sigma(x))\tag{2}$$<br>You often code this function in two steps:</p><ol><li>Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.</li><li>Compute $\sigma’(x) = s(1-s)$</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid_derivative</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    s = sigmoid(x)</span><br><span class="line">    ds = s * (<span class="number">1</span>-s)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"sigmoid_derivative(x) = "</span> + str(sigmoid_derivative(x)))</span><br></pre></td></tr></table></figure><pre><code>sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]</code></pre><p><strong>Expected Output</strong>: </p><table>    <tr>         <td> **sigmoid_derivative([1,2,3])**</td>         <td> [ 0.19661193  0.10499359  0.04517666] </td>     </tr></table> <h3 id="1-3-Reshaping-arrays"><a href="#1-3-Reshaping-arrays" class="headerlink" title="1.3 - Reshaping arrays"></a>1.3 - Reshaping arrays</h3><p>Two common numpy functions used in deep learning are <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2MvbnVtcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9udW1weS5uZGFycmF5LnNoYXBlLmh0bWw=">np.shape<i class="fa fa-external-link-alt"></i></span> and <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2MvbnVtcHkvcmVmZXJlbmNlL2dlbmVyYXRlZC9udW1weS5yZXNoYXBlLmh0bWw=">np.reshape()<i class="fa fa-external-link-alt"></i></span>. </p><ul><li>X.shape is used to get the shape (dimension) of a matrix/vector X. </li><li>X.reshape(…) is used to reshape X into some other dimension. </li></ul><p>For example, in computer science, an image is represented by a 3D array of shape $$ (length, height, depth = 3) $$. However, when you read an image as the input of an algorithm you convert it to a vector of shape - (length<em>height</em>3, 1) </p><p>In other words, you “unroll”, or reshape, the 3D array into a 1D vector.</p><p><strong>Exercise</strong>: Implement <code>image2vector()</code> that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v = v.reshape((v.shape[<span class="number">0</span>]*v.shape[<span class="number">1</span>], v.shape[<span class="number">2</span>])) <span class="comment"># v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c</span></span><br></pre></td></tr></table></figure><ul><li>Please don’t hardcode the dimensions of image as a constant. Instead look up the quantities you need with <code>image.shape[0]</code>, etc. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: image2vector</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    v = image.reshape(image.size, <span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span></span><br><span class="line">image = np.array([[[ <span class="number">0.67826139</span>,  <span class="number">0.29380381</span>],</span><br><span class="line">        [ <span class="number">0.90714982</span>,  <span class="number">0.52835647</span>],</span><br><span class="line">        [ <span class="number">0.4215251</span> ,  <span class="number">0.45017551</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.92814219</span>,  <span class="number">0.96677647</span>],</span><br><span class="line">        [ <span class="number">0.85304703</span>,  <span class="number">0.52351845</span>],</span><br><span class="line">        [ <span class="number">0.19981397</span>,  <span class="number">0.27417313</span>]],</span><br><span class="line"></span><br><span class="line">       [[ <span class="number">0.60659855</span>,  <span class="number">0.00533165</span>],</span><br><span class="line">        [ <span class="number">0.10820313</span>,  <span class="number">0.49978937</span>],</span><br><span class="line">        [ <span class="number">0.34144279</span>,  <span class="number">0.94630077</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"image2vector(image) = "</span> + str(image2vector(image)))</span><br></pre></td></tr></table></figure><pre><code>image2vector(image) = [[0.67826139] [0.29380381] [0.90714982] [0.52835647] [0.4215251 ] [0.45017551] [0.92814219] [0.96677647] [0.85304703] [0.52351845] [0.19981397] [0.27417313] [0.60659855] [0.00533165] [0.10820313] [0.49978937] [0.34144279] [0.94630077]]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:100%">     <tr>        <td> **image2vector(image)** </td>        <td> [[ 0.67826139] [ 0.29380381] [ 0.90714982] [ 0.52835647] [ 0.4215251 ] [ 0.45017551] [ 0.92814219] [ 0.96677647] [ 0.85304703] [ 0.52351845] [ 0.19981397] [ 0.27417313] [ 0.60659855] [ 0.00533165] [ 0.10820313] [ 0.49978937] [ 0.34144279] [ 0.94630077]]</td>      </tr></table><h3 id="1-4-Normalizing-rows"><a href="#1-4-Normalizing-rows" class="headerlink" title="1.4 - Normalizing rows"></a>1.4 - Normalizing rows</h3><p>Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to $ \frac{x}{| x|} $ (dividing each row vector of x by its norm).</p><p>For example, if $$x =<br>\begin{bmatrix}<br>    0 &amp; 3 &amp; 4 \<br>    2 &amp; 6 &amp; 4 \<br>\end{bmatrix}\tag{3}$$ then $$| x| = \<br>np.linalg.norm(x, axis = 1, keepdims = True) = $$<br>$$<br> \begin{bmatrix} \<br>    5 \<br>    \sqrt{56} \<br>\end{bmatrix}\tag{4} $$<br>and<br>$$ x_normalized = \frac{x}{| x|} = $$<br>$$ \begin{bmatrix} \<br>    0 &amp; \frac{3}{5} &amp; \frac{4}{5} \<br>    \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \<br>\end{bmatrix}\tag{5}$$<br>Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you’re going to learn about it in part 5.</p><p><strong>Exercise</strong>: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: normalizeRows</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalizeRows</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a function that normalizes each row of the matrix x (to have unit length).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)</span></span><br><span class="line">    x_norm = np.linalg.norm(x, ord=<span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Divide x by its norm.</span></span><br><span class="line">    x = x / x_norm</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">6</span>, <span class="number">4</span>]])</span><br><span class="line">print(<span class="string">"normalizeRows(x) = "</span> + str(normalizeRows(x)))</span><br></pre></td></tr></table></figure><pre><code>normalizeRows(x) = [[0.         0.6        0.8       ] [0.26726124 0.80178373 0.53452248]]</code></pre><p><strong>Expected Output</strong>: </p><table style="width:60%"><pre><code>&lt;tr&gt;   &lt;td&gt; **normalizeRows(x)** &lt;/td&gt;   &lt;td&gt; [[ 0.          0.6         0.8       ]</code></pre><p> [ 0.13736056  0.82416338  0.54944226]]</td><br>     </tr></p></table><p><strong>Note</strong>:<br>In normalizeRows(), you can try to print the shapes of x_norm and x, and then rerun the assessment. You’ll find out that they have different shapes. This is normal given that x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column. So how did it work when you divided x by x_norm? This is called broadcasting and we’ll talk about it now! </p><h3 id="1-5-Broadcasting-and-the-softmax-function"><a href="#1-5-Broadcasting-and-the-softmax-function" class="headerlink" title="1.5 - Broadcasting and the softmax function"></a>1.5 - Broadcasting and the softmax function</h3><p>A very important concept to understand in numpy is “broadcasting”. It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official <span class="exturl" data-url="aHR0cHM6Ly9kb2NzLnNjaXB5Lm9yZy9kb2MvbnVtcHkvdXNlci9iYXNpY3MuYnJvYWRjYXN0aW5nLmh0bWw=">broadcasting documentation<i class="fa fa-external-link-alt"></i></span>.</p><p><strong>Exercise</strong>: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.</p><p><strong>Instructions</strong>:</p><ul><li><p>$ \text{for } x \in \mathbb{R}^{1\times n} \<br>\text{,     } softmax(x) =<br>softmax(\begin{bmatrix} \<br>  x_1  &amp;&amp; \<br>  x_2 &amp;&amp; \<br>  …  &amp;&amp; \<br>  x_n  \<br>\end{bmatrix}) = $</p><br>$$  \begin{bmatrix}    \frac{e^{x_1}}{\sum_{j}e^{x_j}}  && \\  \frac{e^{x_2}}{\sum_{j}e^{x_j}}  && \\  ...  && \\  \frac{e^{x_n}}{\sum_{j}e^{x_j}} \\\end{bmatrix} $$ </li><li><p>$\text{for a matrix } x \in \mathbb{R}^{m \times n} \text{,  $x_{ij}$<br>maps to the element in the $i^{th}$ row and $j^{th}$<br>column of $x$, thus we have: }$  $$softmax(x) = softmax\begin{bmatrix}<br>  x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1n} \<br>  x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2n} \<br>  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>  x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots  &amp; x_{mn}<br>\end{bmatrix} = \begin{bmatrix}<br>  \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} &amp; \dots  &amp; \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \<br>  \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} &amp; \dots  &amp; \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \<br>  \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>  \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} &amp; \dots  &amp; \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}<br>\end{bmatrix} = \begin{pmatrix}<br>  softmax\text{(first row of x)}  \<br>  softmax\text{(second row of x)} \<br>  …  \<br>  softmax\text{(last row of x)} \<br>\end{pmatrix} $$</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: softmax</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Calculates the softmax for each row of the input x.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Your code should work for a row vector and also for matrices of shape (n, m).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    x -- A numpy matrix of shape (n,m)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    s -- A numpy matrix equal to the softmax of x, of shape (n,m)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    <span class="comment"># Apply exp() element-wise to x. Use np.exp(...).</span></span><br><span class="line">    x_exp = np.exp(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).</span></span><br><span class="line">    x_sum = np.sum(x_exp, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span></span><br><span class="line">    s = x_exp / x_sum</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span> ,<span class="number">0</span>]])</span><br><span class="line">print(<span class="string">"softmax(x) = "</span> + str(softmax(x)))</span><br></pre></td></tr></table></figure><pre><code>softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04  1.21052389e-04] [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04  8.01252314e-04]]</code></pre><p><strong>Expected Output</strong>:</p><table style="width:60%"><pre><code> &lt;tr&gt;    &lt;td&gt; **softmax(x)** &lt;/td&gt;    &lt;td&gt; [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-041.21052389e-04]</code></pre><p> [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04<br>    8.01252314e-04]]</td><br>     </tr></p></table><p><strong>Note</strong>:</p><ul><li>If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). <strong>x_exp/x_sum</strong> works due to python broadcasting.</li></ul><p>Congratulations! You now have a pretty good understanding of python numpy and have implemented a few useful functions that you will be using in deep learning.</p><font color='blue'>**What you need to remember:**- np.exp(x) works for any np.array x and applies the exponential function to every coordinate- the sigmoid function and its gradient- image2vector is commonly used in deep learning- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. - numpy has efficient built-in functions- broadcasting is extremely useful</font><h2> 2) Vectorization </h2><p>In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">x1 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">x2 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">    dot+= x1[i]*x2[i]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dot = "</span> + str(dot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC OUTER PRODUCT IMPLEMENTATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">outer = np.zeros((len(x1),len(x2))) <span class="comment"># we create a len(x1)*len(x2) matrix with only zeros</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x2)):</span><br><span class="line">        outer[i,j] = x1[i]*x2[j]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"outer = "</span> + str(outer) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC ELEMENTWISE IMPLEMENTATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">mul = np.zeros(len(x1))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">    mul[i] = x1[i]*x2[i]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"elementwise multiplication = "</span> + str(mul) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###</span></span><br><span class="line">W = np.random.rand(<span class="number">3</span>,len(x1)) <span class="comment"># Random 3*len(x1) numpy array</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">gdot = np.zeros(W.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(W.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(x1)):</span><br><span class="line">        gdot[i] += W[i,j]*x1[j]</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"gdot = "</span> + str(gdot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure><pre><code>dot = 278 ----- Computation time = 0.10799999999999699msouter = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.] [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.] [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.] [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.] [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.] [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]] ----- Computation time = 0.26900000000007473mselementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.] ----- Computation time = 0.12699999999998823msgdot = [19.74599905 16.58699591 26.1890302 ] ----- Computation time = 0.19999999999997797ms</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x1 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">x2 = [<span class="number">9</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED DOT PRODUCT OF VECTORS ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = np.dot(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"dot = "</span> + str(dot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED OUTER PRODUCT ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">outer = np.outer(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"outer = "</span> + str(outer) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED ELEMENTWISE MULTIPLICATION ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">mul = np.multiply(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"elementwise multiplication = "</span> + str(mul) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">### VECTORIZED GENERAL DOT PRODUCT ###</span></span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = np.dot(W,x1)</span><br><span class="line">toc = time.process_time()</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"gdot = "</span> + str(dot) + <span class="string">"\n ----- Computation time = "</span> + str(<span class="number">1000</span>*(toc - tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure><pre><code>dot = 278 ----- Computation time = 0.11099999999997223msouter = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0] [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0] [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0] [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0] [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0] [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]] ----- Computation time = 0.2409999999999357mselementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0] ----- Computation time = 0.08300000000005525msgdot = [19.74599905 16.58699591 26.1890302 ] ----- Computation time = 0.1049999999997997ms</code></pre><p>As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger. </p><p><strong>Note</strong> that <code>np.dot()</code> performs a matrix-matrix or matrix-vector multiplication. This is different from <code>np.multiply()</code> and the <code>*</code> operator (which is equivalent to  <code>.*</code> in Matlab/Octave), which performs an element-wise multiplication.</p><h3 id="2-1-Implement-the-L1-and-L2-loss-functions"><a href="#2-1-Implement-the-L1-and-L2-loss-functions" class="headerlink" title="2.1 Implement the L1 and L2 loss functions"></a>2.1 Implement the L1 and L2 loss functions</h3><p><strong>Exercise</strong>: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.</p><p><strong>Reminder</strong>:</p><ul><li>The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.sum(np.abs((y - yhat)))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L1 = "</span> + str(L1(yhat,y)))</span><br></pre></td></tr></table></figure><pre><code>L1 = 1.1</code></pre><p><strong>Expected Output</strong>:</p><table style="width:20%"><pre><code>&lt;tr&gt;   &lt;td&gt; **L1** &lt;/td&gt;   &lt;td&gt; 1.1 &lt;/td&gt; &lt;/tr&gt;</code></pre></table><p><strong>Exercise</strong>: Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, …, x_n]$, then <code>np.dot(x,x)</code> = $\sum_{j=0}^n x_j^{2}$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: L2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.sum(np.square(yhat - y))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yhat = np.array([<span class="number">.9</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">.4</span>, <span class="number">.9</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">print(<span class="string">"L2 = "</span> + str(L2(yhat,y)))</span><br></pre></td></tr></table></figure><pre><code>L2 = 0.43</code></pre><p><strong>Expected Output</strong>: </p><table style="width:20%">     <tr>        <td> **L2** </td>        <td> 0.43 </td>      </tr></table><p>Congratulations on completing this assignment. We hope that this little warm-up exercise helps you in the future assignments, which will be more exciting and interesting!</p><font color='blue'>**What to remember:**- Vectorization is very important in deep learning. It provides computational efficiency and clarity.- You have reviewed the L1 and L2 loss.- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc...</font><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%load_ext version_information</span><br><span class="line">%version_information numpy</span><br></pre></td></tr></table></figure><table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.6 64bit [GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)]</td></tr><tr><td>IPython</td><td>7.0.1</td></tr><tr><td>OS</td><td>Darwin 17.7.0 x86_64 i386 64bit</td></tr><tr><td>numpy</td><td>1.15.1</td></tr><tr><td colspan='2'>Mon Oct 15 09:25:10 2018 MDT</td></tr></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Python Basics With Numpy || Deeplearning.ai(Course - 1 Week - 2)|| Neural Networks and Deep(Week 2)&lt;br&gt;&lt;b&gt;Note - These are my notes on the second course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;br&gt;All Credits to the teacher &lt;b&gt;andrew ng &lt;/b&gt; :)&lt;/p&gt;
    
    </summary>
    
    
      <category term="2020" scheme="https://massivefile.com/categories/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/categories/2020/Assignment/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Assignment" scheme="https://massivefile.com/tags/Assignment/"/>
    
      <category term="Python Basics With Numpy" scheme="https://massivefile.com/tags/Python-Basics-With-Numpy/"/>
    
  </entry>
  
  <entry>
    <title>Quiz 2 || Neural Networks and Deep Learning (Week 2)</title>
    <link href="https://massivefile.com/quiz2/"/>
    <id>https://massivefile.com/quiz2/</id>
    <published>2020-04-07T18:30:00.000Z</published>
    <updated>2020-04-14T19:28:34.435Z</updated>
    
    <content type="html"><![CDATA[<p>title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 2) Neural Networks and Deep Learning (Week 2)<br><b>Note - These are my notes on the first course of DeepLearning.ai by AndrewNg</b></p><a id="more"></a><iframe src="https://drive.google.com/file/d/17-GwSwUnRq8uwaAwZnBqGfpR6BmmCpXf/preview" width="680" height="480"></iframe><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 2) Neural Networks and Deep Learning (Week 2)&lt;br&gt;&lt;b&gt;Note - These are my notes on the first course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="2020" scheme="https://massivefile.com/categories/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/categories/2020/Quiz/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/tags/Quiz/"/>
    
  </entry>
  
  <entry>
    <title>Quiz 1 || Neural Networks and Deep Learning (Week 1)</title>
    <link href="https://massivefile.com/Quiz01/"/>
    <id>https://massivefile.com/Quiz01/</id>
    <published>2020-04-07T00:56:53.000Z</published>
    <updated>2020-04-14T19:28:28.916Z</updated>
    
    <content type="html"><![CDATA[<p>title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 1) Neural Networks and Deep Learning (Week 1)<br><b>Note - These are my notes on the first course of DeepLearning.ai by AndrewNg</b></p><a id="more"></a><iframe src="https://drive.google.com/file/d/1ZznUkyDAfMTwx3V2y63slV9UivBTVdFY/preview" width="680" height="480"></iframe><h5>Credits - <a href='https://www.coursera.org/' target="_blank" rel="noopener">Coursera</a>,  <a href = 'https://en.wikipedia.org/wiki/Andrew_Ng' title="Andrew_Ng">Andrew.ng</a> , <a href ='https://www.coursera.org/specializations/deep-learning?'>Deeplearning.ai Course</a><h5>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;title: Quiz 1|| Deeplearnig.ai (Course - 1 Week - 1) Neural Networks and Deep Learning (Week 1)&lt;br&gt;&lt;b&gt;Note - These are my notes on the first course of DeepLearning.ai by AndrewNg&lt;/b&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="2020" scheme="https://massivefile.com/categories/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/categories/2020/Quiz/"/>
    
    
      <category term="2020" scheme="https://massivefile.com/tags/2020/"/>
    
      <category term="Quiz" scheme="https://massivefile.com/tags/Quiz/"/>
    
  </entry>
  
  <entry>
    <title>Classic Excerpt-Normal Random Variables</title>
    <link href="https://massivefile.com/NormalRandomVariables%20/"/>
    <id>https://massivefile.com/NormalRandomVariables%20/</id>
    <published>2020-04-05T18:30:00.000Z</published>
    <updated>2020-04-12T20:46:43.603Z</updated>
    
    <content type="html"><![CDATA[<p><b>The full text is taken from [Introduction to probability, 2nd Edition]</b><br>3.3 normal random variables</p><a id="more"></a><h2 id="Normal-random-variables"><a href="#Normal-random-variables" class="headerlink" title="Normal random variables"></a>Normal random variables</h2><p>If the probability density of a continuous random variable $ X $ has the following form, then the random variable is called normal or Gaussian.<br>$$<br>f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}<br>$$<br>Among them, $ u $ and $ \ sigma $ are two parameters of the density function, and $ \ sigma $ must also be a positive number. It can be proved that $ f_X (x) $ satisfies the normalization condition of the following probability density function (see the exercises on theorems in this chapter):<br>$$<br>\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx=1<br>$$<br>The figure below is the density function and distribution function of the normal distribution $ (\mu=1 \text{ and } \sigma^2=1) $.</p><center><img src='https://lh3.googleusercontent.com/Eyz-hNy4QsZEFVYqFafTXj8GDlJcZi_B509b8xlUXqU2o2lIhh_-CMyKGzR0Hgt5DETP3fI0jvwQZQpCel8XwX0jDN84KijUeCCfKrg-VcZcSZaw8Lsj8W2215kHIlJJQyow3OALQw=w2400' /> [Figure : [A_normal_PDF_and_CDF_with_u = 1_and_sigmal ^ 2 = 1]</center><br><br> <p>It can be seen from the figure that the probability density function of a normal random variable is a symmetrical bell curve with respect to the mean $ \ mu $. When $ x $ leaves $ \ mu $, the term $ e ^ {\ frac {-(x- \ mu) ^ 2} {2 \ sigma ^ 2}} $ in the expression of the probability density function quickly decline. In the figure, the probability density function is very close to $ 0 $ outside the interval $ [-1,3] $.</p><p>The mean and variance of a normal random variable can be given by the following formula:<br>$$<br>E[X]=\mu,\quad var(X)=\sigma^2<br>$$<br>Since the probability density function of $ X $ is symmetric with respect to $ \ mu $, its mean can only be $ \ mu $. As for the formula of variance, one sentence is defined as:<br>$$<br>var(X)=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}(x-\mu)^2e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx<br>$$<br>Replacing the integral in the formula as an integral variable $ y = \ frac { ( x - \ mu ) } { \ sigma } $ and the distributed integral yields:</p><center><img src='https://lh3.googleusercontent.com/QVoXyim2mP0uEHramWNPMdpBBeEZg0bGZlxv0rYx6lUN-gqlYQbVyabn0YaSTyHSA5MFkjr2oL6wQFtsjubdNslVQjE-wLKb1AAXNjDDxJ8b9MN6krAgNl1DDQSvPDD78_Y8IWRSoA=w2400' /></center><p>The last equation above is due to<br>$$<br>\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{-\frac{y^2}{2}}dy=1<br>$$<br>This formula happens to be the normalization condition of the probability density function of the normal random variable when $ \ mu = 0 $ and $ \ sigma ^ 2 = 1 $. The problem is proved in the problem 14 of this chapter. The screenshot is as follows:</p><center><img src='https://lh3.googleusercontent.com/BF16RgTjIo7M1vinN5aNbhRk-e6IXwSNQVjOOstZteK3fbb4K5RMw5c_Bgvi80qSwtOg6zgKbsmScyC1DnlC19-TPG_ovK67xrY1PhXl0EMFxtrzU94y9LqvzD_xC8jCR-xR5drviw=w2400' />[Figure : the_normal_PDF_satisfies_the_normalization_property]</center><br><br> Normal random variables have several important properties. The following properties are particularly important and will be discussed in Chapter 4This is demonstrated in the first section of on Random Variables.<h3 id="The-normality-of-random-variables-remains-unchanged-under-linear-transformation"><a href="#The-normality-of-random-variables-remains-unchanged-under-linear-transformation" class="headerlink" title="The normality of random variables remains unchanged under linear transformation"></a>The normality of random variables remains unchanged under linear transformation</h3><p>Let $ X $ be a normal random variable, the mean of which is $ \ mu $ and the variance is $ \ sigma ^ 2 $. If $ a \ ne 0 $ and $ b $ are two constants, then the random variable<br>$$<br>Y = aX + b<br>$$<br>It is still a normal random variable, and its mean and variance are given by the following formula:<br>$$<br>E[Y]=a\mu+b,\quad var(Y)=a^2\sigma^2<br>$$</p><h2 id="Standard-normal-random-variables"><a href="#Standard-normal-random-variables" class="headerlink" title="Standard normal random variables"></a>Standard normal random variables</h2><p>Suppose the expectation of the normal random variable $ Y $ is $ 0 $ and the variance is $ 1 $, then $ Y $ is called the standard normal random variable. Let $ \ Phi $ be its CDF:<br>$$<br>\Phi(y)=P(Y\le y)=P(Y&lt; y)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}e^{\frac{-t^2}{2}}dt<br>$$<br>It is usually listed as a table-the standard normal cumulative distribution table (see the table below), which is an important tool for calculating the probability of normal random variables. Each item of the standard normal table provides the value of $ \Phi(y)=P(Y\le y) $, where $ Y $ is a normal random variable, and in this table $ y\in [0,4.09] $. How to use this table? For example, to find the value of $ \Phi(1.71) $, look at the row where $ 1.7 $ is located and the column where $ 0.01 $ is located, and you get $ \Phi(1.71)=0.95637 $.</p><p>Note that the following table only lists the value of $ \Phi  ( y ) $ when  y&gt; 0, the symmetry of the probability density function of the standard normal random variable can be used, and $ \Phi ( y ) $ The value is derived. E.g:</p><center>Φ (−0.5) = P(Y ≤ −0.5) = P(Y ≥ 0.5) = 1−P(Y < 0.5)<br>= 1− Φ (0.5) = 1 − 0.69146 = 0.30854 <br><br> ∀ y> 0 , Φ(−y)= 1 − Φ (y)</center><br><br><table><thead><tr><th align="center">y</th><th>+0.00</th><th>+0.01</th><th>+0.02</th><th>+0.03</th><th>+0.04</th><th>+0.05</th><th>+0.06</th><th>+0.07</th><th>+0.08</th><th>+0.09</th></tr></thead><tbody><tr><td align="center">0.0</td><td>0.50000</td><td>0.50399</td><td>0.50798</td><td>0.51197</td><td>0.51595</td><td>0.51994</td><td>0.52392</td><td>0.52790</td><td>0.53188</td><td>0.53586</td></tr><tr><td align="center">0.1</td><td>0.53983</td><td>0.54380</td><td>0.54776</td><td>0.55172</td><td>0.55567</td><td>0.55966</td><td>0.56360</td><td>0.56749</td><td>0.57142</td><td></td></tr><tr><td align="center">0.2</td><td>0.57926</td><td>0.58317</td><td>0.58706</td><td>0.59095</td><td>0.59483</td><td>0.59871</td><td>0.60257</td><td>0.60642</td><td>0.61026</td><td></td></tr><tr><td align="center">0.3</td><td>0.61791</td><td>0.62172</td><td>0.62552</td><td>0.62930</td><td>0.63307</td><td>0.63683</td><td>0.64058</td><td>0.64431</td><td>0.64803</td><td>0.65173</td></tr><tr><td align="center">0.4</td><td>0.65542</td><td>0.65910</td><td>0.66276</td><td>0.66640</td><td>0.67003</td><td>0.67364</td><td>0.67724</td><td>0.68082</td><td>0.68439</td><td></td></tr><tr><td align="center">0.5</td><td>0.69146</td><td>0.69497</td><td>0.69847</td><td>0.70194</td><td>0.70540</td><td>0.70884</td><td>0.71226</td><td>0.71566</td><td>0.71904</td><td>0.72240</td></tr><tr><td align="center">0.6</td><td>0.72575</td><td>0.72907</td><td>0.73237</td><td>0.73565</td><td>0.73891</td><td>0.74215</td><td>0.74537</td><td>0.74857</td><td>0.75175</td><td></td></tr><tr><td align="center">0.7</td><td>0.75804</td><td>0.76115</td><td>0.76424</td><td>0.76730</td><td>0.77035</td><td>0.77337</td><td>0.77637</td><td>0.77935</td><td>0.78230</td><td>0.78524</td></tr><tr><td align="center">0.8</td><td>0.78814</td><td>0.79103</td><td>0.79389</td><td>0.79673</td><td>0.79955</td><td>0.80234</td><td>0.80511</td><td>0.80785</td><td>0.81057</td><td>0.81327</td></tr><tr><td align="center">0.9</td><td>0.81594</td><td>0.81859</td><td>0.82121</td><td>0.82381</td><td>0.82639</td><td>0.82894</td><td>0.83147</td><td>0.83398</td><td>0.83646</td><td>0.83891</td></tr><tr><td align="center">1.0</td><td>0.84134</td><td>0.84375</td><td>0.84614</td><td>0.84849</td><td>0.85083</td><td>0.85314</td><td>0.85543</td><td>0.85769</td><td>0.85993</td><td></td></tr><tr><td align="center">1.1</td><td>0.86433</td><td>0.86650</td><td>0.86864</td><td>0.87076</td><td>0.87286</td><td>0.87493</td><td>​​0.87698</td><td>0.87900</td><td>0.88100</td><td></td></tr><tr><td align="center">1.2</td><td>0.88493</td><td>​​0.88686</td><td>0.88877</td><td>0.89065</td><td>0.89251</td><td>0.89435</td><td>0.89617</td><td>0.89796</td><td>0.89973</td><td></td></tr><tr><td align="center">1.3</td><td>0.90320</td><td>0.90490</td><td>0.90658</td><td>0.90824</td><td>0.90988</td><td>0.91149</td><td>0.91308</td><td>0.91466</td><td>0.91621</td><td>0.91774</td></tr><tr><td align="center">1.4</td><td>0.91924</td><td>0.92073</td><td>0.92220</td><td>0.92364</td><td>0.92507</td><td>0.92647</td><td>0.92785</td><td>0.92922</td><td>0.93056</td><td>0.93189</td></tr><tr><td align="center">1.5</td><td>0.93319</td><td>0.93448</td><td>0.93574</td><td>0.93699</td><td>0.93822</td><td>0.93943</td><td>0.94062</td><td>0.94179</td><td>0.94295</td><td>0.94408</td></tr><tr><td align="center">1.6</td><td>0.94520</td><td>0.94630</td><td>0.94738</td><td>0.94845</td><td>0.94950</td><td>0.95053</td><td>0.95154</td><td>0.95254</td><td>0.95352</td><td>0.95449</td></tr><tr><td align="center">1.7</td><td>0.95543</td><td>0.95637</td><td>0.95728</td><td>0.95818</td><td>0.95907</td><td>0.95994</td><td>0.96080</td><td>0.96164</td><td>0.96246</td><td>0.96327</td></tr><tr><td align="center">1.8</td><td>0.96407</td><td>0.96485</td><td>0.96562</td><td>0.96638</td><td>0.96712</td><td>0.96784</td><td>0.96856</td><td>0.96926</td><td>0.96995</td><td>0.97062</td></tr><tr><td align="center">0.99128</td><td>0.97193</td><td>0.97257</td><td>0.97320</td><td>0.97381</td><td>0.97441</td><td>0.97500</td><td>0.97558</td><td>0.97615</td><td>0.97670</td><td></td></tr><tr><td align="center">2.0</td><td>0.97725</td><td>0.97778</td><td>0.97831</td><td>0.97882</td><td>0.97932</td><td>0.97982</td><td>0.98030</td><td>0.98077</td><td>0.98124</td><td>0.98169</td></tr><tr><td align="center">2.1</td><td>0.98214</td><td>0.98257</td><td>0.98300</td><td>0.98341</td><td>0.98382</td><td>0.98422</td><td>0.98461</td><td>0.98500</td><td>0.98537</td><td></td></tr><tr><td align="center">2.2</td><td>0.98610</td><td>0.98645</td><td>0.98679</td><td>0.98713</td><td>0.98745</td><td>0.98778</td><td>0.98809</td><td>0.98840</td><td>0.98870</td><td></td></tr><tr><td align="center">2.3</td><td>0.98928</td><td>0.98956</td><td>0.98983</td><td>0.99010</td><td>0.99036</td><td>0.99061</td><td>0.99086</td><td>0.99111</td><td>0.99134</td><td>0.99158</td></tr><tr><td align="center">2.4</td><td>0.99180</td><td>0.99202</td><td>0.99224</td><td>0.99245</td><td>0.99266</td><td>0.99286</td><td>0.99305</td><td>0.99324</td><td>0.99343</td><td>0.99361</td></tr><tr><td align="center">2.5</td><td>0.99379</td><td>0.99396</td><td>0.99413</td><td>0.99430</td><td>0.99446</td><td>0.99461</td><td>0.99477</td><td>0.99492</td><td>0.99506</td><td>0.99520</td></tr><tr><td align="center">2.6</td><td>0.99534</td><td>0.99547</td><td>0.99560</td><td>0.99573</td><td>0.99585</td><td>0.99598</td><td>0.99609</td><td>0.99621</td><td>0.99632</td><td>0.99643</td></tr><tr><td align="center">2.7</td><td>0.99653</td><td>0.99664</td><td>0.99674</td><td>0.99683</td><td>0.99693</td><td>0.99702</td><td>0.99711</td><td>0.99720</td><td>0.99728</td><td>0.99736</td></tr><tr><td align="center">2.8</td><td>0.99744</td><td>0.99752</td><td>0.99760</td><td>0.99767</td><td>0.99774</td><td>0.99781</td><td>0.99788</td><td>0.99795</td><td>0.99801</td><td>0.99807</td></tr><tr><td align="center">2.9</td><td>0.99813</td><td>0.99819</td><td>0.99825</td><td>0.99831</td><td>0.99836</td><td>0.99841</td><td>0.99846</td><td>0.99851</td><td>0.99856</td><td>0.99861</td></tr><tr><td align="center">3.0</td><td>0.99865</td><td>0.99869</td><td>0.99874</td><td>0.99878</td><td>0.99882</td><td>0.99886</td><td>0.99889</td><td>0.99893</td><td>0.99896</td><td>0.99900</td></tr><tr><td align="center">3.1</td><td>0.99903</td><td>0.99906</td><td>0.99910</td><td>0.99913</td><td>0.99916</td><td>0.99918</td><td>0.99921</td><td>0.99924</td><td>0.99926</td><td>0.99929</td></tr><tr><td align="center">3.2</td><td>0.99931</td><td>0.99934</td><td>0.99936</td><td>0.99938</td><td>0.99940</td><td>0.99942</td><td>0.99944</td><td>0.99946</td><td>0.99948</td><td>0.99950</td></tr><tr><td align="center">3.3</td><td>0.99952</td><td>0.99953</td><td>0.99955</td><td>0.99957</td><td>0.99958</td><td>0.99960</td><td>0.99961</td><td>0.99962</td><td>0.99964</td><td>0.99965</td></tr><tr><td align="center">3.4</td><td>0.99966</td><td>0.99968</td><td>0.99969</td><td>0.99970</td><td>0.99971</td><td>0.99972</td><td>0.99973</td><td>0.99974</td><td>0.99975</td><td>0.99976</td></tr><tr><td align="center">3.5</td><td>0.99977</td><td>0.99978</td><td>0.99978</td><td>0.99979</td><td>0.99980</td><td>0.99981</td><td>0.99981</td><td>0.99982</td><td>0.99983</td><td>0.99983</td></tr><tr><td align="center">3.6</td><td>0.99984</td><td>0.99985</td><td>0.99985</td><td>0.99986</td><td>0.99986</td><td>0.99987</td><td>0.99987</td><td>0.99988</td><td>0.99988</td><td>0.99989</td></tr><tr><td align="center">3.7</td><td>0.99989</td><td>0.99990</td><td>0.99990</td><td>0.99990</td><td>0.99991</td><td>0.99991</td><td>0.99992</td><td>0.99992</td><td>0.99992</td><td>0.99992</td></tr><tr><td align="center">3.8</td><td>0.99993</td><td>0.99993</td><td>0.99993</td><td>0.99994</td><td>0.99994</td><td>0.99994</td><td>0.99994</td><td>0.99995</td><td>0.99995</td><td>0.99995</td></tr><tr><td align="center">3.9</td><td>0.99995</td><td>0.99995</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99996</td><td>0.99997</td><td>0.99997</td></tr><tr><td align="center">4.0</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99997</td><td>0.99998</td><td>0.99998</td><td>0.99998</td><td>0.99998</td></tr></tbody></table><p>Now use $ X $ to represent a normal random variable with a mean of $ \ mu $ and a variance of $ \ sigma ^ 2 $. Standardize $ X $ (“standardize”) by defining a new random variable $ Y $:<br>$$<br>Y=\frac{X-\mu}{\sigma}<br>$$<br>Because $ Y $ is a linear function of $ X $, $ Y $ is also a normal random variable. and<br>$$<br>E[Y]=\frac{E[X]-u}{\sigma}=0,\quad var(Y)=\frac{var(X)}{\sigma^2}=1<br>$$<br>Therefore, $ Y $ is a standard normal random variable. This fact allows us to redefine the event represented by $ X $ with $ Y $, and then use the standard normal table to calculate.</p><h4 id="Example-of-using-normal-distribution-table"><a href="#Example-of-using-normal-distribution-table" class="headerlink" title="Example of using normal distribution table"></a>Example of using normal distribution table</h4><p>The annual snowfall in a certain area is a normal random variable, the expectation is $ \ mu = 60 $ inches, and the standard deviation is $ \ sigma = 20 $. What is the probability that the snowfall will be at least $ 80 $ inches this year?</p><p>Let $ X $ be the annual snowfall, so that<br>$$<br>Y=\frac{X-\mu}{\sigma}=\frac{X-60}{20}<br>$$<br>Obviously $ Y $ is a standard normal random variable.<br>$$<br>P(X\ge 80)=P(\frac{X-60}{20} \ge \frac{80-60}{20})=P(Y\ge \frac{80-60}{20})=P(Y\ge 1)=1-\Phi(1)<br>$$<br>Where $ \ Phi $ is the standard normal cumulative distribution function. Obtained by querying the above table: $ \Phi(1)=0.84134 $, so<br>$$<br>P(X\ge 80)=1-\Phi(1)=0.15866<br>$$<br>Promoting the method in this example, we get the following:</p><h3 id="Calculation-of-the-cumulative-distribution-function-of-normal-random-variables"><a href="#Calculation-of-the-cumulative-distribution-function-of-normal-random-variables" class="headerlink" title="Calculation of the cumulative distribution function of normal random variables"></a>Calculation of the cumulative distribution function of normal random variables</h3><p>For a normal random variable $ X $ with a mean of $ \ mu $ and a variance of $ \ sigma ^ 2 $, use the following steps:</p><ol><li><p>Normalized $ X $: First subtract $ \ mu $ and then divide by $ \ sigma $ to obtain the standard random variable $ Y $.</p></li><li><p>Read the cumulative distribution function value from the standard normal table:<br> $$<br> P(X\le x)=P(\frac{X-\mu}{\sigma}\le \frac{x-\mu}{\sigma})=P(Y\le \frac{x-\mu}{\sigma})=\Phi(\frac{x-\mu}{\sigma})<br> $$<br> Normal random variables are often used in signal processing and communication engineering to model noise and signal distortion.</p></li></ol><h4 id="Example-3-8-Signal-detection"><a href="#Example-3-8-Signal-detection" class="headerlink" title="Example 3.8 Signal detection"></a>Example 3.8 Signal detection</h4><p>Binary information is transmitted with the signal $ s $. This information is either $ -1 $ and $ + 1 $. The signal will be accompanied by some noise during the channel transmission. The noise satisfies the normal distribution with a mean value of $ \ mu = 0 $ and a variance of $ \ sigma ^ 2 $. The receiver will receive a signal mixed with noise, if the received value is less than $ 0 $, then the signal is considered to be $ -1 $, if the received value is greater than $ 0 $, then the received signal is considered to be $ + 1 $. How big is the error of this judgment method?</p><p>The error only appears in the following two cases:</p><ol><li>The actual transmitted signal is $ -1 $, but the value of the noise variable $ N $ is at least $ 1 $, so $ s + N = -1 + N \ ge 0 $. </li><li>The actual transmitted signal is $ + 1 $, but the value of the noise variable $ N $ is less than $ -1 $. Therefore $ s + N = 1 + N &lt;0 $.<center><img src='https://lh3.googleusercontent.com/26H7cHnyGWMkXDDwy3YHMmzla_X3yZAsbrmZnMNhZCVDD9qWInCvmgFj87feKbHyctBzaaSS8JI7_krDHoWXoTdhZAHqkdYB1NS_c_zrmznaASmeeu-773VILwZYUyTP4rFB6CYWUw=w2400' />[Figure_3.11_The_signal_detection]</center> <br><br></li></ol><p>Therefore, the probability of error in this judgment method in case 1 is:</p><p>P ( N ≥ 1 ) = 1 − P ( N &lt; 1 ) = 1 − P  ( N &lt; 1 ) = 1 − P ( N − Μ / σ &lt; 1 − μ ) / σ )</p><p>= 1 - Φ( 1 − μ ) / σ)</p><p>= 1 - Φ( 1 / σ)</p><p>The probability of an error in the second case is obtained according to the symmetry of the normal distribution as in the previous case. $ \Phi (\frac {1} {\sigma}) $ can be obtained from the normal distribution table. For example, for $ \sigma = 1 $, $ \Phi (\frac {1} {\ sigma}) = \Phi (1) = 0.84134 $, so the probability of error is $ 0.15864 $.</p><p>Normal random variables play an important role in a wide range of probabilistic models. The reason is that normal random variables can well simulate the superposition effect of many independent factors in physics, engineering and statistics. Mathematically, the key fact is that the distribution of the sum of a large number of independent and identically distributed random variables (not necessarily normal) obey the normal distribution, and this fact has nothing to do with the specific distribution of each sum. This fact is the famous central limit theorem, which will be explained in detail in Chapter 5 of this book.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;b&gt;The full text is taken from [Introduction to probability, 2nd Edition]&lt;/b&gt;&lt;br&gt;3.3 normal random variables&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning Maths" scheme="https://massivefile.com/categories/Machine-Learning-Maths/"/>
    
    
      <category term="probability" scheme="https://massivefile.com/tags/probability/"/>
    
  </entry>
  
  <entry>
    <title>Classic Vim without plugins</title>
    <link href="https://massivefile.com/Vim_no_plugins/"/>
    <id>https://massivefile.com/Vim_no_plugins/</id>
    <published>2020-04-05T14:46:00.000Z</published>
    <updated>2020-04-13T10:17:40.907Z</updated>
    
    <content type="html"><![CDATA[<p>This article Covers everything related to be going with Vim for any beginner or intermediate.<br>I have explained the ins and outs of Vim in briefly.<br>This article will be continuously updated based on experience.</p><a id="more"></a> <h2 id="Four-modes-of-vim"><a href="#Four-modes-of-vim" class="headerlink" title="Four modes of vim"></a>Four modes of vim</h2><ol><li><p>General mode: normal mode. You can move the cursor, delete characters or entire lines, and copy and paste file data. Opening vim is to enter this mode, and the switching between the three modes is also transferred here.</p></li><li><p>Edit mode: In general mode, press any one of <code>i``I``o`` O</code> <code>a`` A</code> <code>r`` R</code> <code>s`` S</code> to enter this mode. You can edit the content of the file and press Esc to return to the normal mode.</p><p> -<code>i`` I</code> is insert (in front of the character under the cursor and at the beginning of the line)<br> -<code>o`` O</code> is open new line (start a new line below the line where the cursor is and start a new line above the line where the cursor is<br> -<code>a`` A</code> is append (after the character under the cursor and at the end of the line where the cursor is)<br> -<code>s`` S</code> is to delete (the character where the cursor is and start inserting and the line where the cursor is and start inserting), that is, substitute replacement.<br> -<code>r`` R</code> is to replace the character under the cursor and become the replacement mode</p></li><li><p>Command line mode: Press <code>:</code> <code>/</code> <code>in normal mode?</code>Any one enters this mode (the meaning of these symbols will be described below). You can find data operations, read, save, replace a lot of characters, leave vim, display line number and other operations, press Esc to return to the general mode.</p></li><li><p>Visual mode: In general mode, press <code>v`` V</code> <code>ctr + v</code> to enter the visual mode, which is equivalent to the normal mode after highlighting the selected text, that is, in this mode, you can arbitrarily select a specific area and be The selected area is highlighted, <code>v</code> selects the unit: one character;<code>V</code> is also called the visible line mode, select the unit: line; <code>ctr + v</code> is also called the visible block mode, select the unit: square ; All three are useful, see below for details.</p></li></ol><h2 id="Mobile"><a href="#Mobile" class="headerlink" title="Mobile"></a>Mobile</h2><p>In normal mode:</p><p><code>w</code> → to the beginning of the next word<code>e</code> → to the end of the next word (words are separated by spaces by default)<br><code>W</code> → to the beginning of the next string<code>E</code> → to the end of the next string (a string refers to a string consisting of numbers, letters, and underscores)<br><code>B</code> → Go to the first character of the previous string. The<code>b</code> → “command moves the cursor to the first character of the previous word.</p><blockquote><p>By default, a word is composed of letters, numbers and underscores<br>If you think words are separated by blank characters, then you need to use uppercase E and W (Chen Hao: Note)</p></blockquote><p><code>0</code> → number zero, to the beginning of the line<br><code>^</code> → Go to the first position of the line that is not a blank character (the so-called blank character is a space, tab, line feed, carriage return, etc.)<br><code>$</code> → Go to the end of the line<br><code>g_</code> → to the last position of the line that is not a blank character<br><code>%</code> → Go to the other of the pair of brackets where the cursor is<br><code>gg</code> → first line<br><code>G</code> → last line<br><code>h`` j</code> <code>k`` l</code> (strongly recommended to use it to move the cursor, but it is not necessary) → you can also use the cursor keys (← ↓ ↑ →). Note: j extends downwards, k extends upward</p><ol><li>** High frequency usage scenario 1 **: Change a variable name in the line first and move the cursor: <code>w</code> and<code>b</code>, <code>W</code> and<code>B</code> (or if the line is too long, use the following Search function) to the target word</li><li>** High frequency usage scenario 2 **: Modify the indent and jump to the beginning of the line<code>^</code></li><li>** High frequency usage scenario 3 **: View the completeness of the function or class or the variable scope <code>%</code></li><li>** High frequency usage scenario 4 **: After splitting the screen, jump to different windows: <code>ctrl + w + (h or j or k or l)</code></li><li>** High-frequency use scene 5 **: Move left (left, top, right) <code>(h, j, k, l)</code></li><li>** High frequency usage scenario 6 **: Delete to the end: <code>d $</code> Delete to the beginning: <code>d ^</code></li></ol><h3 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h3><p>Note: ** mark is to find better **, in normal mode:</p><p><code>mx</code> meaning: mark x, x is the name of mark;<br><code>&#39;x</code> meaning: go to the position of x mark</p><ol><li>** High-frequency usage scenario 1: ** You can see how to define other functions in the function. You want to see how to define them. After you read it, you need to come back. Then mark it first and then jump back.</li></ol><h3 id="Syntax-related-jumps"><a href="#Syntax-related-jumps" class="headerlink" title="Syntax related jumps"></a>Syntax related jumps</h3><p>In normal mode:</p><ol><li><code>gd</code> meaning: go to definition</li><li>Press <code>[</code> and then <code>ctrl + d</code> to jump to #define </li><li>Press <code>[</code> and then <code>ctrl + i</code> to jump to functions, variables and #define </li></ol><p>** Note **: The language support is not very good, you can try the language used</p><h3 id="Quick-page-turning"><a href="#Quick-page-turning" class="headerlink" title="Quick page turning"></a>Quick page turning</h3><p>In normal mode:</p><table><thead><tr><th>Partner 1</th><th>Partner 2</th></tr></thead><tbody><tr><td><code>ctr + d</code> page down</td><td><code>ctr + d</code> page up</td></tr><tr><td><code>ctr + f</code> page forward</td><td><code>ctr + b</code> page back</td></tr></tbody></table><h2 id="Action-operation-instruction"><a href="#Action-operation-instruction" class="headerlink" title="Action operation instruction"></a>Action operation instruction</h2><p>In normal mode:</p><table><thead><tr><th>Partner 1</th><th>Partner 2</th></tr></thead><tbody><tr><td><code>d</code> ** d ** elete a character and copy to clipboard</td><td><code>D</code> has been ** deleted ** from the cursor position to the end of the line</td></tr><tr><td><code>y</code> ** c ** opy to clipboard</td><td><code>Y</code> ** Copy ** one line (= <code>yy</code>)</td></tr><tr><td><code>s</code> ** s ** ubstitue a character</td><td><code>S</code> ** Replace ** the line where the cursor is located</td></tr><tr><td><code>r</code> ** r ** eplace a character</td><td><code>R</code>* Not commonly used *, which means to enter replacement mode</td></tr><tr><td><code>c</code> ** c ** hange a character</td><td><code>C</code> * Not commonly used <em>, which means *</em> modify ** the cursor position until the end of the line, the same effect as <code>S</code> rendering</td></tr><tr><td><code>p</code> ** p ** aste after the cursor</td><td><code>P</code> ** Paste ** before the cursor position (if you paste a whole line, paste to the previous line)</td></tr><tr><td><code>u</code> ** u ** ndo a operation</td><td><code>U</code> one-time ** undo ** all operations on a whole line</td></tr><tr><td><code>x</code> cut a character</td><td><code>X</code> * not commonly used <em>, *</em> cut ** to the left, ie backspace: delete the character to the left of the cursor</td></tr><tr><td>** `** Search the word under the current cursor ** downward, and jump to the next word when found</td><td><code>#</code>** Search the word under the current cursor ** upward, jump to the previous word when found</td></tr><tr><td><code>/ word</code> ** Search the word word ** down to the full text, jump to the first word that matches, if there are multiple, continue to search down and press the n key (in the direction of the original command), up to press the N key.</td><td><code>? word</code> ** Search the word word ** up the full text, jump to the first word that matches, and if there are multiple, continue to search upwards and press the n key (in the direction of the original command), down to press the N key.</td></tr><tr><td><code>a</code> ** a ** ppend after the cursor</td><td><code>A</code> is ** append ** at the end of the line where the cursor is located)</td></tr><tr><td><code>i</code> ** i ** nsert before the cursor</td><td><code>I</code> ** insert ** at the beginning of the line where the cursor is located</td></tr><tr><td><code>o</code> Start a new line below the line under the cursor, open the new world?</td><td><code>O</code> starts a new line above the line where the cursor is located</td></tr><tr><td><code>v</code> enters ** v ** isual mode, used to select areas (can cross lines), used to cooperate with other subsequent operations (addition, deletion, and modification)</td><td><code>v</code> enters visual line mode, used to select some lines To cooperate with other follow-up operations (addition, deletion and modification)</td></tr><tr><td><code>f</code> ** f ** ind a character after the cursor</td><td><code>F</code> ** find a character before the cursor position **</td></tr><tr><td><code>t</code> ** t ** ill a character tx is the same as fx, the difference is to jump to the front of character x</td><td><code>T</code> Tx is the same as Fx, the difference is to jump to the character x</td></tr></tbody></table><h3 id="Formed-separately"><a href="#Formed-separately" class="headerlink" title="Formed separately"></a>Formed separately</h3><p><code>.</code> Repeat the operation just now<br><code>~</code> Convert case</p><ol><li>You can change the case of the first letter of the variable</li><li>You can select a string (variable) in combination with the commands provided below, and then change the case of the entire string (variable). For example: macro definition</li></ol><p><code>=</code> Auto format</p><ol><li>Use <code>==</code> for the current line (double press = twice), or use <code>n ==</code> for multiple lines (n is a natural number) to automatically indent the next n lines from the current line</li><li>Or enter the visual line mode, select some lines and then <code>=</code> for formatting, which is equivalent to the code format in the general IDE.</li><li>Use <code>gg = G</code> to typeset the entire code.</li></ol><h3 id="Undo-and-Redo"><a href="#Undo-and-Redo" class="headerlink" title="Undo and Redo"></a>Undo and Redo</h3><ol><li><code>u</code> undo undoes the operation in the previous step. Commands can be combined. For example,<code>Nu</code> N is any integer, which means undoing the N-step operation. The following is the same.</li><li><code>U</code> restore the current line (that is, undo all operations on the current line at once)</li><li><code>ctr + r</code> control + redo restore the previous operation that was cancelled</li><li><code>CTRL-R</code> back to the previous command</li></ol><h3 id="Text-replacement"><a href="#Text-replacement" class="headerlink" title="Text replacement"></a>Text replacement</h3><p>Enter the replacement command in normal mode: <code>: [range] s / pattern / string / [flags]</code></p><p>-pattern is the string to be replaced, which can be expressed by regexp.<br>-string replaces pattern with string.<br>-[range] has the following values:</p><table><thead><tr><th>[range]</th><th>Meaning</th></tr></thead><tbody><tr><td>None</td><td>The default is the line where the cursor is located</td></tr><tr><td><code>.</code></td><td>Current line of cursor</td></tr><tr><td><code>N</code></td><td>Line N</td></tr><tr><td><code>$</code></td><td>Last line</td></tr><tr><td><code>&#39;a</code></td><td>Mark the line where a (marked with ma before)</td></tr><tr><td><code>. + 1</code></td><td>The line below the current cursor line</td></tr><tr><td><code>$ -1</code></td><td>The penultimate line, you can add or subtract a value to determine the relative line</td></tr><tr><td><code>22,33</code></td><td>Lines 22 to 33</td></tr><tr><td><code>1, $</code></td><td>Line 1 to last line</td></tr><tr><td><code>1, .</code></td><td>line 1 to current line</td></tr><tr><td><code>., $</code></td><td>Current line to last line</td></tr><tr><td><code>&#39;a,&#39; b</code></td><td>The line from the mark a to the line from the mark b (Ma and mb have been used to mark before)</td></tr><tr><td><code>%</code></td><td>All rows (equivalent to 1, $)</td></tr><tr><td><code>? str?</code></td><td>Search up from the current position to find the line of the first str (where str can be any string or regular expression)</td></tr><tr><td><code>/ str /</code></td><td>Search down from the current position to find the line of the first str (where str can be any string or regular expression)</td></tr><tr><td>** Note that all the above methods for range can be used to set the relative offset by + and-operations. **</td><td></td></tr></tbody></table><p>-[flags] has the following values:</p><table><thead><tr><th>flags</th><th>meaning</th></tr></thead><tbody><tr><td><code>g</code></td><td>Replace all matches (global) within the specified range</td></tr><tr><td><code>c</code></td><td>Request user confirmation before replacement</td></tr><tr><td><code>e</code></td><td>Ignore errors during execution</td></tr><tr><td><code>i</code></td><td>ignore case-insensitive</td></tr><tr><td>None</td><td>Only the first match in the specified range is replaced</td></tr></tbody></table><p>** Note: All the above flags can be used in combination. For example, gc means to replace all matching items in the specified range, and the user will be asked to confirm before each replacement. **</p><h4 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h4><p>Replace some lines</p><ol><li><code>: 10,20s / from / to / g</code> replaces lines 10 to 20.</li><li><code>: 1, $ s / from / to / g</code> replaces the contents of the first line to the last line (that is, all text)</li><li><code>: 1, .s / from / to / g</code> replaces the content from the first line to the current line.</li><li><code>:., $ S / from / to / g</code> replaces the content from the current line to the last line.</li><li><code>: &#39;a,&#39; bs / from / to / g</code> replaces the line between the marks a and b (including the line where a and b are located), where a and b are the marks previously made with the m command .</li></ol><p>Replace all lines: <code>:% s / from / to / g</code></p><h2 id="Repeat-the-action"><a href="#Repeat-the-action" class="headerlink" title="Repeat the action"></a>Repeat the action</h2><p>In normal mode, any action can be repeated</p><p>Note: N is a number</p><p>-Numbers: <code>Nyy</code> copies N lines down from the current line,<code>Ndd</code> deletes N lines down from the current line, <code>Ngg</code> jumps to the Nth line, and<code>dNw</code> deletes from the current cursor to the first Before N words (does not contain blanks, ie delete N-1 words), <code>yNe</code> copies from the current cursor to the end of the Nth word (note:<code>yy</code> = <code>1yy`` dd</code> = <code>1dd</code>) <code>d $</code> delete to the end of the line<br>-Repeat the previous command: <code>N.</code> (N indicates the number of repetitions)</p><h2 id="Block-selection"><a href="#Block-selection" class="headerlink" title="Block selection"></a>Block selection</h2><p>Note: The brackets are optional</p><p>In normal mode: <code>[ctr +] v + (h or j or k or l)</code></p><ol><li>** High-frequency use scenario 1 *<em>: <code>[ctr +] v</code> Select some line headers and then press<code>=</code>Effect: *</em> Code format is automatically adjusted **</li><li>** High frequency usage scenario 2 *<em>: <code>[ctr +] v</code> After selecting the head of some lines, press<code>I</code>, then press the comment symbol (eg: <code>//</code>) and finally press <code>ESC</code> : All the selected lines are commented *</em> Multi-line quick comment **</li><li>** High frequency usage scenario 3 *<em>: <code>[ctr +] v</code> After selecting some line headers, then press<code>A</code>, then press the contents of the comment and finally press <code>ESC</code> (for example:<code>// This is a test Code</code>) Effect: All the lines at the end of the selected lines are commented on.//This is the test code` *</em> Multi-line quick comment **</li><li>** High-frequency use scenario 4 *<em>: <code>[ctr +] v</code> select some line header comments (for example:<code>//</code>), then press<code>d</code> and finally press <code>ESC</code> effect: selected All these lines are commented out. *</em> Multi-line quick delete comments ** </li><li>** High-frequency use scenario 5 **: After selecting certain blocks, press [ctr +] v` and then press the above-mentioned action buttons to achieve regional operation</li></ol><h2 id="Powerful-combination"><a href="#Powerful-combination" class="headerlink" title="Powerful combination"></a>Powerful combination</h2><h3 id="Operate-a-word-under-the-cursor"><a href="#Operate-a-word-under-the-cursor" class="headerlink" title="Operate a word under the cursor"></a>Operate a word under the cursor</h3><p>In normal mode:</p><p>Action + Move [+ Number of Repeats]<br>The combination has already been used a lot in the past, continue here:</p><table><thead><tr><th>Action Operation Command + Range</th><th>Effect</th></tr></thead><tbody><tr><td>cw or c1 or c1w</td><td>change from current cursor to word end</td></tr><tr><td>caw</td><td>change whole word including current cursor</td></tr><tr><td>dw or d1 or d1w</td><td>delete from current cursor to word end</td></tr><tr><td>daw</td><td>delete whole word including current cursor</td></tr><tr><td>yw or y1 or y1w</td><td>copy from current cursor to word end</td></tr><tr><td>yaw</td><td>copy whole word including current cursor</td></tr><tr><td>d / word</td><td>delete forward until the former character of the next ‘word’</td></tr><tr><td>d? word</td><td>delete backward until the former character of the last ‘word’</td></tr></tbody></table><table><thead><tr><th>Action Operation Command + Range</th><th>Effect</th></tr></thead><tbody><tr><td>dtc</td><td>delete until before the next ‘c’</td></tr><tr><td>dfc</td><td>delete until after the next ‘c’</td></tr><tr><td>Scope + Action Operation Instructions</td><td>Effects</td></tr><tr><td>———————</td><td>———-</td></tr><tr><td><code>bve</code> or<code>BvE</code> + c / d / y</td><td>manipulate a variable or string</td></tr></tbody></table><p>The above table are high-frequency use scenarios</p><h2 id="Autocomplete"><a href="#Autocomplete" class="headerlink" title="Autocomplete"></a>Autocomplete</h2><p>Press directly in insert mode: the most commonly used completion</p><p><code>`ctrl + n  ctrl + p</code> `</p><p>Smart completion</p><p><code>`ctrl + x // Enter completion mode</code> `</p><p>-Complete line completion <code>CTRL-X`` CTRL-L</code><br>-** Complete based on keywords in the current file ** <code>CTRL-X`` CTRL-N</code><br>-** Complete according to dictionary ** <code>CTRL-X`` CTRL-K</code><br>-Completion of <code>CTRL-X`` CTRL-T</code> according to thesaurus<br>-** Complete according to keywords in header file ** <code>CTRL-X`` CTRL-I</code><br>-Completion of <code>CTRL-X`` CTRL-]</code>according to tags<br>-** Complete file name ** <code>CTRL-X`` CTRL-F</code><br>-Completion of macro definition <code>CTRL-X`` CTRL-D</code><br>-Completion of vim command <code>CTRL-X`` CTRL-V</code><br>-User-defined completion method <code>CTRL-X`` CTRL-U</code><br>-** Spelling suggestions ** <code>CTRL-X`` CTRL-S</code> // For example: an English word</p><h2 id="Collapse"><a href="#Collapse" class="headerlink" title="Collapse"></a>Collapse</h2><p>In normal mode:</p><p><code>`zo (fold + open)zi (fold + indent)zc (fold + close)</code> `</p><h2 id="Split-screen"><a href="#Split-screen" class="headerlink" title="Split screen"></a>Split screen</h2><p>** Split command **, in normal mode, enter</p><ol><li><code>vs</code> (Description: Vertically split the screen vertically)</li><li><code>sp</code> (Note: split horizontally splits the screen, which is the default splitting method)</li></ol><p>** Screens jump to each other **</p><ol><li><code>ctr + w</code> and then press<code>h</code> or <code>j</code> or<code>k</code> or <code>l</code></li><li>Explanation: <code>h</code>: left,<code>j</code>: down, <code>k</code>: up,<code>l</code>: right</li></ol><p>** Resize the split window **</p><ol><li><code>ctrl + w</code> before pressing<code>+</code>or<code>-</code> or <code>=</code>, of course press a number before pressing <code>+</code> or <code>-</code> or<code>=</code>to change the window height,<code>=</code>is equal The meaning of points. .</li><li>In normal mode, enter <code>: resize -N</code> or<code>: resize + N</code> to explicitly specify that the window is reduced or increased by N lines</li><li><code>ctrl + w</code> before pressing<code>&lt;</code>or<code>&gt;</code>or<code>=</code>, of course press a number before pressing<code>&lt;</code>or<code>&gt;</code>or<code>=</code>to change the window width,<code>=</code>is equal The meaning of points.</li><li>Sometimes preview large files and feel that the split screen is too small, <code>ctrl + w</code> +<code>T</code> moves the current window to a new tab.</li></ol><h2 id="tab-window"><a href="#tab-window" class="headerlink" title="tab window"></a>tab window</h2><p>Vim has added the function of multi-tab switching since vim7, which is equivalent to multiple windows. Although the previous version also has a multi-file editing function, it is not as convenient as this. Usage in normal mode:</p><p>-<code>: tabnew</code> [++ opt option] [＋ cmd] The file creates a new tab for the specified file<br>-<code>: tabc</code> closes the current tab or<code>: q</code><br>-<code>: tabo</code> close ** other ** tab<br>-<code>: tabs</code> view ** all open ** tabs<br>-<code>: tabp</code> previous tab window<br>-<code>: tabn</code> next tab window</p><p> <code>gt</code>,<code>gT</code> can switch directly between tabs. There are many other commands,: help table.</p><h2 id="table-of-Contents"><a href="#table-of-Contents" class="headerlink" title="table of Contents"></a>table of Contents</h2><p>In normal mode:</p><ol><li><code>: Te</code> displays the current directory in the form of a tab window, then you can switch directories and open a file</li><li><code>:! Ls</code> This is how vim invokes shell commands<code>:! Ls + shell_command</code>, but it does not display the current directory in the form of a tab window.</li></ol><h2 id="Content-operation-of-paired-symbols"><a href="#Content-operation-of-paired-symbols" class="headerlink" title="Content operation of paired symbols"></a>Content operation of paired symbols</h2><p>The following commands can operate on the content within punctuation:</p><ol><li><code>ci&#39;`` ci &quot;</code> <code>ci (</code> <code>ci [</code> <code>ci {</code> <code>ci &lt;</code> respectively change the text content of these paired punctuation marks </li><li><code>di&#39;``di&quot;</code>di (<code>or</code>dib<code>di [</code> di {<code>or</code> diB<code></code>di &lt;` delete the text content of these paired punctuation marks respectively </li><li><code>yi&#39;`` yi &quot;</code> <code>yi (</code> <code>yi [</code> <code>yi {</code> <code>yi &lt;</code> separately copy the text content of these paired punctuation marks </li><li><code>vi&#39;`` vi &quot;</code> <code>vi (</code> <code>vi [</code> <code>vi {</code> <code>vi &lt;</code> respectively select the text content of these paired punctuation marks</li><li><code>cit`` dit</code> <code>yit`` vit</code> separately operate the content between a pair of tags, editing html is very useful</li></ol><p>** In addition, if you change the above <code>i</code> to<code>a</code>, you can operate the paired punctuation and the content of the paired punctuation at the same time **, for example:</p><p>For example, the text to be operated: 111 “222” 333, move the cursor to any character of “222” and enter the command </p><p>-di “, the text will become: 111” “333<br>-If you enter the command da “, the text will become: 111333</p><h2 id="Clipboard"><a href="#Clipboard" class="headerlink" title="Clipboard"></a>Clipboard</h2><h3 id="1-Simple-copy-and-paste"><a href="#1-Simple-copy-and-paste" class="headerlink" title="1. Simple copy and paste"></a>1. Simple copy and paste</h3><p>vim provides 12 clipboards, their names are vim and there are 11 pasteboards, which are 0, 1, 2, …, 9, a, “. If you open the system clipboard, there will be two more + And *. Use the: reg command to view the contents of each pasteboard.</p><p>In vim, simply use <code>y</code> and just copy it to the clipboard of<code>&quot;</code>, and the same thing as paste with <code>p</code> is also the content of this clipboard.</p><h3 id="2-Copy-and-paste-to-the-specified-clipboard"><a href="#2-Copy-and-paste-to-the-specified-clipboard" class="headerlink" title="2. Copy and paste to the specified clipboard"></a>2. Copy and paste to the specified clipboard</h3><p>To copy the content of vim to a pasteboard, enter the normal mode, select the content to be copied, then press <code>&quot; Ny</code> to complete the copy, where N is the pasteboard number (note that you click the double quotes and then press the pasteboard number Finally, press y), for example, to copy the content to the pasteboard a, select the content and press “ay”.</p><p>To paste the contents of a vim pasteboard, you need to exit the editing mode, press <code>&quot; Np</code> in the normal mode, where N is the pasteboard number. For example, you can press <code>&quot; 5p</code> to paste the contents of pasteboard No. If you paste it in, you can also press <code>&quot; + p</code> to paste the content in the system global pasteboard.</p><h3 id="3-System-Clipboard"><a href="#3-System-Clipboard" class="headerlink" title="3. System Clipboard"></a>3. System Clipboard</h3><p>To view the clipboard supported by vim, enter <code>: reg</code> in normal mode</p><p>How should it be used to interact with the system clipboard? When you encounter problems, the first thing you are looking for is the help document. The Clipboard is the Clipboard. View help via <code>: h clipboard</code></p><p>The asterisk * and plus sign + pasteboard are system pasteboards. Under Windows, the * and + clipboards are the same. For X11 systems, * the clipboard stores the selected or highlighted content, + the clipboard stores the copied or cut content. Open the clipboard option, you can access + clipboard; open xterm_clipboard, you can access * clipboard. * One function of the clipboard is that the content selected in one window of vim can be taken out in another window of vim.</p><p>** Copy to system clipboard **</p><p>example:</p><p><code>&quot; * y</code> <code>&quot; + y</code> <code>&quot; + Nyy</code> copy N lines to the system clipboard</p><p>Explanation:</p><table><thead><tr><th>Command</th><th>Meaning</th></tr></thead><tbody><tr><td>{Visual} “+ y</td><td>copy the selected text into the system clipboard</td></tr><tr><td>“+ y {motion}</td><td>copy the text specified by {motion} into the system clipboard</td></tr><tr><td>: [range] yank +</td><td>copy the text specified by [range] into the system clipboard</td></tr></tbody></table><p>** Cut to system clipboard **</p><p>example:</p><p>“+ dd</p><p>** Paste from system clipboard to vim **</p><p>In normal mode:</p><ol><li><code>&quot; * p</code></li><li><code>&quot; + p</code></li><li><code>: put +</code> Meaning: Ex command puts contents of system clipboard on a new line</li></ol><p>In insert mode:</p><p><code>&lt;Cr&gt; +</code> Meaning: From insert mode (or commandline mode)</p><p>“+ p is better than the Ctrl-v command, it can handle the pasting of large blocks of text faster and more reliably, and can also avoid the accumulation of automatic indentation at the beginning of each line when pasting a large amount of text, because Ctrl-v The system caches stream processing, processing the pasted text line by line.</p><h2 id="vim-encoding"><a href="#vim-encoding" class="headerlink" title="vim encoding"></a>vim encoding</h2><p>Vim can edit all kinds of character encoding files very well, which of course includes popular Unicode encoding methods such as UCS-2 and UTF-8.</p><p>Four character encoding options, encoding, fileencoding, fileencodings, termencoding (for possible values ​​of these options, please refer to Vim online help: help encoding-names, their meanings are as follows:</p><p>-** encoding **: The character encoding used internally by Vim</p><p>Including Vim’s buffer (buffer), menu text, message text, etc. The default is to choose according to your locale. The user manual recommends changing its value only in .vimrc. In fact, it seems that it only makes sense to change its value in .vimrc. You can use another encoding to edit and save the file. For example, the encoding of your vim is utf-8, and the edited file uses cp936 encoding. Vim will automatically convert the read file into utf-8 (vim can read) Understand the way), and when you write a file, it will automatically switch back to cp936 (the file save encoding).</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This article Covers everything related to be going with Vim for any beginner or intermediate.&lt;br&gt;I have explained the ins and outs of Vim in briefly.&lt;br&gt;This article will be continuously updated based on experience.&lt;/p&gt;
    
    </summary>
    
    
      <category term="IDE" scheme="https://massivefile.com/categories/IDE/"/>
    
    
      <category term="linux" scheme="https://massivefile.com/tags/linux/"/>
    
      <category term="IDE" scheme="https://massivefile.com/tags/IDE/"/>
    
  </entry>
  
</feed>
